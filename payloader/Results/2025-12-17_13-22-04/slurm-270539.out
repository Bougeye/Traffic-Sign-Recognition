Requirement already satisfied: pip in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.3)
Requirement already satisfied: wheel in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (0.45.1)
Requirement already satisfied: setuptools in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (80.9.0)
Requirement already satisfied: packaging in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.0)
All required imports already available in venv.
Import check passed (including torchvision).
=== ENV ===
HOSTNAME: gruenau1
PARTITION: gpu
CUDA_VISIBLE_DEVICES: 0
PWD: /vol/fob-vol1/mi23/ziglowsa/Payload
numpy: 2.2.6
torch: 2.4.1+cu121 cuda: 12.1 is_available: True
torchvision: 0.19.1+cu121
nvidia-smi -L: GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-502de52b-61ca-040e-e44d-5cee4e192a9d)
device: cuda
Epoch: 0 of 1
[batch 20] samples: 320, Training Loss: 0.2349
   Time since start: 0:00:02.990680
[batch 40] samples: 640, Training Loss: 0.2127
   Time since start: 0:00:04.918765
[batch 60] samples: 960, Training Loss: 0.2342
   Time since start: 0:00:06.683491
[batch 80] samples: 1280, Training Loss: 0.1903
   Time since start: 0:00:08.472727
[batch 100] samples: 1600, Training Loss: 0.2104
   Time since start: 0:00:10.294179
[batch 120] samples: 1920, Training Loss: 0.2053
   Time since start: 0:00:11.980977
[batch 140] samples: 2240, Training Loss: 0.1407
   Time since start: 0:00:13.483435
[batch 160] samples: 2560, Training Loss: 0.1552
   Time since start: 0:00:15.145553
[batch 180] samples: 2880, Training Loss: 0.1378
   Time since start: 0:00:16.723307
[batch 200] samples: 3200, Training Loss: 0.1182
   Time since start: 0:00:18.358211
[batch 220] samples: 3520, Training Loss: 0.1146
   Time since start: 0:00:20.030788
[batch 240] samples: 3840, Training Loss: 0.1118
   Time since start: 0:00:21.682107
[batch 260] samples: 4160, Training Loss: 0.0751
   Time since start: 0:00:23.304304
[batch 280] samples: 4480, Training Loss: 0.0838
   Time since start: 0:00:24.955523
[batch 300] samples: 4800, Training Loss: 0.1071
   Time since start: 0:00:26.816798
[batch 320] samples: 5120, Training Loss: 0.1030
   Time since start: 0:00:28.203516
[batch 340] samples: 5440, Training Loss: 0.0742
   Time since start: 0:00:29.925445
[batch 360] samples: 5760, Training Loss: 0.0843
   Time since start: 0:00:31.338054
[batch 380] samples: 6080, Training Loss: 0.0804
   Time since start: 0:00:32.646530
[batch 400] samples: 6400, Training Loss: 0.0959
   Time since start: 0:00:34.112442
[batch 420] samples: 6720, Training Loss: 0.0679
   Time since start: 0:00:35.515349
[batch 440] samples: 7040, Training Loss: 0.0706
   Time since start: 0:00:36.961216
[batch 460] samples: 7360, Training Loss: 0.0570
   Time since start: 0:00:38.373461
[batch 480] samples: 7680, Training Loss: 0.0704
   Time since start: 0:00:40.132684
[batch 500] samples: 8000, Training Loss: 0.0991
   Time since start: 0:00:41.636931
[batch 520] samples: 8320, Training Loss: 0.0597
   Time since start: 0:00:43.052868
[batch 540] samples: 8640, Training Loss: 0.0788
   Time since start: 0:00:44.677008
[batch 560] samples: 8960, Training Loss: 0.0611
   Time since start: 0:00:46.347579
[batch 580] samples: 9280, Training Loss: 0.0744
   Time since start: 0:00:48.050566
[batch 600] samples: 9600, Training Loss: 0.0841
   Time since start: 0:00:49.658726
[batch 620] samples: 9920, Training Loss: 0.1125
   Time since start: 0:00:51.125568
[batch 640] samples: 10240, Training Loss: 0.0732
   Time since start: 0:00:52.545767
[batch 660] samples: 10560, Training Loss: 0.0666
   Time since start: 0:00:53.997263
[batch 680] samples: 10880, Training Loss: 0.0669
   Time since start: 0:00:55.486638
[batch 700] samples: 11200, Training Loss: 0.0600
   Time since start: 0:00:57.458678
[batch 720] samples: 11520, Training Loss: 0.0639
   Time since start: 0:00:59.437890
[batch 740] samples: 11840, Training Loss: 0.0741
   Time since start: 0:01:01.389825
[batch 760] samples: 12160, Training Loss: 0.0600
   Time since start: 0:01:03.411798
[batch 780] samples: 12480, Training Loss: 0.0705
   Time since start: 0:01:05.361379
[batch 800] samples: 12800, Training Loss: 0.0647
   Time since start: 0:01:07.110784
[batch 820] samples: 13120, Training Loss: 0.0627
   Time since start: 0:01:08.897542
[batch 840] samples: 13440, Training Loss: 0.0612
   Time since start: 0:01:10.566730
[batch 860] samples: 13760, Training Loss: 0.0561
   Time since start: 0:01:12.262806
[batch 880] samples: 14080, Training Loss: 0.0664
   Time since start: 0:01:13.947454
[batch 900] samples: 14400, Training Loss: 0.0459
   Time since start: 0:01:15.734501
[batch 920] samples: 14720, Training Loss: 0.0559
   Time since start: 0:01:17.682380
[batch 940] samples: 15040, Training Loss: 0.0656
   Time since start: 0:01:19.545283
[batch 960] samples: 15360, Training Loss: 0.0901
   Time since start: 0:01:21.395540
[batch 980] samples: 15680, Training Loss: 0.0660
   Time since start: 0:01:23.046998
[batch 1000] samples: 16000, Training Loss: 0.0336
   Time since start: 0:01:24.398183
[batch 1020] samples: 16320, Training Loss: 0.0558
   Time since start: 0:01:26.002287
[batch 1040] samples: 16640, Training Loss: 0.0338
   Time since start: 0:01:27.728270
[batch 1060] samples: 16960, Training Loss: 0.0337
   Time since start: 0:01:29.377678
[batch 1080] samples: 17280, Training Loss: 0.0390
   Time since start: 0:01:31.012797
[batch 1100] samples: 17600, Training Loss: 0.0550
   Time since start: 0:01:32.857391
[batch 1120] samples: 17920, Training Loss: 0.0576
   Time since start: 0:01:34.569344
[batch 1140] samples: 18240, Training Loss: 0.0449
   Time since start: 0:01:36.132117
[batch 1160] samples: 18560, Training Loss: 0.0383
   Time since start: 0:01:37.604210
[batch 1180] samples: 18880, Training Loss: 0.0441
   Time since start: 0:01:39.266166
[batch 1200] samples: 19200, Training Loss: 0.0394
   Time since start: 0:01:40.938197
[batch 1220] samples: 19520, Training Loss: 0.0838
   Time since start: 0:01:42.395093
[batch 1240] samples: 19840, Training Loss: 0.0240
   Time since start: 0:01:44.091059
[batch 1260] samples: 20160, Training Loss: 0.0280
   Time since start: 0:01:45.796615
[batch 1280] samples: 20480, Training Loss: 0.0323
   Time since start: 0:01:47.318421
[batch 1300] samples: 20800, Training Loss: 0.0455
   Time since start: 0:01:48.720172
[batch 1320] samples: 21120, Training Loss: 0.0409
   Time since start: 0:01:50.410288
[batch 1340] samples: 21440, Training Loss: 0.0413
   Time since start: 0:01:51.906268
[batch 1360] samples: 21760, Training Loss: 0.0303
   Time since start: 0:01:53.463156
[batch 1380] samples: 22080, Training Loss: 0.0456
   Time since start: 0:01:55.257694
[batch 1400] samples: 22400, Training Loss: 0.0469
   Time since start: 0:01:56.955789
[batch 1420] samples: 22720, Training Loss: 0.0277
   Time since start: 0:01:58.418848
[batch 1440] samples: 23040, Training Loss: 0.0778
   Time since start: 0:01:59.938198
[batch 1460] samples: 23360, Training Loss: 0.0581
   Time since start: 0:02:01.739730
[batch 1480] samples: 23680, Training Loss: 0.0513
   Time since start: 0:02:03.385674
[batch 1500] samples: 24000, Training Loss: 0.0541
   Time since start: 0:02:05.163656
[batch 1520] samples: 24320, Training Loss: 0.0365
   Time since start: 0:02:06.924485
[batch 1540] samples: 24640, Training Loss: 0.0320
   Time since start: 0:02:08.649467
[batch 1560] samples: 24960, Training Loss: 0.0239
   Time since start: 0:02:10.457974
[batch 1580] samples: 25280, Training Loss: 0.0427
   Time since start: 0:02:11.916538
[batch 1600] samples: 25600, Training Loss: 0.0429
   Time since start: 0:02:13.428055
[batch 1620] samples: 25920, Training Loss: 0.0316
   Time since start: 0:02:14.956398
[batch 1640] samples: 26240, Training Loss: 0.0304
   Time since start: 0:02:16.359194
[batch 1660] samples: 26560, Training Loss: 0.0332
   Time since start: 0:02:17.708704
[batch 1680] samples: 26880, Training Loss: 0.0386
   Time since start: 0:02:19.413692
[batch 1700] samples: 27200, Training Loss: 0.0719
   Time since start: 0:02:21.145014
[batch 1720] samples: 27520, Training Loss: 0.0219
   Time since start: 0:02:22.961609
[batch 1740] samples: 27840, Training Loss: 0.0349
   Time since start: 0:02:24.487005
[batch 1760] samples: 28160, Training Loss: 0.0341
   Time since start: 0:02:25.794191
[batch 1780] samples: 28480, Training Loss: 0.0281
   Time since start: 0:02:27.219038
[batch 1800] samples: 28800, Training Loss: 0.0111
   Time since start: 0:02:28.937009
[batch 1820] samples: 29120, Training Loss: 0.0272
   Time since start: 0:02:30.652980
[batch 1840] samples: 29440, Training Loss: 0.0160
   Time since start: 0:02:32.231829
[batch 1860] samples: 29760, Training Loss: 0.0539
   Time since start: 0:02:33.957970
[batch 1880] samples: 30080, Training Loss: 0.0279
   Time since start: 0:02:35.674247
[batch 1900] samples: 30400, Training Loss: 0.0203
   Time since start: 0:02:37.543330
[batch 1920] samples: 30720, Training Loss: 0.0108
   Time since start: 0:02:39.519457
[batch 1940] samples: 31040, Training Loss: 0.0214
   Time since start: 0:02:41.342721
[batch 1960] samples: 31360, Training Loss: 0.0189
   Time since start: 0:02:43.205184
--m-Epoch 1 done.
   Training Loss: 0.0714
VALIDATING
RUNNING CLASSIFICATION REPORT
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      5916
           1       1.00      0.99      1.00       378
           2       0.99      0.99      0.99      1128
           3       0.97      1.00      0.98       420
           4       0.86      1.00      0.92       576
           5       1.00      0.98      0.99      5688
           6       1.00      0.98      0.99      5040
           7       0.98      0.98      0.98      2226
           8       0.97      1.00      0.99       420
           9       0.84      1.00      0.91       156
          10       0.99      0.98      0.99      2640
          11       0.97      0.79      0.87       570
          12       0.92      0.11      0.20       324
          13       0.84      0.87      0.86       444
          14       0.73      0.81      0.77       450
          15       0.82      0.91      0.87       282
          16       0.95      0.88      0.92       396
          17       0.80      0.51      0.62       456
          18       0.92      0.89      0.90       894
          19       0.91      0.91      0.91       534
          20       0.84      1.00      0.91       156
          21       0.79      0.88      0.84       156
          22       0.89      0.43      0.58        90
          23       0.96      0.74      0.84       108
          24       0.64      0.83      0.72       156
          25       0.95      0.89      0.92       300
          26       0.93      0.91      0.92       240
          27       0.79      0.83      0.81       120
          28       0.00      0.00      0.00        54
          29       0.70      0.70      0.70        54
          30       0.79      0.99      0.88        78
          31       0.68      0.95      0.79       228
          32       0.58      0.94      0.72       264
          33       1.00      0.94      0.97       222
          34       0.95      0.43      0.59        42
          35       0.69      0.58      0.63        72
          36       0.35      0.82      0.49        66
          37       0.87      0.98      0.92       216
          38       0.93      0.94      0.93       126
          39       0.99      0.85      0.91       360
          40       0.80      0.98      0.88       414
          41       0.95      0.90      0.92        60
          42       0.96      0.69      0.81        72

   micro avg       0.95      0.95      0.95     32592
   macro avg       0.85      0.83      0.82     32592
weighted avg       0.96      0.95      0.95     32592
 samples avg       0.96      0.95      0.95     32592

   Validation Loss: 0.0272
