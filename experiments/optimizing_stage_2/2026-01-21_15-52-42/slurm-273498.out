Requirement already satisfied: pip in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.3)
Requirement already satisfied: wheel in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (0.45.1)
Requirement already satisfied: setuptools in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (80.10.1)
Requirement already satisfied: packaging in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.0)
All required imports already available in venv.
Import check passed (including torchvision).
=== ENV ===
HOSTNAME: gruenau2
PARTITION: gpu
CUDA_VISIBLE_DEVICES: 0
PWD: /vol/fob-vol1/mi23/ziglowsa/Payload
numpy: 2.2.6
torch: 2.4.1+cu121 cuda: 12.1 is_available: True
torchvision: 0.19.1+cu121
nvidia-smi -L: GPU 0: Quadro RTX 6000 (UUID: GPU-31e29be8-c653-eba9-6d77-e9fd72722d64)
Collecting samples
Collecting concepts
Concepts collected
Collecting samples
Collecting concepts
Concepts collected
device: cuda
Epoch: 1 of 20
[batch 100] samples: 1600, Training Loss: 0.1355
   Time since start: 0:00:07.972920
[batch 200] samples: 3200, Training Loss: 0.0757
   Time since start: 0:00:14.412601
[batch 300] samples: 4800, Training Loss: 0.0443
   Time since start: 0:00:21.069280
[batch 400] samples: 6400, Training Loss: 0.0356
   Time since start: 0:00:27.641043
[batch 500] samples: 8000, Training Loss: 0.0264
   Time since start: 0:00:34.160655
[batch 600] samples: 9600, Training Loss: 0.0139
   Time since start: 0:00:40.478272
[batch 700] samples: 11200, Training Loss: 0.0109
   Time since start: 0:00:46.753204
[batch 800] samples: 12800, Training Loss: 0.0182
   Time since start: 0:00:53.618225
[batch 900] samples: 14400, Training Loss: 0.0059
   Time since start: 0:01:00.367155
[batch 1000] samples: 16000, Training Loss: 0.0051
   Time since start: 0:01:06.837675
[batch 1100] samples: 17600, Training Loss: 0.0089
   Time since start: 0:01:13.097834
[batch 1200] samples: 19200, Training Loss: 0.0081
   Time since start: 0:01:19.488257
[batch 1300] samples: 20800, Training Loss: 0.0135
   Time since start: 0:01:25.989653
[batch 1400] samples: 22400, Training Loss: 0.0021
   Time since start: 0:01:32.406828
[batch 1500] samples: 24000, Training Loss: 0.0018
   Time since start: 0:01:38.745056
[batch 1600] samples: 25600, Training Loss: 0.0038
   Time since start: 0:01:45.011177
[batch 1700] samples: 27200, Training Loss: 0.0016
   Time since start: 0:01:51.545268
[batch 1800] samples: 28800, Training Loss: 0.0013
   Time since start: 0:01:58.144762
[batch 1900] samples: 30400, Training Loss: 0.0015
   Time since start: 0:02:04.449150
--m-Epoch 1 done.
   Training Loss: 0.0326
   Validation Loss: 0.0017
Epoch: 2 of 20
[batch 100] samples: 1600, Training Loss: 0.0014
   Time since start: 0:02:26.854169
[batch 200] samples: 3200, Training Loss: 0.0012
   Time since start: 0:02:33.494946
[batch 300] samples: 4800, Training Loss: 0.0010
   Time since start: 0:02:39.901605
[batch 400] samples: 6400, Training Loss: 0.0021
   Time since start: 0:02:46.639981
[batch 500] samples: 8000, Training Loss: 0.0006
   Time since start: 0:02:53.245987
[batch 600] samples: 9600, Training Loss: 0.0007
   Time since start: 0:02:59.704866
[batch 700] samples: 11200, Training Loss: 0.0013
   Time since start: 0:03:06.254584
[batch 800] samples: 12800, Training Loss: 0.0012
   Time since start: 0:03:12.649811
[batch 900] samples: 14400, Training Loss: 0.0006
   Time since start: 0:03:19.109774
[batch 1000] samples: 16000, Training Loss: 0.0012
   Time since start: 0:03:25.788372
[batch 1100] samples: 17600, Training Loss: 0.0005
   Time since start: 0:03:32.322513
[batch 1200] samples: 19200, Training Loss: 0.0006
   Time since start: 0:03:39.276063
[batch 1300] samples: 20800, Training Loss: 0.0008
   Time since start: 0:03:45.964100
[batch 1400] samples: 22400, Training Loss: 0.0028
   Time since start: 0:03:52.558611
[batch 1500] samples: 24000, Training Loss: 0.0003
   Time since start: 0:03:59.054394
[batch 1600] samples: 25600, Training Loss: 0.0006
   Time since start: 0:04:05.794995
[batch 1700] samples: 27200, Training Loss: 0.0004
   Time since start: 0:04:12.424488
[batch 1800] samples: 28800, Training Loss: 0.0008
   Time since start: 0:04:18.995879
[batch 1900] samples: 30400, Training Loss: 0.0011
   Time since start: 0:04:25.246306
--m-Epoch 2 done.
   Training Loss: 0.0019
   Validation Loss: 0.0003
Epoch: 3 of 20
[batch 100] samples: 1600, Training Loss: 0.0004
   Time since start: 0:04:47.035390
[batch 200] samples: 3200, Training Loss: 0.0003
   Time since start: 0:04:53.362574
[batch 300] samples: 4800, Training Loss: 0.0004
   Time since start: 0:04:59.880955
[batch 400] samples: 6400, Training Loss: 0.0004
   Time since start: 0:05:06.433138
[batch 500] samples: 8000, Training Loss: 0.0003
   Time since start: 0:05:13.164593
[batch 600] samples: 9600, Training Loss: 0.0003
   Time since start: 0:05:19.643460
[batch 700] samples: 11200, Training Loss: 0.0012
   Time since start: 0:05:25.953639
[batch 800] samples: 12800, Training Loss: 0.0013
   Time since start: 0:05:32.404111
[batch 900] samples: 14400, Training Loss: 0.0012
   Time since start: 0:05:38.785118
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:05:45.193373
[batch 1100] samples: 17600, Training Loss: 0.0008
   Time since start: 0:05:51.788412
[batch 1200] samples: 19200, Training Loss: 0.0033
   Time since start: 0:05:58.573342
[batch 1300] samples: 20800, Training Loss: 0.0004
   Time since start: 0:06:05.059190
[batch 1400] samples: 22400, Training Loss: 0.0006
   Time since start: 0:06:11.657139
[batch 1500] samples: 24000, Training Loss: 0.0003
   Time since start: 0:06:18.262890
[batch 1600] samples: 25600, Training Loss: 0.0013
   Time since start: 0:06:24.859889
[batch 1700] samples: 27200, Training Loss: 0.0002
   Time since start: 0:06:31.722827
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:06:38.298848
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:06:44.895091
--m-Epoch 3 done.
   Training Loss: 0.0010
   Validation Loss: 0.0003
Epoch: 4 of 20
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:07:06.666116
[batch 200] samples: 3200, Training Loss: 0.0005
   Time since start: 0:07:13.178794
[batch 300] samples: 4800, Training Loss: 0.0003
   Time since start: 0:07:19.824226
[batch 400] samples: 6400, Training Loss: 0.0014
   Time since start: 0:07:26.454810
[batch 500] samples: 8000, Training Loss: 0.0033
   Time since start: 0:07:32.975809
[batch 600] samples: 9600, Training Loss: 0.0003
   Time since start: 0:07:39.413554
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:07:45.896344
[batch 800] samples: 12800, Training Loss: 0.0003
   Time since start: 0:07:52.464793
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:07:59.053861
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:08:05.973974
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:08:12.603733
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:08:19.187635
[batch 1300] samples: 20800, Training Loss: 0.0004
   Time since start: 0:08:25.681551
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:08:32.102112
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:08:38.905134
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:08:45.690213
[batch 1700] samples: 27200, Training Loss: 0.0005
   Time since start: 0:08:52.362882
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:08:58.906504
[batch 1900] samples: 30400, Training Loss: 0.0002
   Time since start: 0:09:05.290391
--m-Epoch 4 done.
   Training Loss: 0.0010
   Validation Loss: 0.0005
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:09:27.002895
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:09:33.425245
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:09:40.037477
[batch 400] samples: 6400, Training Loss: 0.0004
   Time since start: 0:09:46.356091
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:09:52.694797
[batch 600] samples: 9600, Training Loss: 0.0153
   Time since start: 0:09:59.238479
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:10:05.795085
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:10:12.355332
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:10:18.968664
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:10:25.641649
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:10:32.159402
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:10:38.686818
[batch 1300] samples: 20800, Training Loss: 0.0002
   Time since start: 0:10:45.323095
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:10:52.082504
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:10:58.888531
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:11:05.294673
[batch 1700] samples: 27200, Training Loss: 0.0003
   Time since start: 0:11:11.648131
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:11:18.151078
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:11:24.883486
--m-Epoch 5 done.
   Training Loss: 0.0006
   Validation Loss: 0.0002
Epoch: 6 of 20
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:11:47.493993
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:11:54.146766
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:12:00.530239
[batch 400] samples: 6400, Training Loss: 0.0002
   Time since start: 0:12:07.063685
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:12:13.648001
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:12:20.291399
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:12:27.135864
[batch 800] samples: 12800, Training Loss: 0.0002
   Time since start: 0:12:33.677081
[batch 900] samples: 14400, Training Loss: 0.0024
   Time since start: 0:12:40.156499
[batch 1000] samples: 16000, Training Loss: 0.0010
   Time since start: 0:12:46.593209
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:12:53.015465
[batch 1200] samples: 19200, Training Loss: 0.0045
   Time since start: 0:12:59.671297
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:13:06.438073
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:13:13.031746
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:13:19.662735
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:13:26.044416
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:13:32.458171
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:13:39.017795
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:13:45.807339
--m-Epoch 6 done.
   Training Loss: 0.0006
   Validation Loss: 0.0037
patience decreased: patience is now  4
Epoch: 7 of 20
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:14:08.182982
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:14:14.776541
[batch 300] samples: 4800, Training Loss: 0.0004
   Time since start: 0:14:21.174383
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:14:27.599288
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:14:34.107250
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:14:40.684786
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:14:47.375406
[batch 800] samples: 12800, Training Loss: 0.0079
   Time since start: 0:14:53.952635
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:15:00.519193
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:15:07.079978
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:15:13.676076
[batch 1200] samples: 19200, Training Loss: 0.0070
   Time since start: 0:15:20.401533
[batch 1300] samples: 20800, Training Loss: 0.0002
   Time since start: 0:15:26.903038
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:15:33.280667
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:15:40.025082
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:15:46.775804
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:15:53.529218
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:16:00.311503
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:16:07.004044
--m-Epoch 7 done.
   Training Loss: 0.0007
   Validation Loss: 0.0001
Epoch: 8 of 20
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:16:28.534785
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:16:35.062972
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:16:41.442626
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:16:47.673288
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:16:54.125002
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:17:00.882907
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:17:07.460491
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:17:13.897038
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:17:20.286769
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:17:26.687257
[batch 1100] samples: 17600, Training Loss: 0.0004
   Time since start: 0:17:33.067739
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:17:39.435215
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:17:45.937491
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:17:52.681724
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:17:59.444560
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:18:06.214232
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:18:12.936564
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:18:19.333657
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:18:25.706669
--m-Epoch 8 done.
   Training Loss: 0.0002
   Validation Loss: 0.0006
patience decreased: patience is now  4
Epoch: 9 of 20
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:18:47.726097
[batch 200] samples: 3200, Training Loss: 0.0002
   Time since start: 0:18:54.078588
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:19:00.465142
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:19:06.821787
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:19:13.322167
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:19:19.875443
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:19:26.568169
[batch 800] samples: 12800, Training Loss: 0.0032
   Time since start: 0:19:33.081223
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:19:39.634657
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:19:46.314739
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:19:52.729537
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:19:59.091576
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:20:05.499556
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:20:12.023151
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:20:18.453567
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:20:24.900073
[batch 1700] samples: 27200, Training Loss: 0.0004
   Time since start: 0:20:31.408688
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:20:37.967101
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:20:44.400510
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0044
patience decreased: patience is now  3
Epoch: 10 of 20
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:21:06.441015
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:21:12.808159
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:21:19.125216
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:21:25.283738
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:21:31.859265
[batch 600] samples: 9600, Training Loss: 0.0006
   Time since start: 0:21:38.264502
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:21:44.783295
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:21:51.321805
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:21:57.840247
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:22:04.379673
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:22:10.932596
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:22:17.570482
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:22:24.183861
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:22:30.762765
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:22:37.260961
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:22:43.648397
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:22:50.035644
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:22:56.482079
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:23:03.263053
--m-Epoch 10 done.
   Training Loss: 0.0004
   Validation Loss: 0.0001
patience decreased: patience is now  2
Epoch: 11 of 20
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:23:25.101339
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:23:31.690337
[batch 300] samples: 4800, Training Loss: 0.0033
   Time since start: 0:23:38.095307
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:23:44.456042
[batch 500] samples: 8000, Training Loss: 0.0002
   Time since start: 0:23:50.784682
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:23:57.210822
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:24:03.623595
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:24:10.009680
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:24:16.207421
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:24:22.419477
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:24:28.632870
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:24:34.844351
[batch 1300] samples: 20800, Training Loss: 0.0011
   Time since start: 0:24:41.538527
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:24:48.339510
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:24:54.909440
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:25:01.315713
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:25:07.734637
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:25:14.243952
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:25:20.743560
--m-Epoch 11 done.
   Training Loss: 0.0004
   Validation Loss: 0.0002
patience decreased: patience is now  1
Epoch: 12 of 20
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:25:42.977140
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:25:49.710288
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:25:56.227924
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:26:02.906842
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:26:09.548383
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:26:16.100767
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:26:22.614101
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:26:29.222873
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:26:36.153022
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:26:42.850253
[batch 1100] samples: 17600, Training Loss: 0.0002
   Time since start: 0:26:49.548280
[batch 1200] samples: 19200, Training Loss: 0.0045
   Time since start: 0:26:55.904270
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:27:02.345038
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:27:08.709511
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:27:15.119866
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:27:21.698681
[batch 1700] samples: 27200, Training Loss: 0.0184
   Time since start: 0:27:28.242126
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:27:34.828874
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:27:41.430118
--m-Epoch 12 done.
   Training Loss: 0.0003
   Validation Loss: 0.0001
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score  support  epoch  class
0     1.000000  1.000000  1.000000   5916.0      1      0
1     1.000000  1.000000  1.000000    378.0      1      1
2     1.000000  1.000000  1.000000   1128.0      1      2
3     1.000000  1.000000  1.000000    420.0      1      3
4     1.000000  1.000000  1.000000    576.0      1      4
..         ...       ...       ...      ...    ...    ...
559   1.000000  1.000000  1.000000     72.0     12     42
560   0.999908  0.999939  0.999923  32592.0     12      0
561   0.999367  0.999607  0.999483  32592.0     12      1
562   0.999909  0.999939  0.999923  32592.0     12      2
563   0.999926  0.999943  0.999931  32592.0     12      3

[564 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 3.6314
   Time since start: 0:00:01.396339
[batch 40] samples: 2560, Training Loss: 3.2887
   Time since start: 0:00:01.448318
[batch 60] samples: 3840, Training Loss: 2.7648
   Time since start: 0:00:01.496796
[batch 80] samples: 5120, Training Loss: 1.8269
   Time since start: 0:00:01.545455
[batch 100] samples: 6400, Training Loss: 1.5762
   Time since start: 0:00:01.593784
[batch 120] samples: 7680, Training Loss: 1.2944
   Time since start: 0:00:01.641071
[batch 140] samples: 8960, Training Loss: 0.9969
   Time since start: 0:00:01.688527
[batch 160] samples: 10240, Training Loss: 0.6135
   Time since start: 0:00:01.734259
[batch 180] samples: 11520, Training Loss: 0.6427
   Time since start: 0:00:01.780204
[batch 200] samples: 12800, Training Loss: 0.5114
   Time since start: 0:00:01.829253
[batch 220] samples: 14080, Training Loss: 0.4132
   Time since start: 0:00:01.878350
[batch 240] samples: 15360, Training Loss: 0.2908
   Time since start: 0:00:01.925757
[batch 260] samples: 16640, Training Loss: 0.2515
   Time since start: 0:00:01.972751
[batch 280] samples: 17920, Training Loss: 0.2329
   Time since start: 0:00:02.017169
[batch 300] samples: 19200, Training Loss: 0.0962
   Time since start: 0:00:02.063293
[batch 320] samples: 20480, Training Loss: 0.0667
   Time since start: 0:00:02.109620
[batch 340] samples: 21760, Training Loss: 0.0705
   Time since start: 0:00:02.155936
[batch 360] samples: 23040, Training Loss: 0.0554
   Time since start: 0:00:02.200823
[batch 380] samples: 24320, Training Loss: 0.0581
   Time since start: 0:00:02.252572
[batch 400] samples: 25600, Training Loss: 0.0406
   Time since start: 0:00:02.299441
[batch 420] samples: 26880, Training Loss: 0.0358
   Time since start: 0:00:02.348224
[batch 440] samples: 28160, Training Loss: 0.0272
   Time since start: 0:00:02.395367
[batch 460] samples: 29440, Training Loss: 0.0229
   Time since start: 0:00:02.441496
[batch 480] samples: 30720, Training Loss: 0.0194
   Time since start: 0:00:02.486138
--m-Epoch 1 done.
   Training Loss: 0.8472
   Validation Loss: 0.0200
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0158
   Time since start: 0:00:03.837755
[batch 40] samples: 2560, Training Loss: 0.0169
   Time since start: 0:00:03.885955
[batch 60] samples: 3840, Training Loss: 0.0119
   Time since start: 0:00:03.936064
[batch 80] samples: 5120, Training Loss: 0.0118
   Time since start: 0:00:03.983823
[batch 100] samples: 6400, Training Loss: 0.0079
   Time since start: 0:00:04.030614
[batch 120] samples: 7680, Training Loss: 0.0111
   Time since start: 0:00:04.076731
[batch 140] samples: 8960, Training Loss: 0.0102
   Time since start: 0:00:04.123328
[batch 160] samples: 10240, Training Loss: 0.0066
   Time since start: 0:00:04.169485
[batch 180] samples: 11520, Training Loss: 0.0064
   Time since start: 0:00:04.216560
[batch 200] samples: 12800, Training Loss: 0.0060
   Time since start: 0:00:04.264859
[batch 220] samples: 14080, Training Loss: 0.0046
   Time since start: 0:00:04.312410
[batch 240] samples: 15360, Training Loss: 0.0046
   Time since start: 0:00:04.362846
[batch 260] samples: 16640, Training Loss: 0.0048
   Time since start: 0:00:04.409578
[batch 280] samples: 17920, Training Loss: 0.0042
   Time since start: 0:00:04.457365
[batch 300] samples: 19200, Training Loss: 0.0037
   Time since start: 0:00:04.504045
[batch 320] samples: 20480, Training Loss: 0.0039
   Time since start: 0:00:04.550595
[batch 340] samples: 21760, Training Loss: 0.0032
   Time since start: 0:00:04.595977
[batch 360] samples: 23040, Training Loss: 0.0041
   Time since start: 0:00:04.642740
[batch 380] samples: 24320, Training Loss: 0.0031
   Time since start: 0:00:04.688783
[batch 400] samples: 25600, Training Loss: 0.0025
   Time since start: 0:00:04.735130
[batch 420] samples: 26880, Training Loss: 0.0027
   Time since start: 0:00:04.785255
[batch 440] samples: 28160, Training Loss: 0.0034
   Time since start: 0:00:04.834763
[batch 460] samples: 29440, Training Loss: 0.0027
   Time since start: 0:00:04.881498
[batch 480] samples: 30720, Training Loss: 0.0025
   Time since start: 0:00:04.927921
--m-Epoch 2 done.
   Training Loss: 0.0071
   Validation Loss: 0.0038
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0026
   Time since start: 0:00:05.374366
[batch 40] samples: 2560, Training Loss: 0.0019
   Time since start: 0:00:05.421119
[batch 60] samples: 3840, Training Loss: 0.0023
   Time since start: 0:00:05.469917
[batch 80] samples: 5120, Training Loss: 0.0014
   Time since start: 0:00:05.516453
[batch 100] samples: 6400, Training Loss: 0.0038
   Time since start: 0:00:05.561837
[batch 120] samples: 7680, Training Loss: 0.0023
   Time since start: 0:00:05.609411
[batch 140] samples: 8960, Training Loss: 0.0015
   Time since start: 0:00:05.660697
[batch 160] samples: 10240, Training Loss: 0.0016
   Time since start: 0:00:05.706825
[batch 180] samples: 11520, Training Loss: 0.0016
   Time since start: 0:00:05.753478
[batch 200] samples: 12800, Training Loss: 0.0015
   Time since start: 0:00:05.800296
[batch 220] samples: 14080, Training Loss: 0.0013
   Time since start: 0:00:05.846389
[batch 240] samples: 15360, Training Loss: 0.0014
   Time since start: 0:00:05.893521
[batch 260] samples: 16640, Training Loss: 0.0011
   Time since start: 0:00:05.939302
[batch 280] samples: 17920, Training Loss: 0.0013
   Time since start: 0:00:05.985628
[batch 300] samples: 19200, Training Loss: 0.0013
   Time since start: 0:00:06.032390
[batch 320] samples: 20480, Training Loss: 0.0013
   Time since start: 0:00:06.083724
[batch 340] samples: 21760, Training Loss: 0.0011
   Time since start: 0:00:06.131024
[batch 360] samples: 23040, Training Loss: 0.0010
   Time since start: 0:00:06.178376
[batch 380] samples: 24320, Training Loss: 0.0010
   Time since start: 0:00:06.224106
[batch 400] samples: 25600, Training Loss: 0.0011
   Time since start: 0:00:06.270008
[batch 420] samples: 26880, Training Loss: 0.0011
   Time since start: 0:00:06.316144
[batch 440] samples: 28160, Training Loss: 0.0010
   Time since start: 0:00:06.362979
[batch 460] samples: 29440, Training Loss: 0.0008
   Time since start: 0:00:06.410362
[batch 480] samples: 30720, Training Loss: 0.0007
   Time since start: 0:00:06.457409
--m-Epoch 3 done.
   Training Loss: 0.0017
   Validation Loss: 0.0023
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0008
   Time since start: 0:00:06.929183
[batch 40] samples: 2560, Training Loss: 0.0009
   Time since start: 0:00:06.974894
[batch 60] samples: 3840, Training Loss: 0.0007
   Time since start: 0:00:07.022617
[batch 80] samples: 5120, Training Loss: 0.0009
   Time since start: 0:00:07.068575
[batch 100] samples: 6400, Training Loss: 0.0006
   Time since start: 0:00:07.114545
[batch 120] samples: 7680, Training Loss: 0.0006
   Time since start: 0:00:07.159966
[batch 140] samples: 8960, Training Loss: 0.0006
   Time since start: 0:00:07.207062
[batch 160] samples: 10240, Training Loss: 0.0007
   Time since start: 0:00:07.252613
[batch 180] samples: 11520, Training Loss: 0.0006
   Time since start: 0:00:07.299193
[batch 200] samples: 12800, Training Loss: 0.0007
   Time since start: 0:00:07.350528
[batch 220] samples: 14080, Training Loss: 0.0005
   Time since start: 0:00:07.396818
[batch 240] samples: 15360, Training Loss: 0.0005
   Time since start: 0:00:07.443427
[batch 260] samples: 16640, Training Loss: 0.0005
   Time since start: 0:00:07.490244
[batch 280] samples: 17920, Training Loss: 0.0005
   Time since start: 0:00:07.536900
[batch 300] samples: 19200, Training Loss: 0.0014
   Time since start: 0:00:07.584334
[batch 320] samples: 20480, Training Loss: 0.0006
   Time since start: 0:00:07.630263
[batch 340] samples: 21760, Training Loss: 0.0006
   Time since start: 0:00:07.676663
[batch 360] samples: 23040, Training Loss: 0.0006
   Time since start: 0:00:07.723333
[batch 380] samples: 24320, Training Loss: 0.0004
   Time since start: 0:00:07.773671
[batch 400] samples: 25600, Training Loss: 0.0004
   Time since start: 0:00:07.818468
[batch 420] samples: 26880, Training Loss: 0.0004
   Time since start: 0:00:07.865915
[batch 440] samples: 28160, Training Loss: 0.0005
   Time since start: 0:00:07.913164
[batch 460] samples: 29440, Training Loss: 0.0005
   Time since start: 0:00:07.958565
[batch 480] samples: 30720, Training Loss: 0.0004
   Time since start: 0:00:08.004563
--m-Epoch 4 done.
   Training Loss: 0.0009
   Validation Loss: 0.0019
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:08.441389
[batch 40] samples: 2560, Training Loss: 0.0004
   Time since start: 0:00:08.487976
[batch 60] samples: 3840, Training Loss: 0.0004
   Time since start: 0:00:08.534564
[batch 80] samples: 5120, Training Loss: 0.0003
   Time since start: 0:00:08.585409
[batch 100] samples: 6400, Training Loss: 0.0004
   Time since start: 0:00:08.632028
[batch 120] samples: 7680, Training Loss: 0.0003
   Time since start: 0:00:08.679989
[batch 140] samples: 8960, Training Loss: 0.0004
   Time since start: 0:00:08.726580
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:08.773835
[batch 180] samples: 11520, Training Loss: 0.0003
   Time since start: 0:00:08.821191
[batch 200] samples: 12800, Training Loss: 0.0003
   Time since start: 0:00:08.867572
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:08.915091
[batch 240] samples: 15360, Training Loss: 0.0004
   Time since start: 0:00:08.962115
[batch 260] samples: 16640, Training Loss: 0.0003
   Time since start: 0:00:09.013518
[batch 280] samples: 17920, Training Loss: 0.0003
   Time since start: 0:00:09.060349
[batch 300] samples: 19200, Training Loss: 0.0003
   Time since start: 0:00:09.106507
[batch 320] samples: 20480, Training Loss: 0.0003
   Time since start: 0:00:09.153639
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:09.199679
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:09.246820
[batch 380] samples: 24320, Training Loss: 0.0003
   Time since start: 0:00:09.292381
[batch 400] samples: 25600, Training Loss: 0.0003
   Time since start: 0:00:09.338050
[batch 420] samples: 26880, Training Loss: 0.0003
   Time since start: 0:00:09.384806
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:09.435240
[batch 460] samples: 29440, Training Loss: 0.0003
   Time since start: 0:00:09.481993
[batch 480] samples: 30720, Training Loss: 0.0018
   Time since start: 0:00:09.528388
--m-Epoch 5 done.
   Training Loss: 0.0007
   Validation Loss: 0.0020
patience decreased: patience is now  4
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:09.954687
[batch 40] samples: 2560, Training Loss: 0.0003
   Time since start: 0:00:10.002343
[batch 60] samples: 3840, Training Loss: 0.0003
   Time since start: 0:00:10.047805
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:10.094187
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:10.140536
[batch 120] samples: 7680, Training Loss: 0.0003
   Time since start: 0:00:10.185667
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:10.231317
[batch 160] samples: 10240, Training Loss: 0.0002
   Time since start: 0:00:10.283839
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:10.330758
[batch 200] samples: 12800, Training Loss: 0.0002
   Time since start: 0:00:10.377066
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:10.423280
[batch 240] samples: 15360, Training Loss: 0.0003
   Time since start: 0:00:10.468933
[batch 260] samples: 16640, Training Loss: 0.0003
   Time since start: 0:00:10.514447
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:10.560841
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:10.607221
[batch 320] samples: 20480, Training Loss: 0.0003
   Time since start: 0:00:10.652884
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:10.704212
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:10.749390
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:10.797051
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:10.843306
[batch 420] samples: 26880, Training Loss: 0.0002
   Time since start: 0:00:10.889218
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:10.935701
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:10.982450
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:11.029862
--m-Epoch 6 done.
   Training Loss: 0.0006
   Validation Loss: 0.0017
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:11.474426
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:11.525839
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:11.575054
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:11.621735
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:11.667979
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:11.713757
[batch 140] samples: 8960, Training Loss: 0.0028
   Time since start: 0:00:11.758785
[batch 160] samples: 10240, Training Loss: 0.0002
   Time since start: 0:00:11.804697
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:11.852243
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:11.897163
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:11.949346
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:11.997903
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:12.044218
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:12.089732
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:12.137187
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:12.183832
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:12.231112
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:12.278579
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:12.325423
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:12.378517
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:12.424152
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:12.470543
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:12.516176
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:12.562436
--m-Epoch 7 done.
   Training Loss: 0.0005
   Validation Loss: 0.0017
patience decreased: patience is now  4
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:13.007046
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:13.053260
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:13.100260
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:13.145945
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:13.192707
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:13.242975
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:13.288085
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:13.333188
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:13.380465
[batch 200] samples: 12800, Training Loss: 0.0014
   Time since start: 0:00:13.428040
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:13.474835
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:13.523654
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:13.570611
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:13.620261
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:13.667711
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:13.714456
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:13.760436
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:13.807375
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:13.853368
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:13.901475
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:13.948053
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:13.993380
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:14.044991
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:14.092014
--m-Epoch 8 done.
   Training Loss: 0.0005
   Validation Loss: 0.0017
patience decreased: patience is now  3
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:14.552469
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:14.599960
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:14.645082
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:14.691540
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:14.739030
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:14.784817
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:14.830330
[batch 160] samples: 10240, Training Loss: 0.1717
   Time since start: 0:00:14.878848
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:14.927610
[batch 200] samples: 12800, Training Loss: 0.0003
   Time since start: 0:00:14.974545
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:15.021865
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:15.067491
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:15.113054
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:15.159365
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:15.206496
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:15.251871
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:15.300399
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:15.348542
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:15.395391
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:15.441999
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:15.488794
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:15.535629
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:15.582760
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:15.630336
--m-Epoch 9 done.
   Training Loss: 0.0005
   Validation Loss: 0.0017
patience decreased: patience is now  2
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:16.070029
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:16.117108
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:16.168402
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:16.215271
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:16.261475
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:16.307202
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:16.352958
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:16.398764
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:16.445144
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:16.491766
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:16.537303
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:16.587399
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:16.634754
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:16.681693
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:16.730829
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:16.776063
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:16.821890
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:16.868180
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:16.915238
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:16.961037
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:17.010875
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:17.056992
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:17.103389
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:17.150194
--m-Epoch 10 done.
   Training Loss: 0.0005
   Validation Loss: 0.0018
patience decreased: patience is now  1
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:17.587955
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:17.633866
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:17.679542
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:17.726792
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:17.773014
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:17.820509
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:17.873113
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:17.919483
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:17.965950
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:18.011904
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:18.059548
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:18.105199
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:18.150972
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:18.198267
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:18.247085
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:18.294060
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:18.340787
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:18.386030
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:18.432513
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:18.480998
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:18.527497
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:18.574868
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:18.621012
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:18.670339
--m-Epoch 11 done.
   Training Loss: 0.0005
   Validation Loss: 0.0018
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
501   1.000000  1.000000  1.000000    48.000000     11     41
502   1.000000  1.000000  1.000000    48.000000     11     42
503   0.999745  0.999745  0.999745     0.999745     11      0
504   0.999379  0.999585  0.999477  7842.000000     11      1
505   0.999748  0.999745  0.999745  7842.000000     11      2

[506 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.166865
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.218708
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.266433
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.312382
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.359598
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:00.406744
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.453354
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.500717
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.549626
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.600275
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.647117
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.693341
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.740853
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.787427
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.834127
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.881082
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.927666
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.975183
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.023084
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.071858
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.123690
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.171054
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.218382
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.265135
--m-Epoch 1 done.
   Training Loss: 0.0005
   Validation Loss: 0.0019
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.759484
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.806137
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.852183
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.898012
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.946978
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.993640
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:02.039715
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.086541
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.132641
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.181064
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.227236
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.273943
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.321189
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.373582
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.419547
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.466711
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.513186
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.560969
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.609092
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.655753
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.702946
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.749727
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.799935
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.846608
--m-Epoch 2 done.
   Training Loss: 0.0004
   Validation Loss: 0.0019
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.320601
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.365638
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.411172
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.458033
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.505266
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.551629
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.599042
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.649129
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.695988
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.742740
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.789676
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.836396
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.883382
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.929738
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.976509
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:04.022610
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:04.073060
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.119421
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.165723
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.213079
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.260839
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.307514
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.353438
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.400339
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0019
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.870130
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.921507
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.969428
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:05.016186
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:05.062428
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:05.108338
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.155160
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.201089
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.246004
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.292264
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.342320
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.387281
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.432481
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.477981
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.526261
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.572528
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.617413
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.664669
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.710968
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.760692
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.806903
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.853745
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.901574
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.949289
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0020
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.424505
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.471148
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.517911
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.569071
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.614895
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.661187
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.708047
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.754817
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:06.800434
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.847572
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.895066
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.943075
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.994194
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:07.042532
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:07.088377
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:07.136533
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.182884
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.229412
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.275665
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.322527
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.370450
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.420993
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.466934
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.514414
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0020
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.961899
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:08.009035
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:08.055707
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:08.101684
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:08.148729
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.194271
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.245900
[batch 160] samples: 10240, Training Loss: 0.0013
   Time since start: 0:00:08.294152
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.342386
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.390345
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.436393
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.482707
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.529820
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.575908
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.622608
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.672387
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.719240
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.765933
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.813378
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.861223
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.908221
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.955744
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:09.001739
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:09.049269
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0021
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999338  0.999515  0.999422  7842.000000      6      1
275   0.999621  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.149194
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.195946
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.244056
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.292135
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337966
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.384058
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431115
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.477667
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.529198
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.576187
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:00.623001
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.670829
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.718083
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.763937
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.809042
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.855512
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.901921
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.953156
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.000925
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.049148
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.095906
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.143857
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.190087
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.236998
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0022
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.706479
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.753826
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.806703
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.854040
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.899539
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.946216
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.993911
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.041124
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.087881
[batch 200] samples: 12800, Training Loss: 0.0004
   Time since start: 0:00:02.134135
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.180803
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.230993
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.277964
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.323309
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.368708
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.414865
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.461492
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.508409
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.555020
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.601185
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.652844
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.699776
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.747271
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.793543
--m-Epoch 2 done.
   Training Loss: 0.0004
   Validation Loss: 0.0023
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.243051
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.291290
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.338290
[batch 80] samples: 5120, Training Loss: 0.0003
   Time since start: 0:00:03.384510
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:03.430999
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.481599
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.529107
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.576136
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.622818
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.669374
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.716183
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.763704
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.809998
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.855829
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.906941
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.953367
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.999245
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.046069
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.092540
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.139199
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.185509
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.232959
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.279564
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.329727
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0023
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.785797
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.833424
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.881223
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.927996
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.975410
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:05.022277
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.068257
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.116711
[batch 180] samples: 11520, Training Loss: 0.1499
   Time since start: 0:00:05.167215
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.214913
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.261250
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.307173
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.354237
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.400451
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.446999
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.495255
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.546051
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.593274
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.640579
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.687434
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.734652
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.781808
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.829060
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.875779
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0025
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.318765
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.366337
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.416728
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.464324
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.511108
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.558797
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.603807
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.649593
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.694587
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.741762
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.789094
[batch 240] samples: 15360, Training Loss: 0.0008
   Time since start: 0:00:06.840203
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:06.887753
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.934934
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.981913
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:07.028236
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.075899
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.123245
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.170323
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.216522
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.267001
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.313926
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.362393
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.410614
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0026
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.863140
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.910115
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.957929
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:08.004908
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:08.051399
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.101858
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.151132
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.196913
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.243597
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.289984
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.335857
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.381066
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.427740
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.474686
[batch 300] samples: 19200, Training Loss: 0.0009
   Time since start: 0:00:08.526139
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:08.573575
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:08.619586
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.666343
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.714333
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.761508
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.808662
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.856363
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.906284
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.952603
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0026
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     0.997481  1.000000  0.998739   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999338  0.999515  0.999422  7842.000000      6      1
275   0.999621  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.153520
[batch 40] samples: 2560, Training Loss: 0.1320
   Time since start: 0:00:00.202138
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:00.248526
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:00.297130
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:00.346236
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.393049
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.439433
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.487610
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.534958
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.581710
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.628640
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.675458
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.722223
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.773042
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.820507
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.867300
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.914984
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.960543
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.006419
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.053064
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.098029
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.144693
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.195375
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.242559
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0027
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.692886
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.740068
[batch 60] samples: 3840, Training Loss: 0.0004
   Time since start: 0:00:01.787508
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:01.833011
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.878813
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.926428
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:01.971567
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.017738
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.068051
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.114225
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.162109
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.209459
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.256515
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.303788
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.350910
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.398163
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.446402
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.496263
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.543391
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.589578
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.636453
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.682865
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.729471
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.775818
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0027
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.220697
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.267996
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.316496
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.367601
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.413960
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.461134
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:03.508569
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.555083
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:03.599982
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:03.647247
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.694201
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.741694
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.792456
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.837501
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.882824
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.929528
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.977029
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.022775
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.068792
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.115610
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.162270
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.213489
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.260015
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.308631
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0028
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.764479
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.811922
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.858960
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.905201
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.952088
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.999232
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.049553
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.096078
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:05.142573
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:05.188212
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.235321
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.282340
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.329833
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.376602
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.422902
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.473926
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.521574
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.569114
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.616090
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.662145
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.707873
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.753984
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.798600
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.844954
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0029
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.262725
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.312325
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.358737
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.406193
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.454639
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.502090
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.549431
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.595672
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.642739
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.689134
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.740354
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.786211
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.832855
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.880642
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.926467
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.974126
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.020583
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.067890
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.114407
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.166274
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.212681
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.258604
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.304536
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:07.350006
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0039
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:07.793954
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:07.841442
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:07.888692
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.936193
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.987990
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.036466
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.083260
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.131322
[batch 180] samples: 11520, Training Loss: 0.0003
   Time since start: 0:00:08.178764
[batch 200] samples: 12800, Training Loss: 0.0004
   Time since start: 0:00:08.225572
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.273342
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.321315
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:08.367560
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.418199
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.465755
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.513743
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.562377
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.611034
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.657438
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.701814
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.749143
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.795887
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.845891
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.891812
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0030
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     0.997481  1.000000  0.998739   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999401  0.999567  0.999479  7842.000000      6      1
275   0.999748  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.161979
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.209598
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.256555
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.302334
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.349619
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.396868
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.444972
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.493494
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.544438
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.591072
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.637498
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.684394
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.731132
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.777332
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.823734
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:00.870561
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:00.916716
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:00.966729
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.014380
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.061205
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.109133
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.156055
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.202893
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.250210
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0032
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.681263
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.727895
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.774737
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.826763
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.874699
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.921426
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.969189
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.015970
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.061626
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.107256
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.152848
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.201097
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.252454
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.301310
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.347899
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.393136
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.440021
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.487370
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.533859
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.581228
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.626903
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.678300
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.723841
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.770319
--m-Epoch 2 done.
   Training Loss: 0.0004
   Validation Loss: 0.0034
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:03.197722
[batch 40] samples: 2560, Training Loss: 0.0006
   Time since start: 0:00:03.244977
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:03.292440
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.339940
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:03.387435
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.433580
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.482421
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.531762
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.578601
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.626239
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.674374
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.722300
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.769652
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.816735
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.863230
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.912759
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.959992
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.006610
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.053150
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.101016
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.147234
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.194564
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.240971
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.286060
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0030
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.700493
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.747987
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.796595
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.843922
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.889918
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.937297
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.984947
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.031888
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.078822
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.124782
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.175329
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.222164
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.269438
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.316029
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.363315
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.410175
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.458719
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:05.506741
[batch 380] samples: 24320, Training Loss: 0.0005
   Time since start: 0:00:05.553591
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.604307
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.650329
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:05.696908
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.743608
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.790090
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0032
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.204350
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.251349
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.298518
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.345445
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.391818
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.443227
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.490914
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.537881
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.585007
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.632617
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.679201
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.726574
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.773372
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.820427
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.870610
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.918990
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:06.964028
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:07.011132
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.058028
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.103892
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.151006
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.198350
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.247042
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.298439
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0032
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.736252
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.783524
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.830748
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.877563
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.924979
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.971770
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.017740
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.064583
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.111024
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.161368
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.208669
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.256103
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:08.301870
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.349202
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:08.396794
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.443971
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.489807
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.536326
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.587739
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.635756
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.682637
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.730550
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.776574
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.822714
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0034
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.230998
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.278593
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.327624
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:09.376045
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.428001
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.475053
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:09.522717
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:09.569009
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.616874
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.663357
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.710681
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.758042
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.804281
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.855737
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.902597
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.950810
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.998928
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.047277
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.094953
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.141346
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.187473
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.233690
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.286136
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.334473
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0036
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.753343
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.800632
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.846761
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.893486
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.940564
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.986493
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:11.033413
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.079132
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.129351
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.177270
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.224152
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.272496
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.319661
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.365629
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.411793
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.459082
[batch 340] samples: 21760, Training Loss: 0.0002
   Time since start: 0:00:11.505288
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.552188
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:11.602962
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.651190
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.698345
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.745845
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.792632
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.839042
--m-Epoch 8 done.
   Training Loss: 0.0004
   Validation Loss: 0.0033
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     0.997481  1.000000  0.998739   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999745  0.999745  0.999745     0.999745      8      0
366   0.999401  0.999567  0.999479  7842.000000      8      1
367   0.999748  0.999745  0.999745  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.148240
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.195008
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241720
[batch 80] samples: 5120, Training Loss: 0.0007
   Time since start: 0:00:00.289595
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337783
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.383312
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431150
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.482993
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.531925
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.578787
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.625783
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.673449
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.721999
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.768852
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.815900
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.861845
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.912733
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.960461
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.005852
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.051402
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.099004
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.146370
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.192439
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.239210
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0031
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.661502
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.709165
[batch 60] samples: 3840, Training Loss: 0.0003
   Time since start: 0:00:01.760258
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:01.806428
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.853964
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.900893
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:01.948546
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.994167
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.040969
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.087425
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.132112
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.183522
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.231893
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.279486
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.325809
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.373975
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.420402
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.467815
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.513484
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.560240
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.613323
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.660890
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.708984
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.756971
--m-Epoch 2 done.
   Training Loss: 0.0004
   Validation Loss: 0.0034
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.171894
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.218909
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.265477
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.312112
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.359199
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.406042
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.457209
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.503854
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.550303
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.598157
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.643650
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.690945
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.737158
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.784178
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.834432
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.882718
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.931112
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.977827
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.024294
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.070194
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.117280
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.164229
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.211015
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.262854
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0025
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.688260
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.735598
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.783342
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.830148
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.876939
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.922994
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.969597
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.017980
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.066558
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.117762
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.163694
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.211709
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.259208
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.306390
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.353502
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.399869
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.447012
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.494792
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.545350
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.591604
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.639027
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.685604
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.732057
[batch 480] samples: 30720, Training Loss: 0.0014
   Time since start: 0:00:05.778369
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0043
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.218737
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.266248
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.313249
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.363746
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.411390
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.457456
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.504412
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.550674
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.597728
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.643929
[batch 220] samples: 14080, Training Loss: 0.0009
   Time since start: 0:00:06.689963
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:06.735739
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.786524
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.832035
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.878751
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.925960
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.971636
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.018203
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.064976
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.112433
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.159062
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.209956
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.257012
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.303608
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0028
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.716952
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.764355
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.812183
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.859069
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.904161
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.951466
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.998164
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.048970
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.096080
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.142549
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.189342
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.238555
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.286167
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.333153
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.380532
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.426687
[batch 340] samples: 21760, Training Loss: 0.0007
   Time since start: 0:00:08.478926
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.526894
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:08.572238
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.618779
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.665447
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.711693
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.757791
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.803816
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0032
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.227544
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.274355
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.325232
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.372062
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.418867
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.464297
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.510596
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.556426
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.602179
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.649771
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.697546
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.748172
[batch 260] samples: 16640, Training Loss: 0.1666
   Time since start: 0:00:09.793914
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:09.841203
[batch 300] samples: 19200, Training Loss: 0.1411
   Time since start: 0:00:09.886428
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:09.932553
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.980156
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.025998
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.072253
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.118689
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.170826
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.217716
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.264135
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.311719
--m-Epoch 7 done.
   Training Loss: 0.0036
   Validation Loss: 0.0046
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.727842
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.774573
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.822467
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.870610
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.916683
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.963435
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:11.014758
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:11.061732
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:11.109879
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:11.157357
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.204404
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.251531
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.297537
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.344727
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.391825
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.441959
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.487814
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.535620
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.584246
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.633295
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.680005
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.725424
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.771801
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.822281
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0051
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     0.997481  1.000000  0.998739   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999617  0.999617  0.999617     0.999617      8      0
366   0.999199  0.999504  0.999347  7842.000000      8      1
367   0.999622  0.999617  0.999618  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.139639
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.186936
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.237915
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.286458
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334136
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.379503
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426196
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473703
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.519748
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.565196
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.611368
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.663291
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.711039
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.758057
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.804478
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.851155
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.898458
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.945664
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.993486
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.042577
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090394
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:01.138821
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:01.185886
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.233406
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0056
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.663073
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.708323
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.753433
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.798534
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.844047
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.895504
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.942610
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.990618
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.036743
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.083663
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.130407
[batch 240] samples: 15360, Training Loss: 0.1273
   Time since start: 0:00:02.177179
[batch 260] samples: 16640, Training Loss: 0.0004
   Time since start: 0:00:02.224281
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:02.271569
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.322303
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.368961
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:02.416527
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.462688
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.508331
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.555011
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.602772
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.649024
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.695865
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.747937
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0060
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.171855
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.218042
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.265905
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.314779
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.361722
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.408850
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.455119
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.502187
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.549019
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.599951
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.647129
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.695381
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.741950
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.788953
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.836179
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.881834
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.927914
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.974043
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.024181
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.070038
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.116939
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.162769
[batch 460] samples: 29440, Training Loss: 0.0005
   Time since start: 0:00:04.207831
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:04.254761
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0065
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.691554
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.738864
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:04.786120
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.837795
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.886762
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.934672
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.981122
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.028158
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.077168
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.124920
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.171293
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.217768
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.269157
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.315769
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.363886
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.410878
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.457218
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.504659
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.550343
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.597211
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.642781
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.692604
[batch 460] samples: 29440, Training Loss: 0.0006
   Time since start: 0:00:05.740139
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:05.786791
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0065
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:06.206741
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.253796
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.302824
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.350844
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.396814
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.444636
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.491795
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.537999
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.587382
[batch 200] samples: 12800, Training Loss: 0.0005
   Time since start: 0:00:06.633209
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:06.680207
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:06.726759
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.773590
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:06.821397
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.869299
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.916263
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.961565
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.011860
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.058813
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.105618
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.151298
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.196680
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.244966
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.292165
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0062
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.707733
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.754375
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.801400
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.854035
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.900489
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.946652
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.992829
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.039884
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.087044
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.135222
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.182540
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.229468
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:08.281677
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.328211
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:08.375728
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.423273
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.470760
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.517821
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.564295
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.610976
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.657688
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.706950
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.753464
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.800457
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0062
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999199  0.999504  0.999347  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.145482
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.192943
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.240488
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.286619
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.332992
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.378625
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.430900
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.477516
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.525545
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.571416
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.619286
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.666497
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.713113
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.760490
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:00.808379
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.859182
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.906508
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.952868
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.999413
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.045231
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.092689
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.139078
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.185979
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.231995
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0077
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.657147
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.708942
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.755596
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.801947
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.849793
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.895462
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.941504
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.988997
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.035803
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.081085
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.131697
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.178196
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.224317
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.270122
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.315456
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.362513
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.409557
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.455334
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.500918
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.552633
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.599115
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:02.645541
[batch 460] samples: 29440, Training Loss: 0.0003
   Time since start: 0:00:02.692269
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.738693
--m-Epoch 2 done.
   Training Loss: 0.0031
   Validation Loss: 0.0057
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.156386
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.204425
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.251730
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.298434
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.344587
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.394974
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.440639
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.487346
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.533605
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.580950
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.629536
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:03.676422
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:03.723915
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:03.774743
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.824117
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.870923
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.918947
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.965129
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.012377
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.059645
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.105202
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.151894
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.203774
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.251281
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0056
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:04.670147
[batch 40] samples: 2560, Training Loss: 0.0004
   Time since start: 0:00:04.716668
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.763184
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:04.809711
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:04.856761
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:04.903352
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.949928
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.997730
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.049057
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.095611
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.142238
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.189607
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.235924
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.282929
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.329280
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.374661
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.420904
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.471474
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.519005
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.564755
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.611704
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.659163
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.705529
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.750859
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0054
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.164343
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.211257
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:06.258640
[batch 80] samples: 5120, Training Loss: 0.0004
   Time since start: 0:00:06.308505
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:06.357204
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:06.404213
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:06.450919
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.498367
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.546558
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.593601
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.640812
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.686852
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.739154
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.788106
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.836601
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.882174
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.929024
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.976395
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.022340
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.069426
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.116464
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.166161
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.213153
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.259512
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0053
patience decreased: patience is now  4
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.676323
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.721975
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.768305
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.815145
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.863381
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.910985
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.957595
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.008869
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.055593
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:08.102188
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.149421
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.195695
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:08.241788
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.288345
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.334177
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.379578
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.429116
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.475531
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.521920
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.569084
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.615808
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.662594
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.708745
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.756251
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0060
patience decreased: patience is now  3
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.174569
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.222102
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.273278
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.318449
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.363028
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.409943
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.456388
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.502458
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.548849
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.596089
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.642509
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.694383
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.741553
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.789561
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.835032
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:09.881149
[batch 340] samples: 21760, Training Loss: 0.0002
   Time since start: 0:00:09.927145
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:09.973661
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.020272
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:10.066773
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:10.117898
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.165063
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.212159
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.257500
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0076
patience decreased: patience is now  2
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.703538
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.753061
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.800820
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.846981
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.895653
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.946121
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.992784
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.038186
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.084706
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.131486
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.177300
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.224780
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.272612
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.318778
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.371476
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.420587
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.466793
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.513831
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.559565
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.605894
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.653406
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.699419
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.745224
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:11.795793
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
patience decreased: patience is now  1
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0005
   Time since start: 0:00:12.211843
[batch 40] samples: 2560, Training Loss: 0.0006
   Time since start: 0:00:12.258179
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:12.306823
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:12.353529
[batch 100] samples: 6400, Training Loss: 0.0412
   Time since start: 0:00:12.399746
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.447032
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:12.493034
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:12.540791
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:12.587063
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.638181
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.685149
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.730626
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.776400
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.822722
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.869306
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.915352
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.960493
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:13.007312
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:13.057253
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.104977
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.151493
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.198136
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.245457
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.293389
--m-Epoch 9 done.
   Training Loss: 0.0036
   Validation Loss: 0.0062
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     0.997753  1.000000  0.998875   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
409   1.000000  1.000000  1.000000    48.000000      9     41
410   1.000000  1.000000  1.000000    48.000000      9     42
411   0.999745  0.999745  0.999745     0.999745      9      0
412   0.999382  0.999585  0.999479  7842.000000      9      1
413   0.999748  0.999745  0.999745  7842.000000      9      2

[414 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.145583
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.193115
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.244262
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.291781
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337906
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.384694
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431852
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.476634
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.523666
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.571534
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.618099
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.669349
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.714508
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:00.762005
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:00.808982
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.854808
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.900819
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.946892
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.994323
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.039817
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090640
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.138203
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.185838
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.232205
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0072
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.648978
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.696960
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:01.743547
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:01.789600
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.837019
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:01.883132
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.934693
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:01.982381
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:02.031043
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.080127
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.126196
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.172269
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.218967
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.265341
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.310990
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.362307
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.408106
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.454734
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.501872
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.547632
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.593986
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.641511
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.687632
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.734658
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0068
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.165382
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.213382
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.264373
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.310818
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.358366
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.406568
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.452809
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.500266
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.545673
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.591814
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.638294
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:03.688673
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.736910
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:03.784266
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:03.829799
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:03.876559
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.924603
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.970083
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.015982
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.062109
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.113844
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.161503
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.209655
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.256491
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0067
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.672198
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.719992
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.766529
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.813149
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.859999
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.905649
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.956828
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.003214
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.049384
[batch 200] samples: 12800, Training Loss: 0.0002
   Time since start: 0:00:05.095447
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.140928
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.188208
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.236234
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.282429
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:05.328596
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.374796
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.424118
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.470804
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.517424
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.563810
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.609938
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.656986
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.704401
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.750917
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0068
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.167627
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.215614
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.266211
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.312373
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.359329
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.406373
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.453059
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.499730
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.547888
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.594636
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.641169
[batch 240] samples: 15360, Training Loss: 0.0003
   Time since start: 0:00:06.692605
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:06.739687
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:06.786410
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.832441
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:06.879739
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.925123
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:06.970425
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.015284
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:07.061852
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.113174
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.160438
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:07.208100
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.254477
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0063
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.669320
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.715492
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.763604
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.811847
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.858432
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.904545
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.956562
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.002263
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.046997
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.094191
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.140402
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.186317
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.234998
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.282037
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.329129
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.378967
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.427889
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.474878
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.521874
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.567846
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.613461
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.659185
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.706213
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:08.753037
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0072
patience decreased: patience is now  3
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:09.183431
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:09.234668
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.281417
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.328271
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.375529
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.423517
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.469855
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.516241
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.562745
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.609253
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.659191
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.706095
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.752631
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.798527
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.845334
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:09.893733
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:09.940870
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:09.987855
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.032900
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.084362
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.130121
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.176018
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.225055
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.271261
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0069
patience decreased: patience is now  2
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0005
   Time since start: 0:00:10.689008
[batch 40] samples: 2560, Training Loss: 0.0003
   Time since start: 0:00:10.735239
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.782953
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:10.829103
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:10.875479
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:10.925307
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.973238
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.020550
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.066381
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.113289
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.159704
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.205715
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.252013
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.297268
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.347852
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.394781
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.441168
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.487531
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.534323
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.581115
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.629740
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.677130
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.723678
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.776410
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0063
patience decreased: patience is now  1
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.194469
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.243082
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.290877
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.336967
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.383332
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.429487
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.475722
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.521620
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.572387
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.619970
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.667141
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.714076
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.759681
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.807049
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.854631
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.901929
[batch 340] samples: 21760, Training Loss: 0.1423
   Time since start: 0:00:12.949175
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:12.998850
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:13.044859
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:13.092672
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.140376
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.186567
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.232122
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.278155
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0068
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
409   1.000000  1.000000  1.000000    48.00000      9     41
410   1.000000  1.000000  1.000000    48.00000      9     42
411   0.999490  0.999490  0.999490     0.99949      9      0
412   0.999109  0.999453  0.999276  7842.00000      9      1
413   0.999494  0.999490  0.999490  7842.00000      9      2

[414 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.139414
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.186501
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.239628
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.287762
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334006
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.379883
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426272
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.472892
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.520699
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567343
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.614716
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.664861
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.713147
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.759907
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.806982
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.853161
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.898644
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.945690
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.992194
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.038674
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090532
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:01.138534
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:01.185828
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:01.233023
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0069
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.671551
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:01.718190
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.764334
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.811142
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:01.858193
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.907945
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.956773
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.004128
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.050202
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.095699
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.141199
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.186021
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.233307
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.279737
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.328864
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.375320
[batch 340] samples: 21760, Training Loss: 0.1330
   Time since start: 0:00:02.423072
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:02.469969
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:02.516183
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:02.561665
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.607797
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:02.654098
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.700097
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.749473
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0063
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.165578
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.212464
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.258371
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.303467
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.350311
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.396540
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.441720
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.489145
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.535968
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.587076
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.635722
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.682391
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.729910
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.776104
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.823723
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.870247
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.917592
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.964742
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.015761
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:04.062707
[batch 420] samples: 26880, Training Loss: 0.0003
   Time since start: 0:00:04.108122
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.154596
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.201580
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.248374
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0071
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.670586
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.717214
[batch 60] samples: 3840, Training Loss: 0.1442
   Time since start: 0:00:04.765212
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.812196
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.862666
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.911028
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.956895
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.004087
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.049726
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.096700
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.143734
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.190878
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.240792
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.288079
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.333783
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.380159
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.426947
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.473279
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.520571
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.567711
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.613539
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.663251
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.710465
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.757431
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0067
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.174218
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.221152
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.267642
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.314967
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.361743
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.408500
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.453950
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.503513
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.551397
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.599030
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.645612
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.690556
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.737355
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.784407
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.831697
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.877465
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.927980
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.975416
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.021036
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.065947
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.111665
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.156585
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.203540
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.250723
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0061
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.665055
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:07.710650
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:07.763185
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.809908
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.857121
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.904829
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.951237
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.998432
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.044819
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.091891
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.137766
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.188506
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.236611
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.283068
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.331102
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.377679
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.424668
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.470836
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.517532
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.562370
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.611926
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.658071
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.703998
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.749608
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0066
patience decreased: patience is now  3
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.170586
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.217542
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.263567
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.309395
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.355184
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.402078
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.451888
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.498836
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.545417
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.592310
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.640346
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.686213
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.732288
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.778942
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.825346
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.876056
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.922015
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.969403
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.017012
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.064199
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.111668
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:10.158777
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.204379
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.249847
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0079
patience decreased: patience is now  2
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.670847
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.723168
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.770407
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.817017
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.864603
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.911433
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.958021
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.004460
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.052496
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.099399
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.149041
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.195199
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.242200
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.290026
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:11.336639
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:11.382672
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.428552
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.475551
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.522580
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.572206
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.618882
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.664837
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.710032
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.756714
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0060
patience decreased: patience is now  1
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.176676
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.223861
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.271314
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.319257
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.370177
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.416695
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:12.465137
[batch 160] samples: 10240, Training Loss: 0.0002
   Time since start: 0:00:12.513809
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.560531
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:12.606353
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:12.652603
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.699394
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.745971
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.796340
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:12.843399
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.889754
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.937198
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.983609
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:13.030600
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.077643
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.123255
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.170360
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.220830
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.268970
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0061
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
409   1.000000  1.000000  1.000000    48.000000      9     41
410   1.000000  1.000000  1.000000    48.000000      9     42
411   0.999617  0.999617  0.999617     0.999617      9      0
412   0.999214  0.999515  0.999359  7842.000000      9      1
413   0.999622  0.999617  0.999618  7842.000000      9      2

[414 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.146673
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.195511
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241581
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.287382
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334414
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.381773
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431480
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.477519
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.525233
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.572312
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.619142
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.667068
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.713062
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.760803
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.808569
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.860657
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.908177
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.953567
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.999210
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.045887
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.093096
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:01.138198
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:01.183822
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.231021
--m-Epoch 1 done.
   Training Loss: 0.0010
   Validation Loss: 0.0093
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.650590
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.701431
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.748718
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.795357
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.842209
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.887148
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.933910
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.979370
[batch 180] samples: 11520, Training Loss: 0.0006
   Time since start: 0:00:02.025601
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.072466
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.124073
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.170622
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.218842
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.265640
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.312687
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.361137
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.407129
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.452991
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.498150
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.548214
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.594355
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.641699
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.686985
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.732842
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.154082
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.199924
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.245278
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.292969
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.343358
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.391142
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.436581
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.483187
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.529714
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.576950
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.623511
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.670545
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.716810
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.767199
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.813433
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.860517
[batch 340] samples: 21760, Training Loss: 0.0004
   Time since start: 0:00:03.907313
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:03.953852
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.999638
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.047389
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.092999
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.139133
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.189430
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.237279
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0075
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.665049
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.713131
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.761469
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.806953
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.852750
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.899786
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.945683
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.992789
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.043267
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.091637
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.138429
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.184966
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:05.231163
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.278131
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.324124
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.370795
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.417082
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.468655
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.515544
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.561633
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.607109
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.653632
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.701636
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.749613
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0075
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.175781
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.223500
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.272958
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.322348
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.369277
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.415413
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.461829
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.508591
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.555247
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.602117
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.649275
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.698792
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.746559
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.794043
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.841188
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.889395
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.936615
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.982097
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:07.030124
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:07.077281
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.127489
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.175656
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.223219
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.269452
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0076
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.683932
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.730631
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.776905
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.822673
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.869769
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.918922
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.970642
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.016510
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.064181
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.110108
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.155523
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.202685
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.249959
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.296017
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.342792
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.393816
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.439908
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.487281
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.533628
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.580309
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.627384
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.674011
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:08.721961
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.768559
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:09.184251
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.234851
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.282187
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.327448
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.374363
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.421088
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.467249
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.512619
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.559572
[batch 200] samples: 12800, Training Loss: 0.0003
   Time since start: 0:00:09.605466
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:09.655597
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:09.702828
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.749954
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.795675
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.843091
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.889547
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.935170
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.981264
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.027620
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.078521
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.124862
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.170785
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.215413
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.261960
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.678135
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.724972
[batch 60] samples: 3840, Training Loss: 0.1309
   Time since start: 0:00:10.772066
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:10.819334
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:10.865184
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:10.914789
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.960951
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.006683
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.051571
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.097945
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.144748
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.191482
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.238580
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.284387
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.335699
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.383231
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.428386
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.474912
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.521088
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.567120
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.614216
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.660499
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.705162
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.755130
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0078
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999617  0.999617  0.999617     0.999617      8      0
366   0.999199  0.999504  0.999347  7842.000000      8      1
367   0.999622  0.999617  0.999618  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.141809
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.192538
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.240930
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.288437
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334783
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.381428
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426548
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473481
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.521229
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567475
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.618263
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.665709
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.711954
[batch 280] samples: 17920, Training Loss: 0.0003
   Time since start: 0:00:00.758777
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:00.804445
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:00.851002
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.897482
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:00.945310
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.992294
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.044034
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090551
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.135424
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.181958
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.229539
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0075
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.650438
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.697681
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.744089
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.790555
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.836482
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.888424
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.935627
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.980273
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.027360
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.073846
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.121066
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.167001
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.213372
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.260335
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.311092
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.357756
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.404124
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.451095
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.497386
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.543359
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:02.590642
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:02.637463
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:02.683982
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:02.735428
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0078
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.179602
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:03.226409
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.273229
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.321065
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.366363
[batch 120] samples: 7680, Training Loss: 0.0003
   Time since start: 0:00:03.414153
[batch 140] samples: 8960, Training Loss: 0.0004
   Time since start: 0:00:03.462009
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:03.508722
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.560025
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:03.606957
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.654678
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.702060
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.748377
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.796002
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.843338
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.890350
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.936943
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.987917
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.034766
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.081587
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.127732
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.175433
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.220767
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.268820
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0075
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.684429
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.731782
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.780098
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.830871
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.877302
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.924834
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.972275
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.019700
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.065796
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.113178
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.158963
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.205328
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.256157
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.302880
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.350470
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.397916
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.446346
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.493061
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.539109
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:05.585328
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.635464
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:05.683007
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:05.728499
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.775680
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0082
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.191623
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.237214
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.284774
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.332092
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.379010
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.426397
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.476177
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.523676
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.571810
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.617785
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:06.666422
[batch 240] samples: 15360, Training Loss: 0.0003
   Time since start: 0:00:06.713326
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.760039
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.806212
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.851386
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.900712
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.949946
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.997100
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.043368
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.089625
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.135235
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.181722
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.227151
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.274525
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.701178
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.753239
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.800193
[batch 80] samples: 5120, Training Loss: 0.0006
   Time since start: 0:00:07.847161
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:07.893775
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.942521
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.989879
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.036474
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.081380
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.127971
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.179257
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.225414
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.273014
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.320389
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.365643
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.413791
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.460247
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.505946
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.551415
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.602237
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.649342
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.696491
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.743417
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.788826
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0076
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.159612
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.207381
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.252958
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.300237
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.347403
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.399183
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.445293
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.492898
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.539589
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:00.587925
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:00.634631
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:00.681916
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.729713
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.775564
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.826503
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.872985
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:00.920437
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.965779
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.012264
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.058228
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.104437
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.150182
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.196736
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.247648
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0079
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.663768
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.709369
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.756685
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.804724
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.851636
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.898950
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.946226
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.993545
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.040778
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.091357
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.138636
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:02.184604
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:02.231854
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.277397
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.322053
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.368260
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:02.416726
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.464510
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.515290
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.561316
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.607764
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.654638
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.700267
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.746923
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0090
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.163611
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.210274
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.257959
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.313893
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.360754
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.408693
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.454729
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.501653
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.549052
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.595405
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.641761
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.688102
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.741345
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.789226
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:03.835095
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:03.881516
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.928224
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.974860
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.022083
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.069016
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.115981
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.168456
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.215736
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.262907
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0082
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.680096
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.726027
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.773163
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.820325
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.866587
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.914053
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.959689
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.011821
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.058711
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.106320
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.153589
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.200642
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.246745
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.292893
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.340519
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.387859
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.438457
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.487179
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:05.533509
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.579340
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.626436
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.672549
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.718167
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.764329
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0094
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.188404
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.234721
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.286663
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.334433
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.381676
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.427963
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.474490
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.521568
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.568130
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.614220
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.665177
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.711042
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.756082
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.802146
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.848852
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.895158
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.940899
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.988334
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.034283
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.084239
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:07.132297
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.179188
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:07.224461
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:07.270625
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0087
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.708423
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.756208
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.804352
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.851232
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.896611
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.949020
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.995219
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.040784
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.086807
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.133738
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.181216
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.227293
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.274144
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.320853
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.372071
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.419630
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.466482
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.512995
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.559289
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.605649
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.652011
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.699200
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.744783
[batch 480] samples: 30720, Training Loss: 0.0003
   Time since start: 0:00:08.795645
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0110
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999214  0.999515  0.999359  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:00.151174
[batch 40] samples: 2560, Training Loss: 0.0003
   Time since start: 0:00:00.197905
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:00.243170
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.292325
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.338964
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.389454
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.437219
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.484802
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.531750
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.579324
[batch 220] samples: 14080, Training Loss: 0.3359
   Time since start: 0:00:00.624754
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.670321
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.717706
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.763883
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.814151
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.861187
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.907221
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.953155
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.999514
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.046805
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.094226
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.140849
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.188000
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.239560
--m-Epoch 1 done.
   Training Loss: 0.0036
   Validation Loss: 0.0075
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.702236
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.750381
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.798072
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.845339
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.893390
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.940276
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.987885
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.034013
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.084038
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.132154
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.177785
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.224002
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.269126
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.315261
[batch 300] samples: 19200, Training Loss: 0.1980
   Time since start: 0:00:02.361765
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:02.407941
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:02.454153
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.503336
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.548624
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.593658
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.639701
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.684933
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.730422
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.776154
--m-Epoch 2 done.
   Training Loss: 0.0007
   Validation Loss: 0.0091
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.258480
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.306283
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.356236
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.404132
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.451009
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.496397
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.542619
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.588367
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.634099
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.680130
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.725774
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.775084
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.821378
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.868707
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.916339
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.962644
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:04.009504
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.054624
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.103357
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.151981
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.200145
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.247421
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.294060
[batch 480] samples: 30720, Training Loss: 0.0005
   Time since start: 0:00:04.340965
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0102
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.768507
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.813703
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.860227
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.906854
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:04.954190
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:05.002383
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:05.050812
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.097820
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.144699
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.191375
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.237290
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.283690
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.330469
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.376279
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.426839
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.474929
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.521793
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.568206
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.614575
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.660974
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.707556
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.753895
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.800710
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.850788
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0085
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.267901
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.315431
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.362192
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.409391
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.454916
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.501092
[batch 140] samples: 8960, Training Loss: 0.0009
   Time since start: 0:00:06.547828
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:06.595865
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:06.642678
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.692220
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.739264
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.787686
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.833112
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.879571
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.927143
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.973227
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.020335
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.066483
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.116450
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.163681
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.211237
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.256875
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.302809
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.349473
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0090
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.768760
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.815782
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.863321
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.909890
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.959899
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.007982
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.055517
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.103567
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.149682
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.196499
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.243623
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.290105
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.339659
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.386848
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.433991
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.479364
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.526903
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.573699
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.619449
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.666504
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.714379
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.766017
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.813469
[batch 480] samples: 30720, Training Loss: 0.0003
   Time since start: 0:00:08.859307
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0115
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999214  0.999515  0.999359  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.139453
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:00.187551
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:00.236964
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.285077
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.332226
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.377501
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.424041
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473582
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.519880
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567763
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.615092
[batch 240] samples: 15360, Training Loss: 0.0004
   Time since start: 0:00:00.664667
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.713030
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.760295
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.808309
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.855865
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.902879
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.948864
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.996717
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.046674
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.092544
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.139197
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.186858
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.235023
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0093
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.660141
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.708114
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.755788
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.802589
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.847776
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.897742
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.945632
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.992069
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.038960
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.085833
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.132923
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.179165
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.226295
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.273509
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.323058
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.371425
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.419305
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.466517
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.513734
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.561486
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:02.609069
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.654734
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:02.702369
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.752007
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0112
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:03.191318
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.238091
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:03.283925
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.330973
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.377364
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.424268
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.470234
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.516508
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.565120
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.611554
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.659111
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.706372
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.753562
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:03.801402
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.847751
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.894783
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.940792
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.992136
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.038323
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.084776
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.131243
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.178143
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.225153
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.271778
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0100
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.687484
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.735658
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.782835
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.832008
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.878407
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.925742
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.972992
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.019203
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.065796
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.112712
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.159838
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.207761
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.256540
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.303696
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.350106
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.396551
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.445395
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.493362
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.541968
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.589088
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.636016
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.684991
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.732459
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:05.779815
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0127
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.197264
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:06.244889
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:06.292425
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.339299
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.386111
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.432287
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.483132
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.530362
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.578400
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.626929
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.673579
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.720225
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.767890
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.813839
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.860113
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.907813
[batch 340] samples: 21760, Training Loss: 0.0004
   Time since start: 0:00:06.954250
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:07.001334
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:07.048370
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.094301
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.141223
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.187324
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.232447
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.279999
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0100
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.694045
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.742999
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.791124
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.838375
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.886906
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.933094
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.980139
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.026464
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:08.073844
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.120636
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.168719
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.216394
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.263878
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.311078
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.358322
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.404983
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.453556
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.499411
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.545734
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.595343
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.641516
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.688446
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.736562
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.783880
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0106
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999214  0.999515  0.999359  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.145076
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.192714
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.238953
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.286786
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.333189
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.382919
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.428129
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.474603
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.520736
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567995
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.614213
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.660399
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.707102
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.754380
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.803118
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.852256
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.899459
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.945873
[batch 380] samples: 24320, Training Loss: 0.1675
   Time since start: 0:00:00.993040
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.039774
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:01.086851
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:01.132171
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.178912
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.227474
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0120
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.648832
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.696314
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.743579
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.789753
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.835757
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.881642
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.927611
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.974017
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.020514
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.070753
[batch 220] samples: 14080, Training Loss: 0.1340
   Time since start: 0:00:02.118168
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.165326
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:02.212558
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.259049
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.306693
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.353949
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.401516
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.450259
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.501427
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.549192
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.597505
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.645217
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.691283
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.739166
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0114
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.156478
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.203000
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.249607
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.300054
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.347710
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.393698
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.439427
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.485275
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.534305
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.581076
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.627474
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.674368
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.723230
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.770686
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.818939
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.865358
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.912293
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.957923
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.004413
[batch 400] samples: 25600, Training Loss: 0.1644
   Time since start: 0:00:04.051236
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:04.098878
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:04.149480
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.196285
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.243729
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0146
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.675819
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.722398
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.769252
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.816660
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.864163
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.910266
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.956982
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.007528
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.055186
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.102796
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.149360
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.195559
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.241449
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.288505
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.336238
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.385752
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:05.431779
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.478000
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:05.526002
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.572936
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.619751
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.665401
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.713740
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.759812
--m-Epoch 4 done.
   Training Loss: 0.0005
   Validation Loss: 0.0112
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.177372
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.228610
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.275762
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.322572
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.368932
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.415455
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.461904
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.508566
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.555139
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.603783
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.654111
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.700382
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.746932
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.794776
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:06.840816
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:06.888230
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.934308
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.980752
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.027007
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.076391
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.124225
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.171341
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.217540
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.265131
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0113
patience decreased: patience is now  2
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.679589
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.727974
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.774185
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.820380
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.867221
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.917104
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.964707
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:08.010985
[batch 180] samples: 11520, Training Loss: 0.0005
   Time since start: 0:00:08.058510
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.105573
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:08.152220
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.198474
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.243574
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.289741
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.340199
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.386113
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.431924
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.479366
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.526937
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.574008
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.620856
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.667479
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.713368
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.761824
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0106
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.203137
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.250678
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.298719
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.346630
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.393418
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.439589
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.486454
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.532691
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.581953
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.630385
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.677894
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.724923
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.771957
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.818618
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.867068
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.914726
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.960981
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.011097
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.058729
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:10.106637
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:10.152480
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.199933
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.246945
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.292468
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0124
patience decreased: patience is now  2
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.703861
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.750935
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.797613
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.843400
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.889056
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.935556
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.982356
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.031966
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.079234
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.125990
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.173388
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.218932
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.265199
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.312560
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.360193
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.409608
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.457495
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.502822
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:11.550029
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:11.596638
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.642453
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.689204
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.735911
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.781644
--m-Epoch 8 done.
   Training Loss: 0.0004
   Validation Loss: 0.0078
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.200414
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.250721
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.298486
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.345979
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.392787
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.439625
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.485344
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.530406
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.577540
[batch 200] samples: 12800, Training Loss: 0.0004
   Time since start: 0:00:12.625260
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:12.675464
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:12.723579
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.770393
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.817008
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.864123
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.910704
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.958207
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:13.004628
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:13.052700
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.102275
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.149072
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.196371
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.242696
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.288909
--m-Epoch 9 done.
   Training Loss: 0.0004
   Validation Loss: 0.0087
patience decreased: patience is now  2
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.734935
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.780050
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.826297
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.873206
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.920169
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.970377
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:14.016765
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:14.064621
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:14.111500
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.158735
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.205690
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:14.251396
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:14.297431
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.344723
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.393135
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.439627
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.486729
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.535226
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.580522
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.627372
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.674120
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.721038
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.770789
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.818412
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0084
patience decreased: patience is now  1
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:15.231863
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:15.279328
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:15.326520
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:15.374485
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:15.421916
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:15.468488
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:15.515744
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:15.561496
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:15.610818
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:15.658549
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:15.704986
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:15.751357
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:15.798897
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:15.844908
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:15.891559
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:15.937459
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:15.983824
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:16.033263
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:16.080891
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:16.129326
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:16.176584
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:16.223788
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:16.270543
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:16.315846
--m-Epoch 11 done.
   Training Loss: 0.0003
   Validation Loss: 0.0088
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
501   1.000000  1.000000  1.000000    48.000000     11     41
502   1.000000  1.000000  1.000000    48.000000     11     42
503   0.999617  0.999617  0.999617     0.999617     11      0
504   0.999214  0.999515  0.999359  7842.000000     11      1
505   0.999622  0.999617  0.999618  7842.000000     11      2

[506 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.138985
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.186041
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.232040
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.277711
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.325871
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.375204
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.422565
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.469565
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.514830
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.561304
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.607185
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.653488
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.699815
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.746816
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.797152
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.843494
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.889533
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.935152
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.981932
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.028860
[batch 420] samples: 26880, Training Loss: 0.0002
   Time since start: 0:00:01.075758
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:01.121878
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.168220
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.218765
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0159
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.685483
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.732836
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.778549
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.826470
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.874720
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.921466
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.968620
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.015254
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.064939
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.111883
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.160146
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.207762
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.253692
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.300841
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.348032
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.394586
[batch 340] samples: 21760, Training Loss: 0.0002
   Time since start: 0:00:02.440575
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:02.492084
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.539559
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.586318
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.632152
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.679509
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.725308
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.771792
--m-Epoch 2 done.
   Training Loss: 0.0004
   Validation Loss: 0.0093
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.236291
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.285775
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.332108
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.379184
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.425982
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.473324
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.520677
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.567600
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.616191
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.663398
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.712064
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.759725
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.807308
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.854656
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.902179
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.948321
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.994389
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.041277
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.089582
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.137998
[batch 420] samples: 26880, Training Loss: 0.0004
   Time since start: 0:00:04.185023
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:04.233138
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.279565
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.326181
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0095
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.785671
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.831940
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.879462
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.926289
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.977453
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:05.024253
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.070856
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.117729
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.164519
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.211490
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.258603
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.305445
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.353965
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.402509
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.450057
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.496449
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.542871
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.588236
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.633551
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.681196
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.728755
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.776709
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:05.827300
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:05.875211
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0097
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.327569
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.375616
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.421856
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.467486
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.513214
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.557612
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.603852
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.653634
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.702429
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:06.749779
[batch 220] samples: 14080, Training Loss: 0.0003
   Time since start: 0:00:06.796312
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:06.842383
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.888212
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.933886
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.980179
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:07.025299
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.074485
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.121369
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.169319
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.216974
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.262157
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.308931
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.356951
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.404356
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0101
patience decreased: patience is now  2
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.855372
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.906128
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.953833
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:08.001590
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:08.047799
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.094426
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.141378
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.187194
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.234367
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.280828
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.329504
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.377244
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.423779
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.471602
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.519658
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.566361
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.612184
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.658086
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.705903
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:08.755384
[batch 420] samples: 26880, Training Loss: 0.0003
   Time since start: 0:00:08.803105
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.849185
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.896809
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.943239
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0105
patience decreased: patience is now  1
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.382949
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.428459
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.475149
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.521952
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.570543
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.617048
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.663365
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.710226
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.757305
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.804345
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.851006
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.896240
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.941684
[batch 280] samples: 17920, Training Loss: 0.0007
   Time since start: 0:00:09.991170
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:10.038048
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:10.084954
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:10.131407
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.177932
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.224521
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.271622
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.317702
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.362666
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.411563
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.458695
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0111
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  1.000000  1.000000   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
317   1.000000  1.000000  1.000000    48.00000      7     41
318   1.000000  1.000000  1.000000    48.00000      7     42
319   0.999490  0.999490  0.999490     0.99949      7      0
320   0.999068  0.999453  0.999255  7842.00000      7      1
321   0.999496  0.999490  0.999491  7842.00000      7      2

[322 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.154861
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.205235
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.256067
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.303942
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.351807
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.399560
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.446372
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.492173
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.539431
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.586065
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.636744
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.683822
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.731524
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.779560
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.826430
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.873106
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.918855
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.966009
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:01.013396
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.063526
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:01.111098
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.157388
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.204099
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.249500
--m-Epoch 1 done.
   Training Loss: 0.0005
   Validation Loss: 0.0122
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.701587
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.748177
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.794204
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.840170
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.891140
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.937734
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.985637
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:02.031378
[batch 180] samples: 11520, Training Loss: 0.0004
   Time since start: 0:00:02.077466
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.124004
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.170155
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.216513
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.262871
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.311027
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.358301
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.404113
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.450298
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.495957
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.543513
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.590466
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.636703
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.682709
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.731798
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.778671
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0122
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.224422
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.271011
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.318350
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.365936
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.413205
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.460802
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.507404
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.555286
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.604047
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.650991
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.698745
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.746758
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:03.792996
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:03.839421
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.885695
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.932243
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.982112
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.028793
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.074953
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.121355
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.167380
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.214465
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.260199
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.306378
--m-Epoch 3 done.
   Training Loss: 0.0005
   Validation Loss: 0.0134
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.745272
[batch 40] samples: 2560, Training Loss: 0.0005
   Time since start: 0:00:04.793038
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.841348
[batch 80] samples: 5120, Training Loss: 0.0015
   Time since start: 0:00:04.889842
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.936716
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.982282
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.028107
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.075912
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.121718
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.168754
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.214991
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.264705
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.312960
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.360859
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.407488
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.456136
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.503389
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.550359
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.596889
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.642987
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.691412
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.738319
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.785997
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.832492
--m-Epoch 4 done.
   Training Loss: 0.0068
   Validation Loss: 0.0092
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.258461
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.306209
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.354390
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.402497
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.448689
[batch 120] samples: 7680, Training Loss: 0.1936
   Time since start: 0:00:06.498410
[batch 140] samples: 8960, Training Loss: 0.0005
   Time since start: 0:00:06.546385
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.594047
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.640934
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.688531
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.734560
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.780356
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.828220
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.874686
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.924991
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.974111
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.021885
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.068608
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.116820
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.163846
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.209950
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.257135
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.304740
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.353256
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0095
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.798113
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.843520
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.890428
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.935527
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.980772
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.028162
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.075146
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.121321
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.171084
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.219130
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.266044
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.312893
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.359337
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.405278
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.452111
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.498634
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.545190
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.593676
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.639956
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.686924
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.735002
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.782193
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.829745
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.877240
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0101
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0002
   Time since start: 0:00:09.316754
[batch 40] samples: 2560, Training Loss: 0.0005
   Time since start: 0:00:09.364436
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:09.414056
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.461684
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.509470
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.558272
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.604260
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.650066
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.697639
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.744655
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.789930
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.839298
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.886646
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.934451
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.981718
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:10.029212
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:10.075278
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.123141
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.168689
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.214186
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.263379
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.310852
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.357848
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.403685
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0094
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.832880
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.880168
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.927845
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.973901
[batch 100] samples: 6400, Training Loss: 0.0003
   Time since start: 0:00:11.020458
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:11.067227
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:11.118714
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.166928
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.214576
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.261923
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.308724
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.355959
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.402953
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.449652
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.499466
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.546851
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.593835
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.641730
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.688792
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.736716
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.784123
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.829685
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.875760
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.924568
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0095
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999745  0.999745  0.999745     0.999745      8      0
366   0.999276  0.999567  0.999417  7842.000000      8      1
367   0.999749  0.999745  0.999745  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.144615
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.193034
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.240043
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.287775
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.333729
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.382019
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.428463
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.477740
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.526938
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.575145
[batch 220] samples: 14080, Training Loss: 0.0007
   Time since start: 0:00:00.622732
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:00.668999
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.716081
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.762990
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.808750
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.854366
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.904035
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.951962
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.999486
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.045527
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.092977
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.139081
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.185814
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.232218
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0096
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.674153
[batch 40] samples: 2560, Training Loss: 0.0005
   Time since start: 0:00:01.725207
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:01.771776
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.817360
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.865140
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.911687
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.959724
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.006740
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.053787
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.099773
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.149077
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.196971
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.243057
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.289729
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.335696
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.382038
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.427933
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.475575
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.522305
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.572499
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.619640
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.667139
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.713679
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.761584
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0095
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.193086
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.241134
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.286261
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.334447
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.380653
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.430808
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.478717
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.527004
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.573697
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.621016
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.668483
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.716044
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.763521
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.814058
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.861164
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.907636
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.953729
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.000218
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.045883
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.092988
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.139186
[batch 440] samples: 28160, Training Loss: 0.0003
   Time since start: 0:00:04.184854
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:04.233776
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:04.281227
--m-Epoch 3 done.
   Training Loss: 0.0023
   Validation Loss: 0.2570
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.1414
   Time since start: 0:00:04.740016
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.787202
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.833678
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.881201
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.929537
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.976156
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.024344
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.073487
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.120357
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.167691
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.215323
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.263030
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.309692
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.356635
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.404307
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.450801
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.500009
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.547837
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.594760
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.641493
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.688599
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.735327
[batch 460] samples: 29440, Training Loss: 0.0004
   Time since start: 0:00:05.782792
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:05.829561
--m-Epoch 4 done.
   Training Loss: 0.0024
   Validation Loss: 0.0085
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.286349
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.336913
[batch 60] samples: 3840, Training Loss: 0.0007
   Time since start: 0:00:06.384576
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.432515
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:06.481092
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.527272
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.574217
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:06.621217
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.667197
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.712761
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.761785
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.808227
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.856296
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.904067
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.951156
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.998616
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.045249
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.092183
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.137516
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.185794
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.232779
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.278851
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.325068
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.372115
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0084
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.827154
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.874729
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.921161
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.967592
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:08.017600
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.065196
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.111858
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.158593
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.205468
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.253699
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.300261
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.348373
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:08.395430
[batch 280] samples: 17920, Training Loss: 0.0005
   Time since start: 0:00:08.444900
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.493131
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.539991
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.587113
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.634806
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.681767
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.730380
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.777296
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.826514
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.874059
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.921754
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0085
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.367016
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.414510
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.461714
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.508107
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.554379
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.600117
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.646939
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.696704
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.743788
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.791282
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.837269
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.884555
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.932805
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.979712
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:10.025478
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:10.072152
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:10.122385
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.169251
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.216293
[batch 400] samples: 25600, Training Loss: 0.0006
   Time since start: 0:00:10.262975
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:10.309133
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:10.354516
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.400735
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.446113
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0086
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.881672
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.930971
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.975692
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:11.019863
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:11.066382
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:11.113113
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:11.159301
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.206848
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.253798
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.299472
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.349637
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.395853
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.442458
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.487653
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.535272
[batch 320] samples: 20480, Training Loss: 0.0005
   Time since start: 0:00:11.580106
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:11.627203
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:11.673949
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:11.721133
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.770140
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.817237
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.866106
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.912035
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.958670
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0088
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999745  0.999745  0.999745     0.999745      8      0
366   0.999276  0.999567  0.999417  7842.000000      8      1
367   0.999749  0.999745  0.999745  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.173315
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.222482
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.269993
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.317742
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.363554
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.411371
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.461405
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.507917
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.556290
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.604503
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.650281
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.697278
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.743889
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.791298
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.838181
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.888005
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.935180
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.981980
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.028251
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.074473
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.121346
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.167172
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.213054
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:01.258942
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0093
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.675459
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.727015
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.774438
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.821903
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.868712
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.915035
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.961574
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.008078
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.054204
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.101144
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.151189
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.198956
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.247293
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.294149
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.341444
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.389404
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.435740
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.482895
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.529289
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.579498
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.625973
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:02.673906
[batch 460] samples: 29440, Training Loss: 0.0003
   Time since start: 0:00:02.720719
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.767287
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0093
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.198993
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.246444
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.294031
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.341789
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.391248
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.438404
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.484731
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.531909
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.577801
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.623760
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.670822
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.717477
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.764020
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.812116
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.860211
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.908019
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.954233
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.001126
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.048512
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.094629
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.140590
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:04.186327
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.235304
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.281374
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0093
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.695112
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.741669
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.789276
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.836658
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.884163
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.931713
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.978479
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.024659
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.073894
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.121912
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.170413
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.216008
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.263003
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.309871
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.356613
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.404094
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.450683
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.498326
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.544319
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.591993
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.639268
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:05.688064
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:05.733476
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.781124
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0094
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.193925
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.241867
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.289517
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.338452
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.385439
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.431690
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.479126
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.525352
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.571295
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.617540
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.665581
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.713236
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.760604
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.807357
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:06.853408
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:06.899650
[batch 340] samples: 21760, Training Loss: 0.0002
   Time since start: 0:00:06.947086
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:06.994424
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.040523
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.088133
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.134309
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.183938
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.229984
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.276583
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0092
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.688674
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.737237
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.783310
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.830715
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.877780
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.923740
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.969987
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.019094
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.068505
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.116300
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.163145
[batch 240] samples: 15360, Training Loss: 0.0003
   Time since start: 0:00:08.210834
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:08.257756
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.304715
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.350806
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.398439
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.447426
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.494637
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.542236
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.589213
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.634745
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.680790
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.728849
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.776203
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0092
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.173056
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.219398
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.267655
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.317897
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.365464
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.412974
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.461032
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.508748
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.555774
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.601437
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.647634
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.694447
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.744042
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.790126
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.836658
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.883101
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.928709
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.976498
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.023646
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.069666
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.115333
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.166214
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.212803
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.259914
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0094
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.691074
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.739557
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.787097
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.834437
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.880522
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.925807
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.971815
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:02.019602
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:02.065788
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.114319
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.160323
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.207238
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.255455
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.302198
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.347831
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.394833
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.443251
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.490336
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.536432
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.583790
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.630443
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.676708
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.724272
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.770072
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0089
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.184243
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.231722
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.281689
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.328450
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:03.375257
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:03.422863
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.469378
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.516020
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.561681
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.608816
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.655180
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.703735
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.751518
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.797626
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.845188
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.892805
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.939773
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.986966
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.032997
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.081263
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.128307
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.175262
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.221846
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.268656
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0084
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.686496
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.734001
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.780444
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.827872
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.875199
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.924511
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.970655
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.018315
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.064908
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.111583
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:05.157954
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:05.204279
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:05.250264
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.298069
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.345841
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.393018
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.439223
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.485770
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.532056
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.577490
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.623416
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.670247
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.717249
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.765729
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0079
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.179544
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.226795
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.273564
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.320358
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.368324
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.414147
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.461088
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.508843
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.555888
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.604652
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.652297
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.698950
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.744959
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.792268
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.838866
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.885503
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.930878
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.978014
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.025386
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.071210
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.117513
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.163611
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.209341
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:07.255969
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0086
patience decreased: patience is now  4
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:07.667316
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.714286
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.760459
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:07.808043
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.855044
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.905197
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.953370
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.001042
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.049604
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.096893
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:08.144357
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:08.191354
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:08.238325
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.284794
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.334613
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.381138
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.428277
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.474623
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.520160
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.566980
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.613749
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.660050
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.706192
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.756151
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0076
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.173306
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.220184
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.267409
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.313770
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.362094
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.409134
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.456799
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.504194
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.550790
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.600678
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.647414
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.695572
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.740998
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.787712
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.833494
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.879946
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.927021
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.974264
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.023870
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.070365
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.117343
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.165565
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.213261
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:10.261017
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0088
patience decreased: patience is now  4
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0009
   Time since start: 0:00:10.672940
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:10.719287
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.765188
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.811342
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.859430
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.907397
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.954767
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.001474
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.049841
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.096231
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.143042
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.190698
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.237435
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.285940
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.332785
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.379412
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.426274
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.472413
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.519328
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.566048
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.612747
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.658521
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.708523
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.756160
--m-Epoch 8 done.
   Training Loss: 0.0002
   Validation Loss: 0.0077
patience decreased: patience is now  3
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.183202
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.229804
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.276348
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.323955
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.371552
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.419012
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.466238
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.513366
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.562772
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.611680
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.659584
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.706540
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.752899
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.798997
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.845328
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.892544
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.938789
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.988300
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:13.035956
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.082737
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.130678
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.178780
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.225242
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.271847
--m-Epoch 9 done.
   Training Loss: 0.0005
   Validation Loss: 0.0069
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.689117
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.735804
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.781868
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.830672
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.877639
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.924851
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.972089
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:14.019018
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:14.065676
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.112921
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.159979
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.206402
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.255173
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.301887
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.349419
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.396335
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.442775
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.488634
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.535468
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:14.581607
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.627727
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.677447
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.724599
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.770773
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0066
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:15.196584
[batch 40] samples: 2560, Training Loss: 0.0004
   Time since start: 0:00:15.244185
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:15.291127
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:15.338162
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:15.384619
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:15.431961
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:15.478179
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:15.527878
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:15.576107
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:15.622956
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:15.668814
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:15.715183
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:15.761465
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:15.807948
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:15.853842
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:15.900243
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:15.949550
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:15.995374
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:16.041543
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:16.089224
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:16.135747
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:16.184102
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:16.231857
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:16.278730
--m-Epoch 11 done.
   Training Loss: 0.0003
   Validation Loss: 0.0063
Epoch: 12 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:16.711453
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:16.759806
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:16.807003
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:16.852837
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:16.899361
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:16.945941
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:16.993148
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:17.039946
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:17.086614
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:17.133056
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:17.181726
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:17.229726
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:17.276074
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:17.323313
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:17.370277
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:17.416590
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:17.463317
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:17.510321
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:17.556376
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:17.604990
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:17.652034
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:17.699052
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:17.745156
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:17.793078
--m-Epoch 12 done.
   Training Loss: 0.0003
   Validation Loss: 0.0064
patience decreased: patience is now  4
Epoch: 13 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:18.206322
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:18.254070
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:18.300553
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:18.347562
[batch 100] samples: 6400, Training Loss: 0.0008
   Time since start: 0:00:18.394585
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:18.440400
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:18.489118
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:18.535282
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:18.582198
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:18.629250
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:18.676893
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:18.722810
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:18.768666
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:18.814785
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:18.861907
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:18.911185
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:18.957698
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:19.005218
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:19.052776
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:19.098993
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:19.145208
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:19.192033
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:19.239864
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:19.285552
--m-Epoch 13 done.
   Training Loss: 0.0003
   Validation Loss: 0.0062
patience decreased: patience is now  3
Epoch: 14 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:19.753188
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:19.801158
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:19.848245
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:19.895386
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:19.942617
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:19.988928
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:20.035092
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:20.080782
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:20.127209
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:20.174378
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:20.222066
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:20.267727
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:20.315087
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:20.361169
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:20.408521
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:20.455326
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:20.501613
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:20.547331
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:20.596453
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:20.642653
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:20.688157
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:20.734483
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:20.782359
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:20.828762
--m-Epoch 14 done.
   Training Loss: 0.0004
   Validation Loss: 0.0067
patience decreased: patience is now  2
Epoch: 15 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:21.269707
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:21.316747
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:21.363861
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:21.412405
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:21.459714
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:21.507311
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:21.554718
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:21.601328
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:21.647460
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:21.693809
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:21.740350
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:21.786582
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:21.834603
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:21.880977
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:21.926601
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:21.972803
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:22.018856
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:22.065059
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:22.112012
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:22.157514
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:22.204035
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:22.250778
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:22.300840
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:22.347224
--m-Epoch 15 done.
   Training Loss: 0.0003
   Validation Loss: 0.0061
patience decreased: patience is now  1
Epoch: 16 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:22.759863
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:22.805498
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:22.853067
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:22.899492
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:22.946554
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:22.993887
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:23.041303
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:23.089317
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:23.138727
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:23.185734
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:23.231827
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:23.279388
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:23.326510
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:23.373106
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:23.419176
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:23.466386
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:23.514024
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:23.564496
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:23.610936
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:23.657633
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:23.704341
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:23.750893
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:23.797165
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:23.843417
--m-Epoch 16 done.
   Training Loss: 0.0004
   Validation Loss: 0.0096
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
731   1.000000  1.000000  1.000000    48.000000     16     41
732   1.000000  1.000000  1.000000    48.000000     16     42
733   0.999745  0.999745  0.999745     0.999745     16      0
734   0.999276  0.999567  0.999417  7842.000000     16      1
735   0.999749  0.999745  0.999745  7842.000000     16      2

[736 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.147151
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.193854
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.240830
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.287392
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.333939
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.381539
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426124
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473409
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.521377
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.569342
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.614437
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.662123
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.708851
[batch 280] samples: 17920, Training Loss: 0.0007
   Time since start: 0:00:00.755259
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:00.801798
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.849094
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.895737
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.944731
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.992131
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.038859
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.084933
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.131997
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.178837
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.226471
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0088
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.651552
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.698384
[batch 60] samples: 3840, Training Loss: 0.0009
   Time since start: 0:00:01.746281
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:01.794197
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:01.838872
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:01.885319
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.931049
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.978756
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.025247
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.072369
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.119135
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.166121
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.214653
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.261775
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.306849
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.353333
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.399673
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.446101
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.492313
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.540029
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.585733
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.633514
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.680114
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.726624
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0090
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.155403
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.202240
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.249874
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.296659
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.342492
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.390717
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.437529
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.484319
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.531250
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.577825
[batch 220] samples: 14080, Training Loss: 0.0004
   Time since start: 0:00:03.625259
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:03.673277
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.718612
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.764287
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.811597
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.856968
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.905002
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.951865
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.998138
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.045627
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.092431
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.138249
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.183974
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.230587
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0140
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.645124
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.691881
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.741331
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.789057
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.835890
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.884899
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.931353
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.977946
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.025478
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.070723
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.116997
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.164886
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.210374
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.255677
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.302447
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.348973
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.394632
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.441281
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.488871
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.535954
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.583390
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.629886
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.675561
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.721205
--m-Epoch 4 done.
   Training Loss: 0.0005
   Validation Loss: 0.0108
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.135143
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.183068
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.229782
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.276334
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.323847
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.371139
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.417310
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.465792
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.512180
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.560211
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.607903
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:06.653938
[batch 260] samples: 16640, Training Loss: 0.0020
   Time since start: 0:00:06.700648
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:06.747206
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.793469
[batch 320] samples: 20480, Training Loss: 0.0011
   Time since start: 0:00:06.840164
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.888903
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.935491
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.980983
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.026118
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.072718
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.119385
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.166702
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.212086
--m-Epoch 5 done.
   Training Loss: 0.0115
   Validation Loss: 0.0078
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.629988
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:07.676857
[batch 60] samples: 3840, Training Loss: 0.0003
   Time since start: 0:00:07.725990
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.773006
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.819359
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.866284
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.912237
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.958971
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.005821
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.052655
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.099603
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.147006
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.193087
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.238714
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.283752
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.331342
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.378199
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.424218
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.471140
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.517304
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.565047
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.612279
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.658672
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.707393
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0080
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.122056
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.169858
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.217829
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.265003
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.311291
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.357694
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.404790
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.450993
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.497031
[batch 200] samples: 12800, Training Loss: 0.0009
   Time since start: 0:00:09.544211
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.592570
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.640304
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.686193
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.733828
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.780324
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.828552
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.876062
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.922632
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.968739
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.016392
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.064834
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.112234
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.158357
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.205002
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0080
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.618747
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.666390
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.713847
[batch 80] samples: 5120, Training Loss: 0.0018
   Time since start: 0:00:10.760710
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:10.807054
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.854585
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.900536
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.945709
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:10.992078
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.037349
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.084462
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.131072
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.178237
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.225836
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.271820
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.318547
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.363881
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.409477
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.457932
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.504198
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.552083
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.598440
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.645227
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.691194
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0080
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999745  0.999745  0.999745     0.999745      8      0
366   0.999276  0.999567  0.999417  7842.000000      8      1
367   0.999749  0.999745  0.999745  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.145479
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.191575
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241205
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.287980
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334078
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.382047
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.428605
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:00.475428
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.523548
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.571325
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.618454
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.665540
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.712882
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.759043
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.805687
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.851905
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.898291
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.944887
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.991549
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.039815
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.087190
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.132945
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.179426
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.226437
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0080
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.646419
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.692969
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.740349
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.787897
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.835993
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.884154
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.932067
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.980637
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.027443
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.075159
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.120806
[batch 240] samples: 15360, Training Loss: 0.0012
   Time since start: 0:00:02.168173
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.217082
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.263716
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.310871
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.357285
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.403835
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.451012
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.499053
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.544149
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.590173
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.637504
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.684172
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.731620
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0081
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.169445
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.215008
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.262785
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.309746
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.355947
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.402980
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.449883
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.497580
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.543160
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.590237
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.637429
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.684345
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.731105
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.777820
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.824983
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.871198
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.919850
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.966862
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.013453
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.061078
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.105980
[batch 440] samples: 28160, Training Loss: 0.0005
   Time since start: 0:00:04.153473
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.199307
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.244495
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0081
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.658277
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.707130
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.755517
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.801655
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.848351
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.896046
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.944442
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.990467
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.037497
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.086017
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.132375
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.179888
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.226800
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.273644
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.320420
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.367662
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.415699
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.463014
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.508944
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.555657
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.603325
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.650950
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.696108
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.741976
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0089
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:06.162983
[batch 40] samples: 2560, Training Loss: 0.0003
   Time since start: 0:00:06.210169
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:06.257431
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.304558
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.351362
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.398772
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.447496
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.494676
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.540493
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.585683
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.632698
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.681384
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.728290
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.775619
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.822837
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.871143
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.917751
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.964910
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.011418
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.057661
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.104167
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.150481
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.197720
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.245208
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0082
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.660043
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.708647
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.756670
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.804060
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.851210
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.897697
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.945313
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.992955
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.039388
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.086745
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.134563
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.180560
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.227951
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.276005
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.321617
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.368234
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.414650
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.462233
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.508841
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.557269
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.607338
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.656697
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.702872
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.750489
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0141
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.147912
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.195204
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241512
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.289467
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.336806
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.382818
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431017
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.479761
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.527031
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.576022
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.622997
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.669113
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.716581
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.762602
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.810298
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.857219
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.904804
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.950749
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.999301
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:01.046594
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:01.094363
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:01.141935
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.189433
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.236047
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0082
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.655154
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.702026
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.748435
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.796060
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.843806
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.889860
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.938057
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.986985
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.034748
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.082505
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.130671
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.175709
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.222554
[batch 280] samples: 17920, Training Loss: 0.0005
   Time since start: 0:00:02.269647
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.317187
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.365569
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.412030
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.460472
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.507365
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.554324
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.600928
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.650159
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.696143
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.742779
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0081
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.155163
[batch 40] samples: 2560, Training Loss: 0.0002
   Time since start: 0:00:03.202188
[batch 60] samples: 3840, Training Loss: 0.0003
   Time since start: 0:00:03.249499
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.295687
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:03.343270
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.388725
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.436280
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.483431
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.531735
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.577432
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.625724
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.672891
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.718826
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.765827
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.812802
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.860568
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.905812
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.954241
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.001448
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.047954
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.094555
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.142030
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.189660
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.236422
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0077
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.648757
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.695856
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.742011
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.791063
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.839247
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.885730
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.933810
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.980231
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.026318
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.073112
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.122005
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.167140
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.214833
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.262665
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.309912
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.355766
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.401322
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.448044
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.494172
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.540510
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.587938
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.636592
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.684435
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.731441
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0084
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0004
   Time since start: 0:00:06.143164
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.191343
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.237572
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.284856
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.331928
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.378436
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.424460
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.473155
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.520379
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.568372
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.616763
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.663859
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.710470
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.759502
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.806213
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.851810
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.900636
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.947656
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.993988
[batch 400] samples: 25600, Training Loss: 0.0005
   Time since start: 0:00:07.040335
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.086977
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:07.134421
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.182081
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.229791
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0076
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.648797
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.695032
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.744243
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.792083
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.838400
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.885026
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.933770
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.982046
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.027464
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.075226
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.122539
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.170518
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.217409
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:08.264543
[batch 300] samples: 19200, Training Loss: 0.0006
   Time since start: 0:00:08.310596
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.358568
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.405384
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.453482
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.500544
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.546190
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.594556
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.641566
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.688605
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.735259
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0074
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.148875
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.196773
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.243740
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.289750
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.335751
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.383977
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.431486
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.478978
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.526927
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.572930
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.618910
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:09.664631
[batch 260] samples: 16640, Training Loss: 0.0004
   Time since start: 0:00:09.712084
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.757354
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.803916
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.851379
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.898304
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.944405
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.991982
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.038475
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.084672
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.131265
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.178728
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.223682
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0070
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.645833
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.694120
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.742539
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.788702
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:10.835282
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:10.882118
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.928334
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.973817
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.019502
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.065853
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.114871
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.163189
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.210708
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.257359
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.303442
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.350704
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.397509
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.443332
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.490948
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.539706
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.586356
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.633633
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.679838
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.726634
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0067
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.144256
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.190432
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:12.238429
[batch 80] samples: 5120, Training Loss: 0.0005
   Time since start: 0:00:12.285988
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:12.332516
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.382088
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.428579
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.475934
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.522127
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.569172
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.616739
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.663847
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.710737
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.758115
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.806000
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.852957
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.899863
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.948160
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.993754
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.039801
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.085918
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.132486
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.180003
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.227027
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0067
patience decreased: patience is now  4
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.636449
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.684348
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.730346
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.778550
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.825603
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.870825
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.918161
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.964432
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:14.012524
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.060509
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.109073
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.157298
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.204171
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.250916
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.299586
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.346465
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.392513
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:14.438730
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:14.486303
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.534949
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.580886
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.629005
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.675625
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.722750
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0067
patience decreased: patience is now  3
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:15.138765
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:15.185797
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:15.232770
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:15.280519
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:15.328376
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:15.375471
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:15.422482
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:15.469033
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:15.515864
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:15.563952
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:15.611920
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:15.658821
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:15.704899
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:15.752928
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:15.801253
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:15.848515
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:15.894267
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:15.941392
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:15.988932
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:16.035419
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:16.082184
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:16.127698
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:16.174014
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:16.222641
--m-Epoch 11 done.
   Training Loss: 0.0004
   Validation Loss: 0.0072
patience decreased: patience is now  2
Epoch: 12 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:16.639658
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:16.687232
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:16.733942
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:16.781382
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:16.828225
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:16.876012
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:16.923772
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:16.969691
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:17.016183
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:17.065332
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:17.110376
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:17.157898
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:17.204050
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:17.251031
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:17.297101
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:17.343889
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:17.390708
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:17.436317
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:17.485328
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:17.531111
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:17.578250
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:17.623713
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:17.670287
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:17.717750
--m-Epoch 12 done.
   Training Loss: 0.0004
   Validation Loss: 0.0092
patience decreased: patience is now  1
Epoch: 13 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:18.136191
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:18.183439
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:18.230012
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:18.277452
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:18.325321
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:18.371753
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:18.419074
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:18.466056
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:18.513092
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:18.559440
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:18.605589
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:18.650857
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:18.697854
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:18.746376
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:18.793890
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:18.842105
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:18.888470
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:18.934389
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:18.980421
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:19.027635
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:19.075162
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:19.121901
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:19.169353
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:19.217172
--m-Epoch 13 done.
   Training Loss: 0.0003
   Validation Loss: 0.0100
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
593   1.000000  1.000000  1.000000    48.000000     13     41
594   1.000000  1.000000  1.000000    48.000000     13     42
595   0.999745  0.999745  0.999745     0.999745     13      0
596   0.999276  0.999567  0.999417  7842.000000     13      1
597   0.999749  0.999745  0.999745  7842.000000     13      2

[598 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.138256
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.185458
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.235605
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.282660
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.328507
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.375025
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.422613
[batch 160] samples: 10240, Training Loss: 0.0004
   Time since start: 0:00:00.469131
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.515342
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:00.562412
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.610122
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.655927
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.702807
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.748867
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.796434
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.842864
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.888284
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.934286
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.981442
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.029780
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.078895
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.125972
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.172277
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.218322
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0103
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.643281
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.690339
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.736884
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.784115
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.831383
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.879723
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.925894
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.973441
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.020611
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.067269
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.113383
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.158820
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.205773
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.252219
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.299649
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.346353
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.391281
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.437775
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:02.485083
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.532622
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.580454
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.627272
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.674555
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.723991
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0108
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.137235
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.185666
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.233772
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.280000
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.327287
[batch 120] samples: 7680, Training Loss: 0.0003
   Time since start: 0:00:03.374050
[batch 140] samples: 8960, Training Loss: 0.0005
   Time since start: 0:00:03.419986
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:03.465338
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.512025
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.560314
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.607508
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.654756
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.702614
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.747223
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.794725
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.843063
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.889732
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.935635
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.984031
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.032422
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.080239
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.126535
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.172963
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.218771
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0103
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.635993
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.684257
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.731850
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.778136
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:04.826773
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.875260
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:04.921963
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.970767
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.017171
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.064267
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.110810
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.158762
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.204669
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.252694
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.299933
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.348473
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.395189
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.442020
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.487815
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.533442
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.579100
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.625614
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.672865
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.718825
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0108
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.134750
[batch 40] samples: 2560, Training Loss: 0.0003
   Time since start: 0:00:06.182158
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.229916
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:06.277044
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:06.323724
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.370927
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.417120
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.464496
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.512923
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.561403
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.609666
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.655463
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.703248
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.751280
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.798291
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.845598
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.892406
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.942986
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.990498
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.036595
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.082187
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.129124
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.175840
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.221974
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0106
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.638159
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.684600
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.731093
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.779746
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.826987
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.872840
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.919013
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.965311
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.011584
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.059695
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:08.106487
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.155007
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.203011
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.249680
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.296671
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.344478
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.391813
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.438358
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.484510
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.532168
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.578544
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.626993
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.676646
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.724721
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0111
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.141195
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.189931
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.236455
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.284720
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.332578
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.379859
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426227
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.472718
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.518893
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.565933
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.613104
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.661284
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.708757
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.756094
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.802518
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.848836
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.895983
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.941760
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.987304
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.036065
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.082720
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.129689
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.176352
[batch 480] samples: 30720, Training Loss: 0.0004
   Time since start: 0:00:01.222613
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0109
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.648700
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:01.695073
[batch 60] samples: 3840, Training Loss: 0.0006
   Time since start: 0:00:01.742570
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:01.789700
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:01.836811
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:01.886158
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.933907
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:01.980006
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.027139
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.073588
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.120314
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.166598
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.212528
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.259011
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.307572
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.355019
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.401292
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.448970
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.494721
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.542420
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.588795
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.635326
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.681518
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.730834
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0113
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.147336
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.195779
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.243500
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.291489
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:03.338871
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.386652
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.434116
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.481820
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:03.527571
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.576211
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.622151
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.669212
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.717736
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.764564
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.810007
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.856326
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.903529
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.949247
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.996157
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.042004
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.090075
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.138062
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.185974
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.235027
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0109
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.647818
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.695144
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.742332
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.789346
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.838790
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.886928
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.933263
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.979040
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.027390
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.074645
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.122775
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.170071
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.215765
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.264348
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.311497
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.359285
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.406882
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.453719
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.499624
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.546004
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.592356
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.637472
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.684558
[batch 480] samples: 30720, Training Loss: 0.1505
   Time since start: 0:00:05.731313
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0112
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0002
   Time since start: 0:00:06.146659
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.194119
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:06.241194
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.288244
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.335077
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.382641
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.429586
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.475837
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.523265
[batch 200] samples: 12800, Training Loss: 0.0005
   Time since start: 0:00:06.568514
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:06.615552
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.661820
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:06.707073
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.752884
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.801595
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:06.848108
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.893941
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.940652
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.989027
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.035356
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.083094
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.129851
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.176771
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.224230
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0111
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.640726
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.687359
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.736154
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.784877
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.832042
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.880845
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.928038
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.974808
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.020857
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.066299
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.114100
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.159886
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.207552
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.254438
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.300520
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.347722
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.394137
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.441731
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.487573
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.534229
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.580733
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.628051
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:08.675496
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:08.721519
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0121
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0002
   Time since start: 0:00:00.141644
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.187962
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:00.235383
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.281519
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.327243
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.375141
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.421696
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.469003
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:00.517015
[batch 200] samples: 12800, Training Loss: 0.0002
   Time since start: 0:00:00.563242
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.609661
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.655938
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.704480
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.751693
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.798861
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.845591
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.892581
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.938811
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.986933
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.032544
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.079682
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.125246
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.171183
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.218050
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0110
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.641232
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.687809
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.734360
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.780259
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.828748
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.876107
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.924160
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.970724
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.018700
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.064741
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.114181
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.161930
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.209798
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.259251
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.305021
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.352150
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.399845
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.446147
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.493264
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.539609
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.588373
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.635015
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.683463
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.730412
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0104
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.149487
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.198172
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.244110
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.289684
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.334877
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.380574
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.427455
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.475075
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.523225
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.570616
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.617276
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.665224
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.711373
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.758051
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.805325
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.853421
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.899674
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.947857
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.993794
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.039809
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.085962
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.132783
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.179795
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.226117
--m-Epoch 3 done.
   Training Loss: 0.0005
   Validation Loss: 0.0127
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.639231
[batch 40] samples: 2560, Training Loss: 0.0006
   Time since start: 0:00:04.687055
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:04.733664
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:04.782166
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.830206
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.876030
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.923826
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.970873
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.017587
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.063146
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.109998
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.156242
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.205240
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.252158
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.299742
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.346226
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.393152
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.440711
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.486989
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.532915
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.579127
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.626902
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.673947
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.721032
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0139
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.139203
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.186697
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.234225
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.281880
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.328496
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.375264
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.423949
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.471315
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.519155
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.565587
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.613089
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.658296
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.703169
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.749344
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.796365
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.844027
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.891225
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.936529
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.983212
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.030485
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.075804
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.121821
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.169652
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.217887
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0121
patience decreased: patience is now  2
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.633137
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.679254
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.727394
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.775537
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.822112
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.868756
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.915846
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.963428
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.010128
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.056256
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.103616
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.149746
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.195322
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.243096
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.290044
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.339544
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.385283
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.431281
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.477669
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.525342
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.571489
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.620091
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.666691
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.714155
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0210
patience decreased: patience is now  1
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.129422
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.176731
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.224128
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.270573
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.318329
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.366212
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.413335
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.460966
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.507450
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.554372
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.600517
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.646841
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.693403
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.739939
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.787184
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.836032
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.883963
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.932061
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.979262
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.026563
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.073768
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.120407
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:10.167357
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:10.216395
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0145
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
317   1.000000  1.000000  1.000000    48.000000      7     41
318   1.000000  1.000000  1.000000    48.000000      7     42
319   0.999745  0.999745  0.999745     0.999745      7      0
320   0.999276  0.999567  0.999417  7842.000000      7      1
321   0.999749  0.999745  0.999745  7842.000000      7      2

[322 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.142966
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.192927
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.240868
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.286801
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334636
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.380888
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.427551
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.476077
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.524464
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.572665
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.619705
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.666166
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.712724
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.759600
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.806354
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.852117
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.899776
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.946618
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.995429
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.041681
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.087538
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.133707
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.179538
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.226038
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0160
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.642480
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.690278
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.738777
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.786942
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.835730
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.882571
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.928817
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.975456
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.021542
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.069088
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.116109
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.163838
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.211989
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.258317
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.305776
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.351748
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.400373
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.446559
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.491835
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.538903
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.586024
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.632796
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.680565
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:02.725783
--m-Epoch 2 done.
   Training Loss: 0.0005
   Validation Loss: 0.0156
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:03.141241
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.189600
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.236346
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.284078
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.329680
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.375577
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.422617
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.470271
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.516563
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.563013
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.609612
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.658333
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.704469
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.751283
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.797423
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.846941
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.894307
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.940923
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.987577
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.034094
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.080161
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.126704
[batch 460] samples: 29440, Training Loss: 0.0007
   Time since start: 0:00:04.173363
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.220191
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.639596
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.689051
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.734755
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.780975
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.827761
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.874245
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.921664
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.966954
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.012959
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.059279
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.108174
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.155771
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.203057
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.250240
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.295903
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.341130
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.387793
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.434141
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.480015
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.529207
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.576089
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.622497
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.669118
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.716059
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0152
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.131188
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.177458
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.223818
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.271797
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.318991
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.367875
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.414236
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.460835
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.507562
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.552960
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.600436
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.646312
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.691744
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.739189
[batch 300] samples: 19200, Training Loss: 0.0006
   Time since start: 0:00:06.787506
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:06.834617
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.882042
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.928406
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.976124
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.024361
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.071426
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.118399
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.165661
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.213466
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0154
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.627134
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.674877
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.721173
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.767781
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.816083
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.863847
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.909946
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.956969
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.003166
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.050431
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.096736
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.143658
[batch 260] samples: 16640, Training Loss: 0.0002
   Time since start: 0:00:08.190818
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.237785
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:08.284597
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.332167
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.378082
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.424924
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.471065
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.518033
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.565755
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.611783
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.658322
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.705403
--m-Epoch 6 done.
   Training Loss: 0.0005
   Validation Loss: 0.0153
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.127083
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.174025
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.221236
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.269101
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.317341
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.364180
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.412064
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.460342
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.506467
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.553827
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.600633
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.647396
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.694948
[batch 280] samples: 17920, Training Loss: 0.0008
   Time since start: 0:00:09.743478
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.789823
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.835646
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.881351
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.927408
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.972661
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.018354
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.064786
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.111136
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.159852
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.207776
--m-Epoch 7 done.
   Training Loss: 0.0080
   Validation Loss: 0.0119
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.625335
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.671213
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.718694
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.766572
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.813046
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.861092
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.907553
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.954388
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.002877
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.050182
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.096755
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.142459
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.189523
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.236272
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.283828
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.330695
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.378714
[batch 360] samples: 23040, Training Loss: 0.0014
   Time since start: 0:00:11.425665
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:11.473145
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.520120
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.567888
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.616333
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.663772
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.710168
--m-Epoch 8 done.
   Training Loss: 0.0030
   Validation Loss: 0.0162
patience decreased: patience is now  2
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.122576
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.170672
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.219576
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.267666
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.316166
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.363005
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.409984
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.456626
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.502824
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.550359
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:12.597619
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:12.645766
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.692861
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.739769
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.788416
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.833918
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.879800
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.926534
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.972011
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.018248
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.065720
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.113540
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.161859
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.208714
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  1
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.623416
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.670840
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.718724
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.765081
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.811934
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.857810
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.904257
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.950541
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:13.997999
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.044467
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.091318
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.138134
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.184248
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.230542
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.276968
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.324032
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.370644
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.416873
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.464312
[batch 400] samples: 25600, Training Loss: 0.0011
   Time since start: 0:00:14.512009
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:14.559789
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.607872
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.654081
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.701951
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
455   1.000000  1.000000  1.000000    48.000000     10     41
456   1.000000  1.000000  1.000000    48.000000     10     42
457   0.999617  0.999617  0.999617     0.999617     10      0
458   0.999220  0.999515  0.999363  7842.000000     10      1
459   0.999622  0.999617  0.999618  7842.000000     10      2

[460 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.192017
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.237941
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.286734
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.336508
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.383135
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.430467
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.479629
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.527374
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.574183
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.621911
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.668210
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.716518
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.767337
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.814299
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.861650
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.910126
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.957834
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:01.004184
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.050201
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.096929
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.145181
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.193660
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.240933
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.287242
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0162
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.699474
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.744545
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.790688
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.838222
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.883991
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.931197
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.977959
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.027841
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.075429
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.120862
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.166871
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.213904
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.261577
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.308282
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.354905
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.402932
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.453207
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.500414
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.548128
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.594303
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:02.641187
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.687345
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.733826
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.779488
--m-Epoch 2 done.
   Training Loss: 0.0005
   Validation Loss: 0.0164
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.199225
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.247525
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.298048
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.345048
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.393152
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.441101
[batch 140] samples: 8960, Training Loss: 0.0007
   Time since start: 0:00:03.489824
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:03.537246
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:03.583112
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.629479
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.678964
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.726374
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.773561
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.820744
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.867971
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.916241
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.963174
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.009887
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.055697
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.104175
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.152937
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.200598
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.248025
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.295094
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0164
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.713697
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.762453
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.809498
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.856239
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.903775
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.953971
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.001377
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.048701
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.095648
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.144290
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.191307
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.237601
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.283799
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.330578
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.379122
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.425267
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.471368
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.516811
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.563909
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.609760
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.656512
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.703745
[batch 460] samples: 29440, Training Loss: 0.0010
   Time since start: 0:00:05.750034
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:05.799899
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0167
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.212581
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.259105
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.305058
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.351068
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.396325
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.444051
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.492068
[batch 160] samples: 10240, Training Loss: 0.0002
   Time since start: 0:00:06.538682
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:06.585664
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:06.634446
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.681177
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.729334
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.776573
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.824489
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.870937
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.917078
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.963969
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.010850
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.060716
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.106354
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.152315
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.198061
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.245534
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.293159
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.708752
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.756547
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.805384
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.853347
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.902987
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.949874
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:07.996161
[batch 160] samples: 10240, Training Loss: 0.0002
   Time since start: 0:00:08.043015
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.088787
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.135501
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:08.182195
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.228874
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.275535
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.324817
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.370775
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.416867
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.462671
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.509480
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.556880
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.604160
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.651442
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.697690
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.747120
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.794717
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999220  0.999515  0.999363  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.146864
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.194682
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241192
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.290679
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337786
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.386084
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.432707
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.479496
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.526938
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.576514
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.623272
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.670721
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.716919
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.763739
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.810601
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.858270
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.903213
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.949874
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.997397
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.043668
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.091004
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.137226
[batch 460] samples: 29440, Training Loss: 0.0009
   Time since start: 0:00:01.184840
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.233045
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0166
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.665579
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.712706
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.758889
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.804970
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.854223
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.901055
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.947684
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.992964
[batch 180] samples: 11520, Training Loss: 0.0003
   Time since start: 0:00:02.038975
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.085960
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.132110
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.177980
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.224679
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.272830
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.320459
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.367967
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.415935
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.463140
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.508641
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.554948
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.601238
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.648812
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.697658
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.745670
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0164
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.159048
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.206388
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.254839
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.302646
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.348782
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.394942
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.441398
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.488128
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:03.537550
[batch 200] samples: 12800, Training Loss: 0.0002
   Time since start: 0:00:03.585626
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.633457
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.680497
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:03.728855
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.777237
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.826135
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.873261
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.921408
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.969711
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.015284
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.062078
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.110644
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.156888
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.202518
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.249706
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0162
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.685605
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.732730
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.781179
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.828921
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.876102
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.920537
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.967754
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.014922
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.062011
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.108546
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.155195
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.204224
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.250870
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.297965
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.344517
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.392899
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.440629
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.488513
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.536913
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.584817
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.640439
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.691761
[batch 460] samples: 29440, Training Loss: 0.0003
   Time since start: 0:00:05.738863
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:05.784930
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0163
patience decreased: patience is now  4
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.214143
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.262729
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.310141
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.357691
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.405264
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.453978
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.500853
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.547904
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:06.594939
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:06.641841
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.687840
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.736094
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.782521
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.829014
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.877362
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.926334
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.973224
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.020452
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.067258
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.113875
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.160140
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.207329
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.254197
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.302505
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0158
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.720482
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.768451
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.814110
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.860162
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.907173
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.953872
[batch 140] samples: 8960, Training Loss: 0.0003
   Time since start: 0:00:08.000965
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:08.049006
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.095550
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.145606
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.191354
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.238614
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.283859
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.330747
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.376346
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.422330
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.469887
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.515520
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.565606
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.612848
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.660223
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.707146
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.755105
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.801520
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0157
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.223720
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.270874
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.318236
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.364918
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:09.413863
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:09.460985
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.508208
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.553646
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.600922
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.648300
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.695793
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.742029
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.788596
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.838652
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.885310
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.931605
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.978263
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:10.025155
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.072674
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.120024
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.166038
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.213356
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.262510
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.309350
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0156
patience decreased: patience is now  3
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.724224
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.771194
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.818849
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.865684
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.911874
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.958944
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:11.005750
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.053770
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.103682
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.150494
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.196832
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.243878
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.289245
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.335257
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.381755
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.428517
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.475594
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.524589
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.571304
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.617463
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.663775
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.709890
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.756695
[batch 480] samples: 30720, Training Loss: 0.0003
   Time since start: 0:00:11.802540
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0159
patience decreased: patience is now  2
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:12.215801
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:12.264715
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:12.311429
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.360546
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.407418
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.453879
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.499430
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.547966
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.595637
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.642462
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.688351
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.735003
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.783018
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.831123
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.878896
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.926084
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.972721
[batch 360] samples: 23040, Training Loss: 0.0005
   Time since start: 0:00:13.018806
[batch 380] samples: 24320, Training Loss: 0.0004
   Time since start: 0:00:13.065136
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:13.111882
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:13.162463
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.209644
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.255153
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.301323
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0154
patience decreased: patience is now  1
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.720714
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.767812
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.814618
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.860873
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.907399
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.954169
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:14.002772
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:14.051676
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:14.099180
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.145619
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.191803
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.239516
[batch 260] samples: 16640, Training Loss: 0.0003
   Time since start: 0:00:14.285444
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:14.332269
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.378786
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.427919
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.474698
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.521407
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.569712
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.615701
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.662555
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.708340
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.753845
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.801561
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0155
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
455   1.000000  1.000000  1.000000    48.000000     10     41
456   1.000000  1.000000  1.000000    48.000000     10     42
457   0.999617  0.999617  0.999617     0.999617     10      0
458   0.999220  0.999515  0.999363  7842.000000     10      1
459   0.999622  0.999617  0.999618  7842.000000     10      2

[460 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.142426
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.190474
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.238078
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.284907
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.331222
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.380009
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.425599
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473674
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.520499
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.565717
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.612971
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.659733
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.707304
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.754788
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.801756
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.846820
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.893111
[batch 360] samples: 23040, Training Loss: 0.0003
   Time since start: 0:00:00.939861
[batch 380] samples: 24320, Training Loss: 0.0006
   Time since start: 0:00:00.986350
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.031383
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:01.077585
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.123690
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.169582
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.218992
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0156
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.654726
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.703016
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.747928
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.794941
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.842393
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.888354
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.934678
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.981109
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.029102
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.075512
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.123523
[batch 240] samples: 15360, Training Loss: 0.0005
   Time since start: 0:00:02.168803
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:02.215060
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:02.261417
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.306499
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.353142
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.398980
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.447255
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.493223
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.541251
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.588631
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.635002
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.681774
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.727903
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0149
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.145267
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.193149
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.240252
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.289008
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.335622
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.383041
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.429341
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.477112
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.523881
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.571108
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.618723
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:03.665484
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:03.713949
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.761807
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.808554
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:03.854916
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.900052
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.947173
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.993365
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.040039
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.087121
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.141271
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.187801
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.234155
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0158
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.649782
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.696635
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.743987
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.790154
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.835184
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.881132
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.927494
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.977225
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.022789
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.069683
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.116212
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.161651
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.208509
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.255030
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.302853
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.349537
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:05.398252
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:05.446865
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.494904
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.543214
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.589458
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.635130
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.681869
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.727722
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0163
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.146212
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.194521
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.245511
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.292638
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.340431
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.388924
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.436409
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.485115
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.531785
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.578115
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.624667
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:06.675899
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.723360
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.769626
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.814084
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.860245
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.906807
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.953712
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.001836
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.047532
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.096395
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.142973
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.189504
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.235870
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0158
patience decreased: patience is now  2
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.649093
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.695548
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.742720
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.790349
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.837548
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.885264
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.934839
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.981519
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:08.027210
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:08.073931
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:08.121350
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.168985
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.215755
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.262882
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.309194
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.357889
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.404261
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.450530
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.496311
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.542535
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.590215
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.636733
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.682616
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.729249
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0161
patience decreased: patience is now  1
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.174863
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.221737
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.268348
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.315375
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.363166
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.409989
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.456038
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.502272
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.548942
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.597553
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.644511
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.690171
[batch 260] samples: 16640, Training Loss: 0.0003
   Time since start: 0:00:09.737340
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:09.782319
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.829361
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.876934
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:09.922943
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.968334
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.017963
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.064837
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.112538
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.158048
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.205215
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.253972
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0164
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
317   1.000000  1.000000  1.000000    48.000000      7     41
318   1.000000  1.000000  1.000000    48.000000      7     42
319   0.999617  0.999617  0.999617     0.999617      7      0
320   0.999220  0.999515  0.999363  7842.000000      7      1
321   0.999622  0.999617  0.999618  7842.000000      7      2

[322 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:00.143875
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:00.190466
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:00.238149
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.285892
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:00.334907
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.381776
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431587
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.478893
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.526790
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.573876
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.619864
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.666006
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.713875
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.762000
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.809649
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.859799
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.906157
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.953569
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.002119
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.047564
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.094299
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.140790
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.187900
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.236607
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0162
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.657558
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.705438
[batch 60] samples: 3840, Training Loss: 0.1399
   Time since start: 0:00:01.752542
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.799193
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:01.845112
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:01.893177
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.938936
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.985062
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.032312
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.080374
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.127066
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.173728
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.220610
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.267466
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.314063
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.360518
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.407303
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.454742
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.502815
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.549060
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.595313
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.640877
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.688620
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.735593
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0163
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.153875
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.202297
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.250560
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.297787
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.347387
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.395458
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.443667
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.492100
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.540497
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.586719
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.634384
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.680073
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.727263
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.777762
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.825782
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.874050
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.922001
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.968284
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.015178
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.061154
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.107761
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:04.154373
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:04.203650
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:04.251307
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0164
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:05.603004
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:05.649535
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:05.696138
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:05.742702
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:05.787846
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:05.834003
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:05.882185
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.929257
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.976099
[batch 200] samples: 12800, Training Loss: 0.0003
   Time since start: 0:00:06.023319
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:06.070101
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.118579
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:06.164956
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.211114
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.257554
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.306983
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.353731
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.399270
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.445578
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:06.493091
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:06.539588
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:06.587040
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:06.633586
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:06.679749
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0164
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.097014
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.146097
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.190896
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.235921
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.282039
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.327916
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.375333
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.422278
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:07.469694
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:07.516536
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:07.566560
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:07.613459
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:07.661001
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:07.706656
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:07.754891
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:07.802740
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:07.849045
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:07.895672
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.941421
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.990718
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.038394
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.086128
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.133044
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.179104
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0167
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:08.598913
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:08.645426
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:08.692699
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:08.738981
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:08.786879
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:08.835887
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:08.881838
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.929338
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.977083
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.023567
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.070572
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:09.116137
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.161799
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:09.210877
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.257366
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.305135
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.352213
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.399404
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.446427
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:09.491161
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:09.538909
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:09.585121
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:09.633086
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:09.681753
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0167
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999220  0.999515  0.999363  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.150527
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.199509
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.248437
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.294749
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.343369
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.392278
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.442333
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.489041
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.535360
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.582166
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.628271
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.674542
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.722160
[batch 280] samples: 17920, Training Loss: 0.0003
   Time since start: 0:00:00.768308
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:00.813428
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:00.862368
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.909388
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.958461
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.005803
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.052637
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.099583
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.145437
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.192344
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.240247
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0170
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.660284
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.711440
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:01.758512
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.806844
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.852333
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.899106
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.946547
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.992740
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.039597
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.087322
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.136069
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.183699
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.230853
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.278503
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.325618
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.371571
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.418024
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.464379
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.511101
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.559883
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.607064
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.652878
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.698964
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.745386
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0171
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.163760
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.210771
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.257682
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.305115
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.352876
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.400911
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.446051
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.492456
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.539157
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.586864
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.632647
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.680161
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.727198
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.778125
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.826638
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.871937
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.918050
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.964733
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.011431
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.058159
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.104541
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.151276
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.199215
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.246622
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0173
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.660900
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.707914
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.754244
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.800145
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.848170
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.894409
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.941607
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.987939
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.035716
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.084484
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.132160
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.179991
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:05.227498
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.274823
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.321729
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.369120
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.416164
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.461947
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.509682
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.556997
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.605551
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.652083
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.699399
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.745280
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0176
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.163202
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.210870
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.257977
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.303969
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:06.353604
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.400950
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.447354
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.494138
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.540465
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.587074
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.633000
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.679675
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.727256
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.778267
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.824416
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.871253
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.917635
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.965066
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.011798
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.056261
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.102084
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.148234
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.198265
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.245592
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0189
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.660537
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.707842
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.754924
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.800805
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.848087
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.894394
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.941144
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.988000
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.038277
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.083958
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.130844
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.179607
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.227363
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.274272
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.321376
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.367945
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.413957
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.463773
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.512248
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.558558
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.606370
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.652676
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.698451
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.743983
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0195
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999214  0.999515  0.999359  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.1350
   Time since start: 0:00:00.141774
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.187617
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.234367
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.280876
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.327729
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.377174
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.424964
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.470374
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.516601
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.563746
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.611027
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.657700
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.704154
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.750472
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.799892
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.847045
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.895004
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.941353
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.988999
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.036210
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.083067
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.130426
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.176770
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.225569
--m-Epoch 1 done.
   Training Loss: 0.0018
   Validation Loss: 0.0128
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.656569
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.703476
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.748278
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.794945
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.841413
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.889432
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.937175
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.982560
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.028378
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.075925
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.122664
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.169343
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.216782
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.264220
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.310229
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.357713
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.404016
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.451930
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.500863
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.547211
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.593766
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.640023
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.684885
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.732241
--m-Epoch 2 done.
   Training Loss: 0.0005
   Validation Loss: 0.0135
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.145457
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.190906
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.237193
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.285059
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.333731
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.382146
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.428656
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.477004
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.524044
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.571945
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.617841
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:03.665988
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.712433
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.760645
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.807574
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.853375
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.900105
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.946656
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.992995
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.039881
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.088821
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.135808
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.184866
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.232752
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0135
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.646661
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.694708
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.742635
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.789820
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.836551
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.882846
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.931064
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.977580
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.025766
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.072774
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.120398
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.166740
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.212601
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.259889
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.306440
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.353448
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.399435
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.449511
[batch 380] samples: 24320, Training Loss: 0.0005
   Time since start: 0:00:05.496845
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:05.542881
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:05.588616
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.635416
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.682121
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.728849
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.143245
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.189468
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.236053
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.284677
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.331489
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.376858
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.423210
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.469846
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.516386
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.563329
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.609898
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.656247
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.704777
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.752189
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.798137
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.845206
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.893083
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.939586
[batch 380] samples: 24320, Training Loss: 0.0004
   Time since start: 0:00:06.986228
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:07.033754
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:07.080600
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.129930
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:07.176632
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.223830
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.646159
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.693228
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.739872
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.785646
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.834259
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.881564
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.927754
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.976899
[batch 180] samples: 11520, Training Loss: 0.0003
   Time since start: 0:00:08.024501
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:08.069948
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:08.116295
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.162909
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.209680
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.256953
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.303610
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.351100
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.399866
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.446260
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.493674
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.540634
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.586503
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.633477
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.678372
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.725778
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0135
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.139055
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.184805
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.233006
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.279369
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.327048
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.375181
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.422349
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.469358
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.516769
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.563862
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.611696
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.658515
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.705509
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.751636
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.802120
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.849482
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.896110
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.942690
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.988409
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.035084
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.081480
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.128760
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.174926
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:01.223260
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0144
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.644938
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.691686
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.737942
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.784576
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.829553
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.875165
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.921147
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.966775
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.013247
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.063033
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.109670
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.156516
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.203843
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.250328
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.298113
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.346267
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.392208
[batch 360] samples: 23040, Training Loss: 0.0004
   Time since start: 0:00:02.441045
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:02.491109
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.537676
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:02.585187
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.631721
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.680198
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.728842
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.151895
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.197526
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.244773
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.292404
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.342001
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.389856
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.436633
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.484029
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.530523
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.577356
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.624699
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.671159
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.718266
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.767179
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.814552
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.861214
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.908645
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.955554
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.001903
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.048449
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.095134
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.141236
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.189621
[batch 480] samples: 30720, Training Loss: 0.0009
   Time since start: 0:00:04.237138
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0142
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.651499
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:04.698992
[batch 60] samples: 3840, Training Loss: 0.0001
   Time since start: 0:00:04.746032
[batch 80] samples: 5120, Training Loss: 0.0003
   Time since start: 0:00:04.791843
[batch 100] samples: 6400, Training Loss: 0.0004
   Time since start: 0:00:04.839447
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:04.885593
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:04.931972
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.978176
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.025477
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.073413
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.121704
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.168471
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.214755
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.262248
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.308446
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.353815
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.400157
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.450301
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.495530
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.543520
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.589276
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.637168
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.683670
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.730532
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0132
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.144761
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.192220
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.239504
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.289145
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.337019
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.384041
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.430771
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.477509
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.524518
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.570813
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.617297
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.664116
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.712624
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.759805
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.808632
[batch 320] samples: 20480, Training Loss: 0.0002
   Time since start: 0:00:06.854171
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.900366
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:06.947005
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.994048
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.040073
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.087212
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.135708
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.183439
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.230788
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0134
patience decreased: patience is now  4
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.649602
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.696189
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.742810
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.788965
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.835601
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.882976
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:07.931910
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:07.980805
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.027171
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.073218
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.121821
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.167546
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.214837
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.260655
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.305086
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.350424
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.398248
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.445616
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.492216
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.538519
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.584621
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.632132
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.679332
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.726679
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0130
patience decreased: patience is now  3
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.150134
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.197138
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.246077
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.293312
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.340015
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.385905
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.431210
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.478349
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.525070
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.570942
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.617622
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.665359
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.710830
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.756958
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.802775
[batch 320] samples: 20480, Training Loss: 0.0006
   Time since start: 0:00:09.850404
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.896370
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.943037
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.990147
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.035964
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.085896
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.132835
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.180025
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.226928
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0131
patience decreased: patience is now  2
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.645218
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.692002
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.738048
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.784672
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:10.831880
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.879231
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.928527
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.976479
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.022851
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.069273
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.114679
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.160930
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.208747
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.254819
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.302450
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.352636
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.399298
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.446835
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.495827
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.541988
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.587395
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.631809
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.676702
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.724041
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0132
patience decreased: patience is now  1
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.139657
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.188211
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.235310
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.281673
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.329789
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.377421
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.424251
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.471766
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.518663
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.565640
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.615843
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.662172
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.708857
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.754484
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.801734
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.847003
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.893098
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:12.938955
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:12.985491
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.033960
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.080371
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.128139
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.174961
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.220689
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0137
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
409   1.000000  1.000000  1.000000    48.000000      9     41
410   1.000000  1.000000  1.000000    48.000000      9     42
411   0.999745  0.999745  0.999745     0.999745      9      0
412   0.999276  0.999567  0.999417  7842.000000      9      1
413   0.999749  0.999745  0.999745  7842.000000      9      2

[414 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.153364
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.202698
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.249570
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.297096
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.343467
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.389673
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.438690
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.488099
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.535206
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.581640
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.629159
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.675833
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.722530
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.768610
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.814802
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.863862
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.910933
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.957714
[batch 380] samples: 24320, Training Loss: 0.0003
   Time since start: 0:00:01.003356
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:01.048912
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.095391
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.143895
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.189666
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.236230
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0144
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.655027
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.702903
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.749542
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.795514
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.841201
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.889362
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.936266
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.983401
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.030544
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.077277
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.127462
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.175566
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.223660
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.272055
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.318731
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.365030
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.411495
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.458809
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.505070
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:02.553975
[batch 420] samples: 26880, Training Loss: 0.0003
   Time since start: 0:00:02.600969
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.647763
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.693772
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.739697
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0154
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.160536
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.207707
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.254885
[batch 80] samples: 5120, Training Loss: 0.0006
   Time since start: 0:00:03.301412
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.347229
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.395833
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:03.442610
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.489101
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.535940
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.582582
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.628961
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.676718
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.723932
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.770243
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.818982
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.865463
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.913038
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.959190
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.007504
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.055018
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.102987
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.150600
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.196962
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.246351
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0155
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.659140
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.706092
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.753251
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.800678
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.848060
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.895339
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.941624
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.988031
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.034435
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.082566
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.130192
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.176376
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.223967
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.270406
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.316746
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.363447
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.411887
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.459346
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.509014
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.555934
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:05.603301
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:05.651453
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.697949
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.745212
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0152
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.160147
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.207985
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.252981
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.300029
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.350471
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.396419
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.444288
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.491184
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.537468
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.583124
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.630706
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.677826
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.725878
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.774863
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.824398
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.869914
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.917108
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.963178
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.009741
[batch 400] samples: 25600, Training Loss: 0.0006
   Time since start: 0:00:07.056024
[batch 420] samples: 26880, Training Loss: 0.0002
   Time since start: 0:00:07.102211
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.148096
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.196757
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.244306
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0147
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.657996
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.703436
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.749837
[batch 80] samples: 5120, Training Loss: 0.1239
   Time since start: 0:00:07.797372
[batch 100] samples: 6400, Training Loss: 0.0005
   Time since start: 0:00:07.844966
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:07.891188
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:07.937926
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.984964
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.034876
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.081568
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.127761
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.175047
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.221242
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.267752
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.315399
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.362746
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.411102
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.460301
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.506301
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.552947
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.600593
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.647971
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.694840
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.742317
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0158
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999745  0.999745  0.999745     0.999745      6      0
274   0.999276  0.999567  0.999417  7842.000000      6      1
275   0.999749  0.999745  0.999745  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.137447
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.185090
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.232111
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.279777
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.329561
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.377752
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.425996
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.473335
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.519771
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.566368
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.613919
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.660342
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.707683
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:00.755139
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.802409
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.848745
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.895494
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.942403
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.989613
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.036547
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.082227
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.128508
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.176743
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.223336
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0162
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.646788
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.693601
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.740167
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.787593
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.833815
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.882066
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.930065
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.976177
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.023976
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.072003
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.118460
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.165176
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.211342
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.258642
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.305384
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.353070
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.399245
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.447476
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.492426
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.539131
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.586563
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.634612
[batch 460] samples: 29440, Training Loss: 0.0003
   Time since start: 0:00:02.680725
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:02.727539
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0168
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.140080
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.187124
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.234864
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.286112
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.331872
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.379183
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.426731
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.473788
[batch 180] samples: 11520, Training Loss: 0.0004
   Time since start: 0:00:03.521323
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.567786
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.614793
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:03.662544
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.710635
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.757510
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.803963
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.849846
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.896473
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.943293
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.989399
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.036415
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.082737
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.129321
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.175210
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.220813
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.636366
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.683387
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.729714
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.776930
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:04.824670
[batch 120] samples: 7680, Training Loss: 0.0003
   Time since start: 0:00:04.872090
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:04.919435
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.965550
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.014265
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.060054
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.105341
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.150902
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.197592
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.242778
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.289645
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.335617
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.382524
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.431730
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.479874
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.525880
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.572992
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.620773
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.668560
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.716732
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0163
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.132899
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.179735
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.228018
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.278879
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.325098
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.371276
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.417890
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.465219
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.511313
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.557536
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.604517
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.651133
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.700114
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.747676
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.794150
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.841452
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.885949
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.931931
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:06.977828
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:07.024803
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.071299
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.120579
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.167800
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.212630
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0165
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.637883
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.685076
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.730344
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.776823
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.822689
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.868984
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.915715
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.963821
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.009767
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.055901
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.103300
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.150373
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.197258
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.244177
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.290967
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.337850
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.386973
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.434942
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:08.482035
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:08.528874
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.575529
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.621512
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.668239
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.715194
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0155
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.137073
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.184612
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.232916
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.279682
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.326395
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.373106
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.419421
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.465250
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.512424
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.558018
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.605137
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.653372
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.700275
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.746641
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.793203
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.840060
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.887411
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.933995
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.980903
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.027617
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.077125
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.124624
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.171278
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.217344
--m-Epoch 7 done.
   Training Loss: 0.0005
   Validation Loss: 0.0161
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.633076
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:10.680216
[batch 60] samples: 3840, Training Loss: 0.0002
   Time since start: 0:00:10.727687
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:10.776140
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:10.823577
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:10.871278
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.920555
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.968703
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.016140
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.062318
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.109577
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.156917
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.203242
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.250431
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.295902
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.344501
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.391136
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.438288
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.486133
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.532919
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.580504
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.626561
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.672606
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.719407
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0159
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999745  0.999745  0.999745     0.999745      8      0
366   0.999276  0.999567  0.999417  7842.000000      8      1
367   0.999749  0.999745  0.999745  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.149078
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.196536
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.244463
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.291646
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337700
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.386950
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.438035
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.485937
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.532133
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.578517
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.626579
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.671855
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.717927
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.763770
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.811471
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.860543
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.907690
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.955062
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.000252
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.046700
[batch 420] samples: 26880, Training Loss: 0.0002
   Time since start: 0:00:01.091883
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:01.138560
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:01.184083
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.230288
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0162
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.652534
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.701216
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.747339
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.794405
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.840014
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.886306
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.932073
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.979232
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.024596
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.070049
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.120893
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.168404
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.214208
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.260954
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.307158
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.354074
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:02.399844
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.446223
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.492755
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.541057
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.588328
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.634945
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.682354
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.728770
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0161
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.145566
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.192202
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.239613
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.285514
[batch 100] samples: 6400, Training Loss: 0.0003
   Time since start: 0:00:03.332375
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:03.380207
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.427531
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.474012
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.522304
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.567771
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.613740
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.660388
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.706783
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.752541
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.801181
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.847327
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.894023
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.939502
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.986604
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.033710
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.080471
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.126106
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.172390
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.220332
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0157
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.634734
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.685100
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.733135
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.779135
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.824646
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.871754
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.917783
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.964191
[batch 180] samples: 11520, Training Loss: 0.0003
   Time since start: 0:00:05.011252
[batch 200] samples: 12800, Training Loss: 0.0002
   Time since start: 0:00:05.059066
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.107569
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.154532
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.200381
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.246443
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.293105
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.339141
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.386937
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.433958
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.480289
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.529449
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.576240
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.623819
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.670910
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.718539
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0149
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.136193
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.183746
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.231353
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.278523
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.325583
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.374369
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:06.419637
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:06.466759
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.513053
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.560306
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.605564
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.651728
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.696877
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.742617
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.791047
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.837970
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.884423
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.930490
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.977306
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.023566
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.070161
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.117194
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.163815
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.211938
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0133
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.628163
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.675322
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.721504
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.769400
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.815403
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.862717
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.911171
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.958527
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.003671
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.051831
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.097902
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.143722
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.188937
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.235385
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.281477
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.326066
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.371416
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.416930
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.463696
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.511657
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.558714
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.605989
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.653131
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.698498
--m-Epoch 6 done.
   Training Loss: 0.0024
   Validation Loss: 0.0158
patience decreased: patience is now  4
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.114710
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.161231
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.208536
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.254669
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.304592
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.350085
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.397635
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.444061
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.491010
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.538780
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.586428
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.632760
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.678328
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.726528
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.775333
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.822217
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.869983
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.915953
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.964889
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:10.011333
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:10.057533
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:10.104691
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.155163
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.201786
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0159
patience decreased: patience is now  3
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.616235
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.662882
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.710552
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.756240
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.804372
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.852092
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.899034
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.948279
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:10.997920
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.043886
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.090822
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.138341
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.184690
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.230274
[batch 300] samples: 19200, Training Loss: 0.0009
   Time since start: 0:00:11.276225
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.323436
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:11.369209
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:11.418379
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.464995
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.512736
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.559006
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.605853
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.651882
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.697627
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0157
patience decreased: patience is now  2
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.114223
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.161193
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.207613
[batch 80] samples: 5120, Training Loss: 0.0008
   Time since start: 0:00:12.256412
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:12.303689
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:12.349659
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.395100
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.441201
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.488537
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.534305
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.580983
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.626195
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.672286
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.720264
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.766308
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.812043
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.859251
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.904655
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.951806
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:12.997420
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.044005
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.090346
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.139036
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.186153
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0155
patience decreased: patience is now  1
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.597304
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.643079
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.691287
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.738202
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.784283
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.831948
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.877759
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.925827
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:13.974550
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.021443
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.066966
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.114745
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.160892
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.209326
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.256143
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.302461
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.349078
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.398238
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.444344
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.491510
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.538284
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.585082
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.632008
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.677830
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0154
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
455   1.000000  1.000000  1.000000    48.000000     10     41
456   1.000000  1.000000  1.000000    48.000000     10     42
457   0.999617  0.999617  0.999617     0.999617     10      0
458   0.999221  0.999515  0.999363  7842.000000     10      1
459   0.999622  0.999617  0.999618  7842.000000     10      2

[460 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:00.147024
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.197392
[batch 60] samples: 3840, Training Loss: 0.0010
   Time since start: 0:00:00.244279
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:00.291512
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.337787
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.385398
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.431713
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.479746
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.527171
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.572781
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.620278
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.667391
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.714384
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.761042
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.807493
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.853731
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.902157
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.949213
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.995502
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.041652
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.087962
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.135144
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.181634
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.228017
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0147
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.704688
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.753787
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.799849
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.846432
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.892412
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.938647
[batch 140] samples: 8960, Training Loss: 0.0002
   Time since start: 0:00:01.984855
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:02.032323
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:02.078863
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.126939
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.176181
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.223267
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:02.270628
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.316448
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.363871
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.410623
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.457240
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.504396
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.551473
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.600444
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.646936
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.694458
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.742005
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.789082
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0148
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.204791
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.250854
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.297974
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.344631
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.390720
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.439853
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.487320
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.533952
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.580269
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.626747
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.673820
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.720858
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.768729
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.815952
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.863899
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.910185
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.956334
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:04.002965
[batch 380] samples: 24320, Training Loss: 0.0003
   Time since start: 0:00:04.050164
[batch 400] samples: 25600, Training Loss: 0.0002
   Time since start: 0:00:04.097158
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:04.143424
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.190826
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.237332
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.286261
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0151
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.700112
[batch 40] samples: 2560, Training Loss: 0.0002
   Time since start: 0:00:04.747853
[batch 60] samples: 3840, Training Loss: 0.0003
   Time since start: 0:00:04.793556
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.840135
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.887255
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.934307
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.981577
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.028165
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.073998
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.122141
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.170205
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.216371
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.262530
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.308372
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.356822
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.404871
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.450425
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.496974
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.545362
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.591479
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.639113
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.686700
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.733185
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.779350
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0129
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.190924
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.237676
[batch 60] samples: 3840, Training Loss: 0.0004
   Time since start: 0:00:06.285142
[batch 80] samples: 5120, Training Loss: 0.0002
   Time since start: 0:00:06.332508
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:06.381305
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.428392
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.475662
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.522240
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.569355
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.614608
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.661564
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.707898
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.754573
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.801241
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.849422
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.895983
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.943056
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.989925
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.035219
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.081681
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.128965
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.175068
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.222549
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.268636
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0148
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.685077
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.732076
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.779693
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.826734
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.872265
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.918040
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.965300
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.012528
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.060407
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.109061
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.155300
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.201416
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.248743
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.296174
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.343434
[batch 320] samples: 20480, Training Loss: 0.1469
   Time since start: 0:00:08.390340
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:08.436816
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.483867
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.534013
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:08.583296
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.630249
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.675805
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.722395
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.769906
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0157
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.186685
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.234183
[batch 60] samples: 3840, Training Loss: 0.0005
   Time since start: 0:00:09.280663
[batch 80] samples: 5120, Training Loss: 0.0003
   Time since start: 0:00:09.328153
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:09.378516
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:09.423884
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.470034
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:09.515893
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.563369
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.609486
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.655858
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.701980
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.748956
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.797743
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.843915
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.889256
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.935552
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.981305
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.027204
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.073456
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.120720
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.167084
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.216083
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.262090
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0163
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.681353
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.727793
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.773517
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.820419
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.866760
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.912179
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.957588
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:11.004008
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.053741
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.100756
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.147305
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.192704
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.239125
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.286071
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.332388
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.378693
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.425500
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.474314
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.520270
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.567070
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.612873
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.659508
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.705972
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.752904
--m-Epoch 8 done.
   Training Loss: 0.0005
   Validation Loss: 0.0184
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999617  0.999617  0.999617     0.999617      8      0
366   0.999214  0.999515  0.999359  7842.000000      8      1
367   0.999622  0.999617  0.999618  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.147511
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.195524
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.241539
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.290246
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.336459
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.383978
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.430367
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:00.479513
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.526672
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.573509
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.618863
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.665174
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.711881
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.758934
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.806493
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.853548
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.903330
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.949785
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.996724
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.043322
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090756
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.137643
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.184252
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.230657
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0184
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.661624
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.708273
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.757231
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.804935
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.852542
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.898398
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.945287
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.992123
[batch 180] samples: 11520, Training Loss: 0.0002
   Time since start: 0:00:02.039214
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:02.085875
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.131058
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.178853
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.226238
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.273731
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.321438
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.367655
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.414913
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.462031
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.507953
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:02.554812
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.604364
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.651199
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.698431
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.745977
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0186
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.164431
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.211365
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.258705
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.305948
[batch 100] samples: 6400, Training Loss: 0.0005
   Time since start: 0:00:03.352514
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:03.399450
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:03.449297
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.495204
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.541250
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:03.588007
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.635303
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.681109
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.726040
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.771425
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.817672
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.866234
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.913016
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.961009
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.007747
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.054933
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.101168
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.147136
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.193121
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.239974
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0186
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.655233
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.704236
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.750872
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.799263
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.846401
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.893253
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.938953
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.986695
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.033427
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.080088
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.128645
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.175154
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.221118
[batch 280] samples: 17920, Training Loss: 0.0003
   Time since start: 0:00:05.266497
[batch 300] samples: 19200, Training Loss: 0.0004
   Time since start: 0:00:05.312741
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:05.358636
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.405638
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.451988
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.497247
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.546114
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.593818
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.641018
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.687563
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.734955
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0189
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.152459
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.198840
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.245453
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.291932
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.338413
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.385433
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.434222
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.482712
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.529838
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.575812
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.623118
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.670922
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.718301
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.765369
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.813249
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.861094
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.907808
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.954396
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.000714
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.047060
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.093439
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.138729
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.185458
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.230716
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0192
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:07.652738
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:07.701560
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.748312
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:07.794947
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.841125
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.889714
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.935765
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.982693
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.029257
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.076475
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.124132
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.171914
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.218878
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.265537
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.312211
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.359478
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.406139
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.453070
[batch 380] samples: 24320, Training Loss: 0.0002
   Time since start: 0:00:08.500790
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.548429
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.594015
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.639577
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.685456
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.730994
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0195
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
271   1.000000  1.000000  1.000000    48.000000      6     41
272   1.000000  1.000000  1.000000    48.000000      6     42
273   0.999617  0.999617  0.999617     0.999617      6      0
274   0.999214  0.999515  0.999359  7842.000000      6      1
275   0.999622  0.999617  0.999618  7842.000000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.151107
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.199753
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.247432
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.293622
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.342827
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.389361
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.438948
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.486592
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.533934
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.581042
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.628382
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:00.674418
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.721271
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.769795
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:00.817041
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:00.865635
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:00.914251
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.961345
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:01.008210
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.053611
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.099200
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.146230
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.192049
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.237816
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0199
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.657887
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.706776
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.752234
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.799987
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.847780
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.895158
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.941893
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.988847
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.034756
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.081271
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.129358
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.175464
[batch 260] samples: 16640, Training Loss: 0.0003
   Time since start: 0:00:02.221993
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:02.269378
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.315146
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:02.361292
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:02.407353
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.453456
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:02.499599
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.546782
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.594748
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.641448
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.688504
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.736321
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0205
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.152387
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.199789
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.245654
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.290008
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.335782
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.384191
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.433215
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.479646
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.526353
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.573395
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.618125
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.665322
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:03.712114
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.758135
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.804415
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.852464
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.898339
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.944350
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.990738
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.038569
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.084982
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.131267
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.177314
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.222810
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0212
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.642355
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.692753
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.739822
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.786503
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.834191
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.881121
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.929340
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.975338
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.022232
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.068946
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.118181
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.164136
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.209359
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:05.255753
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:05.303468
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.349703
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.396672
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.441950
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.488248
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.537826
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.583648
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.629441
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.675585
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.721791
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0244
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.134741
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.182253
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.228158
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.275745
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.322589
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.370378
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.416753
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.463031
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.509783
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.556601
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.604342
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.652429
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.698356
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.745353
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.793956
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.840904
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.888190
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.934533
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.980775
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.027287
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.073445
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.120797
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.167125
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.216465
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0207
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.639349
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.687548
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.734497
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.780737
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.827504
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.874213
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.921164
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.967731
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.014495
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.062812
[batch 220] samples: 14080, Training Loss: 0.0004
   Time since start: 0:00:08.110696
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.155976
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.201632
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.249715
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.296673
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.343280
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.389721
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.436772
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.486550
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.532751
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.580051
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.628442
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.673992
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.721643
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0193
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.137560
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.184799
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.232477
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.279501
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.328333
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.375791
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.421340
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:09.468826
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.517417
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.564051
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:09.611166
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.657610
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.703281
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.752578
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.800147
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.846066
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.893711
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.940670
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.986876
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.034546
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.081676
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.129380
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.177677
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.225403
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0193
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.639403
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.686342
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.732851
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.778895
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.824704
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.871649
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.918097
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.965955
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.014882
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.061993
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.108907
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.155089
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.202227
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.249808
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.295139
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.341504
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.388037
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.435783
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.485241
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.533269
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.580312
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.626165
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.674037
[batch 480] samples: 30720, Training Loss: 0.0002
   Time since start: 0:00:11.720044
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0215
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
363   1.000000  1.000000  1.000000    48.000000      8     41
364   1.000000  1.000000  1.000000    48.000000      8     42
365   0.999617  0.999617  0.999617     0.999617      8      0
366   0.999214  0.999515  0.999359  7842.000000      8      1
367   0.999622  0.999617  0.999618  7842.000000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0002
   Time since start: 0:00:00.137376
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:00.183047
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.230772
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.279824
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.326464
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.373337
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.421545
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.469029
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.515788
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.562302
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.610777
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.659139
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.709749
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.756222
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.803592
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.851278
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.899247
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.946519
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.992779
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.039536
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.087172
[batch 440] samples: 28160, Training Loss: 0.1608
   Time since start: 0:00:01.136678
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:01.183629
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.230563
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0200
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:01.660270
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:01.707757
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.755617
[batch 80] samples: 5120, Training Loss: 0.0001
   Time since start: 0:00:01.803668
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.851837
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.899838
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.948219
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.994686
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.040629
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.086607
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.132264
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.179246
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.225339
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.273578
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.319789
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.367903
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.413393
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.461447
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.507738
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.554596
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.601645
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.648566
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.696311
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.743003
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0179
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.162193
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.210860
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.257378
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.305472
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.354230
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.400699
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.447964
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.494541
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.539891
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.585679
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.633435
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.680379
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.726136
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.772088
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.819365
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.865430
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.910726
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.957476
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:04.003626
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.050831
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.098812
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.145973
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.192411
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.238807
--m-Epoch 3 done.
   Training Loss: 0.0004
   Validation Loss: 0.0188
patience decreased: patience is now  4
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.658901
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.706044
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.752655
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.800169
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:04.847408
[batch 120] samples: 7680, Training Loss: 0.0002
   Time since start: 0:00:04.895219
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:04.941643
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:04.988838
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.036131
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.082517
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.129406
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.176534
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.223272
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.270850
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.320328
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.368172
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.413838
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.460507
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.507310
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.552758
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.597682
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.645087
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.691283
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.739093
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0187
patience decreased: patience is now  3
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.157634
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.205908
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.253264
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.299676
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.346200
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.394212
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.442261
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.489316
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.535036
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.580911
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.628677
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.675154
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:06.721700
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:06.768552
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.815529
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.862274
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.909461
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.955421
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.001786
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.050143
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.096972
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.144146
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.190536
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.236949
--m-Epoch 5 done.
   Training Loss: 0.0005
   Validation Loss: 0.0206
patience decreased: patience is now  2
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:08.819101
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:08.866101
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:08.914053
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:08.962225
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.008495
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.055577
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.103478
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.151428
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.198633
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.245976
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.292904
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.339322
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.386339
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.433116
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.479825
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.526630
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.574591
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.620806
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.666904
[batch 400] samples: 25600, Training Loss: 0.0005
   Time since start: 0:00:09.713997
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:09.760690
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:09.806665
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:09.853994
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:09.900086
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0209
patience decreased: patience is now  1
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.319688
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.366374
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.416874
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.464308
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.510906
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.558517
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.605586
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.652377
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:10.699752
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:10.744901
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:10.792033
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:10.839893
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:10.886057
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:10.932495
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:10.977558
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.023079
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.070688
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:11.117774
[batch 380] samples: 24320, Training Loss: 0.0008
   Time since start: 0:00:11.162411
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.208417
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.255980
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.300124
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.347336
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.393337
--m-Epoch 7 done.
   Training Loss: 0.0077
   Validation Loss: 0.0151
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:11.808231
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:11.855845
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:11.903425
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:11.950492
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:11.997417
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.044644
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.092337
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.138340
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.183770
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.232152
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.279123
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.326613
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.372843
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.418654
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.464514
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.511380
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.557924
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.604802
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.651646
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:12.698151
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:12.746201
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:12.792971
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:12.839391
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:12.885475
--m-Epoch 8 done.
   Training Loss: 0.0011
   Validation Loss: 0.0138
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.301488
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.349170
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.396478
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.443149
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.490267
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.538211
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.584378
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.630095
[batch 180] samples: 11520, Training Loss: 0.0009
   Time since start: 0:00:13.675580
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:13.721775
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:13.768533
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:13.815584
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:13.862637
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:13.909916
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:13.957382
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.004695
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.051246
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.098062
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.144734
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.192604
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.239507
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.287631
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.334893
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.381269
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0135
patience decreased: patience is now  2
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:14.799626
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:14.845164
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:14.891763
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:14.937963
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:14.984042
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:15.030750
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:15.079845
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:15.127564
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:15.175214
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:15.221879
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:15.267944
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:15.314296
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:15.361636
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:15.407726
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:15.454041
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:15.503151
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:15.550064
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:15.597706
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:15.644176
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:15.691546
[batch 420] samples: 26880, Training Loss: 0.0005
   Time since start: 0:00:15.738391
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:15.786619
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:15.832955
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:15.879353
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0139
patience decreased: patience is now  1
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:16.291277
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:16.340308
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:16.385417
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:16.432448
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:16.479429
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:16.526103
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:16.572496
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:16.618426
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:16.664979
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:16.711427
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:16.759786
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:16.806670
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:16.854778
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:16.902031
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:16.950190
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:16.996579
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:17.043602
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:17.089842
[batch 380] samples: 24320, Training Loss: 0.0003
   Time since start: 0:00:17.136158
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:17.184175
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:17.229474
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:17.275680
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:17.322292
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:17.368310
--m-Epoch 11 done.
   Training Loss: 0.0003
   Validation Loss: 0.0137
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
501   1.000000  1.000000  1.000000    48.00000     11     41
502   1.000000  1.000000  1.000000    48.00000     11     42
503   0.999490  0.999490  0.999490     0.99949     11      0
504   0.998945  0.999453  0.999193  7842.00000     11      1
505   0.999496  0.999490  0.999491  7842.00000     11      2

[506 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.138509
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.185522
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.232816
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.280028
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.329070
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.376373
[batch 140] samples: 8960, Training Loss: 0.0009
   Time since start: 0:00:00.424076
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:00.469748
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:00.516779
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.563890
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.611367
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.656870
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.704157
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.752835
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.799086
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.847629
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.895103
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.942751
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.988429
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.036378
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.082466
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.129874
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.178395
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.226047
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0137
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.644769
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.692569
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.738851
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.785246
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.831550
[batch 120] samples: 7680, Training Loss: 0.0005
   Time since start: 0:00:01.879525
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:01.925160
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:01.971725
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.019090
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.066611
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.115308
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.161291
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.207357
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.252369
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.299848
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.345638
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.392327
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.440006
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.486789
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.533927
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.581412
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.628882
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.674681
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.722002
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.133454
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.178222
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.225480
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.274376
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.321485
[batch 120] samples: 7680, Training Loss: 0.0004
   Time since start: 0:00:03.368549
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:03.415855
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:03.462834
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:03.509281
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.555319
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.601593
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.646522
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.693151
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.740161
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.786651
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.833587
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.880588
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.927090
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:03.975574
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.022066
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.068407
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.115856
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.162745
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.208992
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.623741
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.670745
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.718555
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.765243
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:04.811341
[batch 120] samples: 7680, Training Loss: 0.0004
   Time since start: 0:00:04.857450
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:04.903979
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.950858
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:04.997976
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.044910
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.091921
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.138245
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.185520
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.233408
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.279881
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.325737
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.373840
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.420362
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.467597
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.514077
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.561761
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.608805
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.656038
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.702289
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0140
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.112242
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.159645
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.207572
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.256618
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.303426
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.349637
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.395397
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.441758
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.488279
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.535156
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.581682
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.628247
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.676460
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.723696
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.769838
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.816405
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.862021
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.909697
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.956185
[batch 400] samples: 25600, Training Loss: 0.0004
   Time since start: 0:00:07.003164
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:07.049563
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.098019
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.145501
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.192898
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0140
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.611916
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.659241
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.704928
[batch 80] samples: 5120, Training Loss: 0.0003
   Time since start: 0:00:07.751918
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:07.798625
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:07.844367
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:07.890613
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.939590
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:07.986726
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.033719
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.079850
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.127402
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.173309
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.219281
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.265342
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.313162
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.361096
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.407587
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.454549
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.500708
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.546602
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.593040
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.641479
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.689733
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0139
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
271   1.000000  1.000000  1.000000    48.00000      6     41
272   1.000000  1.000000  1.000000    48.00000      6     42
273   0.999490  0.999490  0.999490     0.99949      6      0
274   0.998945  0.999453  0.999193  7842.00000      6      1
275   0.999496  0.999490  0.999491  7842.00000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.140536
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.187768
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.233787
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.281711
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.331333
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.379300
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.425214
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.472449
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.520479
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567839
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.614696
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.661608
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.708818
[batch 280] samples: 17920, Training Loss: 0.0004
   Time since start: 0:00:00.755891
[batch 300] samples: 19200, Training Loss: 0.0006
   Time since start: 0:00:00.801965
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.849245
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.897521
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.944550
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.990425
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.037556
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.085195
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.132188
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.181336
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.228293
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0143
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.661478
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.708815
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.755864
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.803080
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.849471
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.895030
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.943256
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.988792
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.035485
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.082683
[batch 220] samples: 14080, Training Loss: 0.0005
   Time since start: 0:00:02.130354
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.175780
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.224294
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.272001
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.318451
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.365138
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.411350
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.458561
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.505466
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.552414
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.597947
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.645483
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.693039
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.739493
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0143
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.156902
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.202678
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.248672
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.297830
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.344671
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.390805
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.436972
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.485113
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.533860
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:03.580741
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:03.628957
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:03.675466
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.723880
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.772132
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.820324
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.866960
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.915409
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.961445
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.008144
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.055367
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.102866
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.151982
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.199488
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.245781
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0141
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.658576
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.704752
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.750252
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.796906
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.844709
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.892042
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.938493
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.986889
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.032189
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.079653
[batch 220] samples: 14080, Training Loss: 0.0003
   Time since start: 0:00:05.126465
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.172189
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:05.218759
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.264680
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.310746
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.357462
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.405833
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.452588
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.499081
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.546878
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.592748
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.639771
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.686494
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.733501
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0142
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.162793
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.209120
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.257696
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.305392
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.352329
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.400257
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.446927
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.493728
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.541057
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.589762
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.636419
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.685069
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.733176
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.779586
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.826275
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.873172
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.920679
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.967937
[batch 380] samples: 24320, Training Loss: 0.0011
   Time since start: 0:00:07.014672
[batch 400] samples: 25600, Training Loss: 0.0003
   Time since start: 0:00:07.060235
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.109481
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.157047
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.202465
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.249486
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0140
patience decreased: patience is now  1
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.682625
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.729942
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.777066
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.821978
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.869481
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.916281
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.964560
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:08.010772
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.055751
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.103993
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.150539
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:08.197265
[batch 260] samples: 16640, Training Loss: 0.0004
   Time since start: 0:00:08.244706
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:08.291163
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.337248
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.386586
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.434282
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.481413
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.528109
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.574448
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.619933
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.668372
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.715466
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.762102
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0142
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
271   1.000000  1.000000  1.000000    48.00000      6     41
272   1.000000  1.000000  1.000000    48.00000      6     42
273   0.999490  0.999490  0.999490     0.99949      6      0
274   0.998945  0.999453  0.999193  7842.00000      6      1
275   0.999496  0.999490  0.999491  7842.00000      6      2

[276 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.141291
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.189355
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.237650
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.284537
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.334212
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.381839
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.429234
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:00.475133
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:00.521358
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.568404
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.614840
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.661814
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.708794
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.758523
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.807136
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.853253
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.898883
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.946757
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.993712
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.041818
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.088054
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.135335
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.184746
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.232125
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0141
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.651604
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.698261
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.745451
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.792346
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.837236
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.883153
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.930460
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:01.977477
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.025130
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.071314
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.117581
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.163808
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.209973
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.256593
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.302784
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.350051
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.396235
[batch 360] samples: 23040, Training Loss: 0.0002
   Time since start: 0:00:02.445343
[batch 380] samples: 24320, Training Loss: 0.0006
   Time since start: 0:00:02.491523
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:02.538020
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.584725
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:02.631740
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:02.678152
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.724402
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0140
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.155671
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.204238
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.250999
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.299971
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.346875
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.395361
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.441229
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.488253
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.536255
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.583283
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.630403
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.678825
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:03.727032
[batch 280] samples: 17920, Training Loss: 0.0001
   Time since start: 0:00:03.773161
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:03.820375
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:03.866722
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.913709
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.961012
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.007986
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.053639
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.100569
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.150858
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.196689
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.244462
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.657893
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.704551
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.750478
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.796935
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.844480
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.892399
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.939553
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.989882
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.037857
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.084306
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:05.131227
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:05.177030
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:05.223327
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.269465
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.315839
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.361311
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.409838
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.456926
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.504386
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.551504
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.598832
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.647032
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.693997
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.741759
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0137
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.156708
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.203306
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.251959
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.299707
[batch 100] samples: 6400, Training Loss: 0.0002
   Time since start: 0:00:06.346341
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.393436
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:06.440522
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.485879
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.533338
[batch 200] samples: 12800, Training Loss: 0.0001
   Time since start: 0:00:06.580319
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.626532
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.675703
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.722309
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.769398
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.815969
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.862998
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.909125
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.955299
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.002581
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.049373
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.098049
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.147253
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.193641
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.239084
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0139
patience decreased: patience is now  3
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.653408
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.700847
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.746710
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.794209
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.840734
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.889539
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.938984
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.987392
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.036637
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.083596
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.129768
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.177288
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.223346
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.269673
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:08.315667
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:08.364252
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:08.411904
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:08.457647
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.505976
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.552558
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.598479
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.645561
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.692379
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.738667
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.155559
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.202134
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.252713
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.299969
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:09.346193
[batch 120] samples: 7680, Training Loss: 0.0005
   Time since start: 0:00:09.393311
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:09.440934
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.487847
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:09.534107
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.579883
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.626261
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.674070
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.721022
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.768626
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.815832
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.862201
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.909522
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.956404
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.002168
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.048645
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.097106
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.144773
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.191659
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.238017
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0137
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.654531
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.701991
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.749769
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.798354
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.845728
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.892581
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.941977
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.988448
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.034274
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.081119
[batch 220] samples: 14080, Training Loss: 0.0002
   Time since start: 0:00:11.128453
[batch 240] samples: 15360, Training Loss: 0.0002
   Time since start: 0:00:11.176353
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.222337
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.269161
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:11.315075
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.363365
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.408273
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.455039
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.502212
[batch 400] samples: 25600, Training Loss: 0.0001
   Time since start: 0:00:11.549855
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.596407
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.643435
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.689857
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.735631
--m-Epoch 8 done.
   Training Loss: 0.0003
   Validation Loss: 0.0139
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
363   1.000000  1.000000  1.000000    48.00000      8     41
364   1.000000  1.000000  1.000000    48.00000      8     42
365   0.999490  0.999490  0.999490     0.99949      8      0
366   0.998945  0.999453  0.999193  7842.00000      8      1
367   0.999496  0.999490  0.999491  7842.00000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0003
   Time since start: 0:00:00.142784
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:00.190297
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.238566
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.285267
[batch 100] samples: 6400, Training Loss: 0.0001
   Time since start: 0:00:00.333857
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:00.381357
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:00.429251
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.475976
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.524186
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.571675
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.618429
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.665810
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.712995
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.760221
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.807811
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.854152
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.901853
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.949678
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.995186
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.041835
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.090357
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.138439
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.186008
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.232923
--m-Epoch 1 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.671801
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.719038
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.766467
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.814310
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.861606
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.908365
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.957320
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:02.005054
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.051673
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.099406
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:02.147241
[batch 240] samples: 15360, Training Loss: 0.0001
   Time since start: 0:00:02.194366
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:02.240808
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.287840
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.334732
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:02.384690
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.431883
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.477601
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.523794
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.570880
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.617481
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.664915
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.711661
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.758320
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.175723
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.224695
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.272516
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.318424
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.363748
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.410979
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.458504
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.504669
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.550551
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.597305
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.647327
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.694258
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.738925
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.785309
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.831810
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.877599
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.923438
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.970827
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.017651
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.067579
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:04.113568
[batch 440] samples: 28160, Training Loss: 0.0002
   Time since start: 0:00:04.159811
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.206783
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.253958
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.677239
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:04.723177
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.770160
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.817639
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.866172
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:04.914823
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.960755
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:05.008109
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:05.053724
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.100299
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:05.146574
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.193169
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.240092
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.286803
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:05.335550
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.382675
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.429119
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:05.475951
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.523577
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.570901
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.617219
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.663896
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.710318
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.759293
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0138
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.173951
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.221226
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.268389
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.314661
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.361855
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.407046
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.454879
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.502644
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.549497
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.597750
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.644161
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.691294
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.737631
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.785001
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.832743
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.879147
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.925307
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.971337
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:07.018790
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.065127
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.110533
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.155927
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.202265
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.248265
--m-Epoch 5 done.
   Training Loss: 0.0004
   Validation Loss: 0.0129
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.661282
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:07.709324
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.757446
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.805641
[batch 100] samples: 6400, Training Loss: 0.0003
   Time since start: 0:00:07.855771
[batch 120] samples: 7680, Training Loss: 0.0001
   Time since start: 0:00:07.903659
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:07.950062
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.996607
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.043790
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.090120
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.135262
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.180388
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.227206
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.275980
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.322783
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.369473
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.416382
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.463779
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.511292
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.558687
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.604973
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.651203
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.700389
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.747073
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0147
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.158837
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.205515
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.252082
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.298630
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.345888
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.391932
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.437194
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.482593
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.527852
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.577310
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.625462
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.671756
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.719441
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.765187
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.813486
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.860075
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.906784
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.953566
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:10.002301
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.050409
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:10.096363
[batch 440] samples: 28160, Training Loss: 0.0001
   Time since start: 0:00:10.141691
[batch 460] samples: 29440, Training Loss: 0.0001
   Time since start: 0:00:10.187326
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:10.233727
--m-Epoch 7 done.
   Training Loss: 0.0003
   Validation Loss: 0.0151
patience decreased: patience is now  1
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.662137
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.709733
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.757163
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.804184
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.853947
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.900461
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.945199
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.993256
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.040928
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.087448
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.134598
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.180575
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.227391
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.275884
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.322087
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.367138
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.414296
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.460845
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.506331
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.552283
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.598052
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.644437
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.694360
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.740472
--m-Epoch 8 done.
   Training Loss: 0.0004
   Validation Loss: 0.0151
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score     support  epoch  class
0     1.000000  1.000000  1.000000    42.00000      1      0
1     1.000000  1.000000  1.000000   444.00000      1      1
2     1.000000  0.997778  0.998888   450.00000      1      2
3     1.000000  1.000000  1.000000   282.00000      1      3
4     1.000000  1.000000  1.000000   396.00000      1      4
..         ...       ...       ...         ...    ...    ...
363   1.000000  1.000000  1.000000    48.00000      8     41
364   1.000000  1.000000  1.000000    48.00000      8     42
365   0.999490  0.999490  0.999490     0.99949      8      0
366   0.998945  0.999453  0.999193  7842.00000      8      1
367   0.999496  0.999490  0.999491  7842.00000      8      2

[368 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.138599
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.185531
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.233159
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.281871
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.329914
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.378824
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.426062
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.472521
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.519572
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.568422
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.615393
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.662660
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.708449
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.757622
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.804760
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:00.851961
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:00.897820
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.943593
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.990502
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.036789
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.083170
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.131365
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.181241
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.228762
--m-Epoch 1 done.
   Training Loss: 0.0004
   Validation Loss: 0.0152
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.647573
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.693542
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.740013
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.788577
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.835330
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.883330
[batch 140] samples: 8960, Training Loss: 0.0001
   Time since start: 0:00:01.929003
[batch 160] samples: 10240, Training Loss: 0.0003
   Time since start: 0:00:01.977881
[batch 180] samples: 11520, Training Loss: 0.0001
   Time since start: 0:00:02.024630
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.071439
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.117471
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.165020
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.211003
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.258621
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.305350
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.352747
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.402502
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.449423
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.496740
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.543875
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.589579
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.635377
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.680861
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.726823
--m-Epoch 2 done.
   Training Loss: 0.0003
   Validation Loss: 0.0151
patience decreased: patience is now  4
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.147503
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.195755
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.244215
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:03.290196
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.337489
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.383463
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.430365
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.477315
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.524497
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.570876
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.616811
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.665581
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.712111
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.758866
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.804859
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.851608
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:03.898107
[batch 360] samples: 23040, Training Loss: 0.0004
   Time since start: 0:00:03.946090
[batch 380] samples: 24320, Training Loss: 0.0001
   Time since start: 0:00:03.992877
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.038653
[batch 420] samples: 26880, Training Loss: 0.0001
   Time since start: 0:00:04.087282
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.135111
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.182324
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.228618
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0150
patience decreased: patience is now  3
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0001
   Time since start: 0:00:04.655333
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.700638
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.748069
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.794911
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.841633
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.887902
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.936967
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.984703
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.031381
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.077236
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.124440
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.171304
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.217949
[batch 280] samples: 17920, Training Loss: 0.0003
   Time since start: 0:00:05.264964
[batch 300] samples: 19200, Training Loss: 0.0002
   Time since start: 0:00:05.310568
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.358413
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.404922
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:05.451957
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.498849
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.545072
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.589749
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.635693
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.682951
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.728685
--m-Epoch 4 done.
   Training Loss: 0.0003
   Validation Loss: 0.0149
patience decreased: patience is now  2
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.148066
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.197170
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.244132
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.290804
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.336268
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.382848
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.430147
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.477689
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.525994
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.573531
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:06.621251
[batch 240] samples: 15360, Training Loss: 0.0003
   Time since start: 0:00:06.667933
[batch 260] samples: 16640, Training Loss: 0.0001
   Time since start: 0:00:06.712580
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:06.758090
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:06.802545
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.848294
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:06.894173
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.940908
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.986798
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.036150
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.082381
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.129444
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.176204
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.221212
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0148
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.636863
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.683496
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.729323
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.776073
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.822384
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.871240
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.917996
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.964525
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.009018
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.055649
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.101448
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.147239
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.192666
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.239056
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.286779
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.333347
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:08.380341
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.427708
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.474916
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.522193
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.569302
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.615578
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.661169
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.710036
--m-Epoch 6 done.
   Training Loss: 0.0004
   Validation Loss: 0.0149
patience decreased: patience is now  2
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.127075
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.174214
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.220618
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.265977
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.312393
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.358689
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.404570
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.450860
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.496451
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.544551
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.591404
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.639588
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.685077
[batch 280] samples: 17920, Training Loss: 0.0002
   Time since start: 0:00:09.731045
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:09.776292
[batch 320] samples: 20480, Training Loss: 0.0001
   Time since start: 0:00:09.821879
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.867893
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.913987
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.962072
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.007930
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.054109
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.100479
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:10.147515
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:10.197204
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0139
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.615204
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.663366
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.711739
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.759069
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.807980
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.854718
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.900021
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.947103
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:10.993515
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.040818
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.084658
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.130258
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.175365
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.223968
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.271038
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.317992
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.364891
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.411800
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.458208
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.503431
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:11.549790
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.596442
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.644661
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.691703
--m-Epoch 8 done.
   Training Loss: 0.0005
   Validation Loss: 0.0140
patience decreased: patience is now  2
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0018
   Time since start: 0:00:12.100640
[batch 40] samples: 2560, Training Loss: 0.0001
   Time since start: 0:00:12.148048
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.198057
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.244382
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.290383
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.336988
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.383766
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.430230
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.481090
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.528966
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.577045
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.623755
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.671329
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.718011
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.763809
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.809313
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:12.855505
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:12.905232
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.951230
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:12.997939
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.043300
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.089772
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.136185
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.183847
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0136
patience decreased: patience is now  1
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.609495
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.656770
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.703991
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.753119
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.799193
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.845556
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.892707
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.940501
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:13.987528
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.035227
[batch 220] samples: 14080, Training Loss: 0.0001
   Time since start: 0:00:14.082536
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.128464
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.178265
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.223534
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.269623
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.315811
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.363947
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.411054
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.457171
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.503084
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.549802
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:14.597851
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.645828
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.693132
--m-Epoch 10 done.
   Training Loss: 0.0004
   Validation Loss: 0.0144
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  0.997778  0.998888   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
455   1.000000  1.000000  1.000000    48.000000     10     41
456   1.000000  1.000000  1.000000    48.000000     10     42
457   0.999617  0.999617  0.999617     0.999617     10      0
458   0.999214  0.999515  0.999359  7842.000000     10      1
459   0.999622  0.999617  0.999618  7842.000000     10      2

[460 rows x 6 columns]
device: cuda
Epoch: 1 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:00.138991
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:00.190529
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:00.236633
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:00.284903
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:00.331877
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:00.378000
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:00.425369
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:00.472885
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:00.519057
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:00.567034
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:00.617423
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:00.665669
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:00.710766
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:00.757764
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:00.803965
[batch 320] samples: 20480, Training Loss: 0.0450
   Time since start: 0:00:00.849381
[batch 340] samples: 21760, Training Loss: 1.0313
   Time since start: 0:00:00.893650
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:00.941150
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:00.987424
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:01.037644
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:01.084042
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:01.130771
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:01.178862
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:01.224828
--m-Epoch 1 done.
   Training Loss: 0.0094
   Validation Loss: 0.0143
Epoch: 2 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:01.651252
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:01.698735
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:01.746566
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:01.792755
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:01.839998
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:01.889698
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:01.938447
[batch 160] samples: 10240, Training Loss: 0.0001
   Time since start: 0:00:01.985049
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:02.032228
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:02.079710
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:02.126711
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:02.173242
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:02.220738
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:02.267420
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:02.317290
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:02.363453
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:02.410970
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:02.457248
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:02.503904
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:02.549501
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:02.595851
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:02.642457
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:02.688759
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:02.739097
--m-Epoch 2 done.
   Training Loss: 0.0005
   Validation Loss: 0.0138
Epoch: 3 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:03.153496
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:03.202223
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:03.249340
[batch 80] samples: 5120, Training Loss: 0.0033
   Time since start: 0:00:03.297851
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:03.343580
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:03.389956
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:03.436765
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:03.484299
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:03.529004
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:03.579108
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:03.626706
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:03.675120
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:03.721628
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:03.768139
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:03.814082
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:03.860963
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:03.906406
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:03.953223
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:04.001616
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:04.047326
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:04.093734
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:04.141253
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:04.187707
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:04.234639
--m-Epoch 3 done.
   Training Loss: 0.0003
   Validation Loss: 0.0133
Epoch: 4 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:04.643521
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:04.690935
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:04.737050
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:04.783945
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:04.833776
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:04.881306
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:04.926264
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:04.974403
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:05.021513
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:05.067945
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:05.114580
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:05.160931
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:05.207279
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:05.257537
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:05.305292
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:05.352589
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:05.398789
[batch 360] samples: 23040, Training Loss: 0.0021
   Time since start: 0:00:05.446771
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:05.493625
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:05.540802
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:05.588408
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:05.633979
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:05.684409
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:05.731921
--m-Epoch 4 done.
   Training Loss: 0.0004
   Validation Loss: 0.0129
Epoch: 5 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:06.144479
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:06.191291
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:06.240025
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:06.287577
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:06.334273
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:06.381736
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:06.427381
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:06.473137
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:06.522923
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:06.569384
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:06.615800
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:06.662070
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:06.709682
[batch 280] samples: 17920, Training Loss: 0.0007
   Time since start: 0:00:06.755078
[batch 300] samples: 19200, Training Loss: 0.0001
   Time since start: 0:00:06.802320
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:06.848560
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:06.895379
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:06.945589
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:06.992101
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:07.038622
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:07.084932
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:07.131309
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:07.177760
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:07.224534
--m-Epoch 5 done.
   Training Loss: 0.0003
   Validation Loss: 0.0117
Epoch: 6 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:07.642256
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:07.689920
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:07.736886
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:07.786766
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:07.833443
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:07.879284
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:07.925502
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:07.972512
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:08.018766
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:08.065311
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:08.111297
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:08.157182
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:08.207104
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:08.253960
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:08.300679
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:08.347392
[batch 340] samples: 21760, Training Loss: 0.0001
   Time since start: 0:00:08.393772
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:08.439602
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:08.484969
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:08.532001
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:08.578363
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:08.628485
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:08.676207
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:08.724176
--m-Epoch 6 done.
   Training Loss: 0.0003
   Validation Loss: 0.0107
Epoch: 7 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:09.137927
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:09.186059
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:09.233060
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:09.279628
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:09.326885
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:09.372381
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:09.418722
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:09.468330
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:09.515144
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:09.560621
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:09.608341
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:09.655540
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:09.702330
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:09.749778
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:09.796920
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:09.843612
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:09.893506
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:09.940093
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:09.986998
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:10.032795
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:10.081211
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:10.127554
[batch 460] samples: 29440, Training Loss: 0.0002
   Time since start: 0:00:10.174597
[batch 480] samples: 30720, Training Loss: 0.0001
   Time since start: 0:00:10.220883
--m-Epoch 7 done.
   Training Loss: 0.0004
   Validation Loss: 0.0098
Epoch: 8 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:10.640911
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:10.687557
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:10.737489
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:10.783204
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:10.831211
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:10.878751
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:10.924265
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:10.970433
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:11.017229
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:11.065418
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:11.112958
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:11.160313
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:11.207123
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:11.254134
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:11.300808
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:11.347394
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:11.393811
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:11.441553
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:11.487730
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:11.537299
[batch 420] samples: 26880, Training Loss: 0.0012
   Time since start: 0:00:11.582653
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:11.629343
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:11.677636
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:11.725111
--m-Epoch 8 done.
   Training Loss: 0.0004
   Validation Loss: 0.0101
patience decreased: patience is now  4
Epoch: 9 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:12.144529
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:12.193504
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:12.239965
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:12.287174
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:12.334742
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:12.384301
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:12.432092
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:12.479621
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:12.525721
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:12.572448
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:12.619164
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:12.663953
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:12.709866
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:12.756083
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:12.804792
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:12.852076
[batch 340] samples: 21760, Training Loss: 0.0008
   Time since start: 0:00:12.899424
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:12.947361
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:12.994234
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:13.041139
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:13.087584
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:13.133200
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:13.179487
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:13.229924
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0110
patience decreased: patience is now  3
Epoch: 10 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:13.647287
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:13.694127
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:13.740813
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:13.788032
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:13.835411
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:13.882825
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:13.928219
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:13.974054
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:14.020274
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:14.069449
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:14.117178
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:14.164027
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:14.210417
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:14.255608
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:14.301253
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:14.348267
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:14.395644
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:14.442789
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:14.493084
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:14.540960
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:14.586868
[batch 440] samples: 28160, Training Loss: 0.0004
   Time since start: 0:00:14.634125
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:14.679842
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:14.725964
--m-Epoch 10 done.
   Training Loss: 0.0003
   Validation Loss: 0.0121
patience decreased: patience is now  2
Epoch: 11 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:15.134600
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:15.182588
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:15.228033
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:15.275799
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:15.325662
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:15.373315
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:15.419041
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:15.465050
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:15.511378
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:15.556896
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:15.602460
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:15.647659
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:15.694088
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:15.742320
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:15.789752
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:15.837042
[batch 340] samples: 21760, Training Loss: 0.0000
   Time since start: 0:00:15.884134
[batch 360] samples: 23040, Training Loss: 0.0000
   Time since start: 0:00:15.930726
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:15.977977
[batch 400] samples: 25600, Training Loss: 0.0005
   Time since start: 0:00:16.024438
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:16.071030
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:16.117372
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:16.166043
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:16.213026
--m-Epoch 11 done.
   Training Loss: 0.0003
   Validation Loss: 0.0127
patience decreased: patience is now  1
Epoch: 12 of 20
[batch 20] samples: 1280, Training Loss: 0.0000
   Time since start: 0:00:16.627381
[batch 40] samples: 2560, Training Loss: 0.0000
   Time since start: 0:00:16.673564
[batch 60] samples: 3840, Training Loss: 0.0000
   Time since start: 0:00:16.720047
[batch 80] samples: 5120, Training Loss: 0.0000
   Time since start: 0:00:16.767469
[batch 100] samples: 6400, Training Loss: 0.0000
   Time since start: 0:00:16.814430
[batch 120] samples: 7680, Training Loss: 0.0000
   Time since start: 0:00:16.860247
[batch 140] samples: 8960, Training Loss: 0.0000
   Time since start: 0:00:16.905834
[batch 160] samples: 10240, Training Loss: 0.0000
   Time since start: 0:00:16.953044
[batch 180] samples: 11520, Training Loss: 0.0000
   Time since start: 0:00:17.002057
[batch 200] samples: 12800, Training Loss: 0.0000
   Time since start: 0:00:17.048783
[batch 220] samples: 14080, Training Loss: 0.0000
   Time since start: 0:00:17.096088
[batch 240] samples: 15360, Training Loss: 0.0000
   Time since start: 0:00:17.142263
[batch 260] samples: 16640, Training Loss: 0.0000
   Time since start: 0:00:17.190152
[batch 280] samples: 17920, Training Loss: 0.0000
   Time since start: 0:00:17.235688
[batch 300] samples: 19200, Training Loss: 0.0000
   Time since start: 0:00:17.282977
[batch 320] samples: 20480, Training Loss: 0.0000
   Time since start: 0:00:17.330203
[batch 340] samples: 21760, Training Loss: 0.0003
   Time since start: 0:00:17.378094
[batch 360] samples: 23040, Training Loss: 0.0001
   Time since start: 0:00:17.426428
[batch 380] samples: 24320, Training Loss: 0.0000
   Time since start: 0:00:17.473825
[batch 400] samples: 25600, Training Loss: 0.0000
   Time since start: 0:00:17.519868
[batch 420] samples: 26880, Training Loss: 0.0000
   Time since start: 0:00:17.567050
[batch 440] samples: 28160, Training Loss: 0.0000
   Time since start: 0:00:17.612736
[batch 460] samples: 29440, Training Loss: 0.0000
   Time since start: 0:00:17.660167
[batch 480] samples: 30720, Training Loss: 0.0000
   Time since start: 0:00:17.706713
--m-Epoch 12 done.
   Training Loss: 0.0003
   Validation Loss: 0.0141
patience decreased: patience is now  0
Stopping early
     precision    recall  f1-score      support  epoch  class
0     1.000000  1.000000  1.000000    42.000000      1      0
1     1.000000  1.000000  1.000000   444.000000      1      1
2     1.000000  1.000000  1.000000   450.000000      1      2
3     1.000000  1.000000  1.000000   282.000000      1      3
4     1.000000  1.000000  1.000000   396.000000      1      4
..         ...       ...       ...          ...    ...    ...
547   1.000000  1.000000  1.000000    48.000000     12     41
548   1.000000  1.000000  1.000000    48.000000     12     42
549   0.999617  0.999617  0.999617     0.999617     12      0
550   0.999214  0.999515  0.999359  7842.000000     12      1
551   0.999622  0.999617  0.999618  7842.000000     12      2

[552 rows x 6 columns]
