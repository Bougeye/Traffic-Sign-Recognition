Requirement already satisfied: pip in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.3)
Requirement already satisfied: wheel in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (0.45.1)
Requirement already satisfied: setuptools in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (80.9.0)
Requirement already satisfied: packaging in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.0)
All required imports already available in venv.
Import check passed (including torchvision).
=== ENV ===
HOSTNAME: gruenau1
PARTITION: gpu
CUDA_VISIBLE_DEVICES: 0
PWD: /vol/fob-vol1/mi23/ziglowsa/Payload
numpy: 2.2.6
torch: 2.4.1+cu121 cuda: 12.1 is_available: True
torchvision: 0.19.1+cu121
nvidia-smi -L: GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-502de52b-61ca-040e-e44d-5cee4e192a9d)
device: cuda
Epoch: 1 of 1
[batch 20] samples: 320, Training Loss: nan
   Time since start: 0:00:02.604373
[batch 40] samples: 640, Training Loss: nan
   Time since start: 0:00:03.770160
[batch 60] samples: 960, Training Loss: nan
   Time since start: 0:00:04.943463
[batch 80] samples: 1280, Training Loss: nan
   Time since start: 0:00:06.293464
[batch 100] samples: 1600, Training Loss: nan
   Time since start: 0:00:07.937830
[batch 120] samples: 1920, Training Loss: nan
   Time since start: 0:00:09.616687
[batch 140] samples: 2240, Training Loss: nan
   Time since start: 0:00:11.177330
[batch 160] samples: 2560, Training Loss: nan
   Time since start: 0:00:12.862024
[batch 180] samples: 2880, Training Loss: nan
   Time since start: 0:00:14.510831
[batch 200] samples: 3200, Training Loss: nan
   Time since start: 0:00:16.129810
[batch 220] samples: 3520, Training Loss: nan
   Time since start: 0:00:17.664814
[batch 240] samples: 3840, Training Loss: nan
   Time since start: 0:00:19.185611
[batch 260] samples: 4160, Training Loss: nan
   Time since start: 0:00:20.528214
[batch 280] samples: 4480, Training Loss: nan
   Time since start: 0:00:22.231234
[batch 300] samples: 4800, Training Loss: nan
   Time since start: 0:00:23.510234
[batch 320] samples: 5120, Training Loss: nan
   Time since start: 0:00:24.658408
[batch 340] samples: 5440, Training Loss: nan
   Time since start: 0:00:25.842686
[batch 360] samples: 5760, Training Loss: nan
   Time since start: 0:00:27.302510
[batch 380] samples: 6080, Training Loss: nan
   Time since start: 0:00:28.915469
[batch 400] samples: 6400, Training Loss: nan
   Time since start: 0:00:30.645114
[batch 420] samples: 6720, Training Loss: nan
   Time since start: 0:00:31.865598
[batch 440] samples: 7040, Training Loss: nan
   Time since start: 0:00:33.589411
[batch 460] samples: 7360, Training Loss: nan
   Time since start: 0:00:35.334765
[batch 480] samples: 7680, Training Loss: nan
   Time since start: 0:00:37.053988
[batch 500] samples: 8000, Training Loss: nan
   Time since start: 0:00:38.757159
[batch 520] samples: 8320, Training Loss: nan
   Time since start: 0:00:40.448102
[batch 540] samples: 8640, Training Loss: nan
   Time since start: 0:00:42.054566
[batch 560] samples: 8960, Training Loss: nan
   Time since start: 0:00:43.506906
[batch 580] samples: 9280, Training Loss: nan
   Time since start: 0:00:45.260683
[batch 600] samples: 9600, Training Loss: nan
   Time since start: 0:00:46.963117
[batch 620] samples: 9920, Training Loss: nan
   Time since start: 0:00:48.751204
[batch 640] samples: 10240, Training Loss: nan
   Time since start: 0:00:50.015937
[batch 660] samples: 10560, Training Loss: nan
   Time since start: 0:00:51.446028
[batch 680] samples: 10880, Training Loss: nan
   Time since start: 0:00:52.895291
[batch 700] samples: 11200, Training Loss: nan
   Time since start: 0:00:54.341651
[batch 720] samples: 11520, Training Loss: nan
   Time since start: 0:00:55.620232
[batch 740] samples: 11840, Training Loss: nan
   Time since start: 0:00:56.986899
[batch 760] samples: 12160, Training Loss: nan
   Time since start: 0:00:58.513541
[batch 780] samples: 12480, Training Loss: nan
   Time since start: 0:00:59.916662
[batch 800] samples: 12800, Training Loss: nan
   Time since start: 0:01:01.550911
[batch 820] samples: 13120, Training Loss: nan
   Time since start: 0:01:03.270895
[batch 840] samples: 13440, Training Loss: nan
   Time since start: 0:01:04.987495
[batch 860] samples: 13760, Training Loss: nan
   Time since start: 0:01:06.696164
[batch 880] samples: 14080, Training Loss: nan
   Time since start: 0:01:08.406922
[batch 900] samples: 14400, Training Loss: nan
   Time since start: 0:01:10.112616
[batch 920] samples: 14720, Training Loss: nan
   Time since start: 0:01:11.782366
[batch 940] samples: 15040, Training Loss: nan
   Time since start: 0:01:13.474188
[batch 960] samples: 15360, Training Loss: nan
   Time since start: 0:01:15.102435
[batch 980] samples: 15680, Training Loss: nan
   Time since start: 0:01:16.777341
[batch 1000] samples: 16000, Training Loss: nan
   Time since start: 0:01:18.391390
[batch 1020] samples: 16320, Training Loss: nan
   Time since start: 0:01:19.903677
[batch 1040] samples: 16640, Training Loss: nan
   Time since start: 0:01:21.627994
[batch 1060] samples: 16960, Training Loss: nan
   Time since start: 0:01:22.992148
[batch 1080] samples: 17280, Training Loss: nan
   Time since start: 0:01:24.721000
[batch 1100] samples: 17600, Training Loss: nan
   Time since start: 0:01:26.413089
[batch 1120] samples: 17920, Training Loss: nan
   Time since start: 0:01:27.958399
[batch 1140] samples: 18240, Training Loss: nan
   Time since start: 0:01:29.466307
[batch 1160] samples: 18560, Training Loss: nan
   Time since start: 0:01:30.646309
[batch 1180] samples: 18880, Training Loss: nan
   Time since start: 0:01:32.218018
[batch 1200] samples: 19200, Training Loss: nan
   Time since start: 0:01:33.854213
[batch 1220] samples: 19520, Training Loss: nan
   Time since start: 0:01:35.524750
[batch 1240] samples: 19840, Training Loss: nan
   Time since start: 0:01:36.808423
[batch 1260] samples: 20160, Training Loss: nan
   Time since start: 0:01:38.343495
[batch 1280] samples: 20480, Training Loss: nan
   Time since start: 0:01:40.032892
[batch 1300] samples: 20800, Training Loss: nan
   Time since start: 0:01:41.749833
[batch 1320] samples: 21120, Training Loss: nan
   Time since start: 0:01:43.209363
[batch 1340] samples: 21440, Training Loss: nan
   Time since start: 0:01:44.831489
[batch 1360] samples: 21760, Training Loss: nan
   Time since start: 0:01:46.291336
[batch 1380] samples: 22080, Training Loss: nan
   Time since start: 0:01:47.918941
[batch 1400] samples: 22400, Training Loss: nan
   Time since start: 0:01:49.638947
[batch 1420] samples: 22720, Training Loss: nan
   Time since start: 0:01:51.394771
[batch 1440] samples: 23040, Training Loss: nan
   Time since start: 0:01:53.165648
[batch 1460] samples: 23360, Training Loss: nan
   Time since start: 0:01:54.903551
[batch 1480] samples: 23680, Training Loss: nan
   Time since start: 0:01:56.599218
[batch 1500] samples: 24000, Training Loss: nan
   Time since start: 0:01:58.210565
[batch 1520] samples: 24320, Training Loss: nan
   Time since start: 0:01:59.755740
[batch 1540] samples: 24640, Training Loss: nan
   Time since start: 0:02:01.441034
[batch 1560] samples: 24960, Training Loss: nan
   Time since start: 0:02:03.136068
[batch 1580] samples: 25280, Training Loss: nan
   Time since start: 0:02:04.888013
[batch 1600] samples: 25600, Training Loss: nan
   Time since start: 0:02:06.486323
[batch 1620] samples: 25920, Training Loss: nan
   Time since start: 0:02:08.124645
[batch 1640] samples: 26240, Training Loss: nan
   Time since start: 0:02:09.793740
[batch 1660] samples: 26560, Training Loss: nan
   Time since start: 0:02:11.558031
[batch 1680] samples: 26880, Training Loss: nan
   Time since start: 0:02:13.314758
[batch 1700] samples: 27200, Training Loss: nan
   Time since start: 0:02:14.955868
[batch 1720] samples: 27520, Training Loss: nan
   Time since start: 0:02:16.183544
[batch 1740] samples: 27840, Training Loss: nan
   Time since start: 0:02:17.545954
[batch 1760] samples: 28160, Training Loss: nan
   Time since start: 0:02:18.991425
[batch 1780] samples: 28480, Training Loss: nan
   Time since start: 0:02:20.227958
[batch 1800] samples: 28800, Training Loss: nan
   Time since start: 0:02:21.403330
[batch 1820] samples: 29120, Training Loss: nan
   Time since start: 0:02:22.648976
[batch 1840] samples: 29440, Training Loss: nan
   Time since start: 0:02:24.242210
[batch 1860] samples: 29760, Training Loss: nan
   Time since start: 0:02:25.892767
[batch 1880] samples: 30080, Training Loss: nan
   Time since start: 0:02:27.256321
[batch 1900] samples: 30400, Training Loss: nan
   Time since start: 0:02:28.410586
[batch 1920] samples: 30720, Training Loss: nan
   Time since start: 0:02:29.847701
[batch 1940] samples: 31040, Training Loss: nan
   Time since start: 0:02:31.105789
[batch 1960] samples: 31360, Training Loss: nan
   Time since start: 0:02:32.460092
--m-Epoch 1 done.
   Training Loss: nan
   Validation Loss: nan
              precision  recall  f1-score  support  epoch  class
0                   0.0     0.0       0.0   5916.0      1      0
1                   0.0     0.0       0.0    378.0      1      1
2                   0.0     0.0       0.0   1128.0      1      2
3                   0.0     0.0       0.0    420.0      1      3
4                   0.0     0.0       0.0    576.0      1      4
5                   0.0     0.0       0.0   5688.0      1      5
6                   0.0     0.0       0.0   5040.0      1      6
7                   0.0     0.0       0.0   2226.0      1      7
8                   0.0     0.0       0.0    420.0      1      8
9                   0.0     0.0       0.0    156.0      1      9
10                  0.0     0.0       0.0   2640.0      1     10
11                  0.0     0.0       0.0    570.0      1     11
12                  0.0     0.0       0.0    324.0      1     12
13                  0.0     0.0       0.0    444.0      1     13
14                  0.0     0.0       0.0    450.0      1     14
15                  0.0     0.0       0.0    282.0      1     15
16                  0.0     0.0       0.0    396.0      1     16
17                  0.0     0.0       0.0    456.0      1     17
18                  0.0     0.0       0.0    894.0      1     18
19                  0.0     0.0       0.0    534.0      1     19
20                  0.0     0.0       0.0    156.0      1     20
21                  0.0     0.0       0.0    156.0      1     21
22                  0.0     0.0       0.0     90.0      1     22
23                  0.0     0.0       0.0    108.0      1     23
24                  0.0     0.0       0.0    156.0      1     24
25                  0.0     0.0       0.0    300.0      1     25
26                  0.0     0.0       0.0    240.0      1     26
27                  0.0     0.0       0.0    120.0      1     27
28                  0.0     0.0       0.0     54.0      1     28
29                  0.0     0.0       0.0     54.0      1     29
30                  0.0     0.0       0.0     78.0      1     30
31                  0.0     0.0       0.0    228.0      1     31
32                  0.0     0.0       0.0    264.0      1     32
33                  0.0     0.0       0.0    222.0      1     33
34                  0.0     0.0       0.0     42.0      1     34
35                  0.0     0.0       0.0     72.0      1     35
36                  0.0     0.0       0.0     66.0      1     36
37                  0.0     0.0       0.0    216.0      1     37
38                  0.0     0.0       0.0    126.0      1     38
39                  0.0     0.0       0.0    360.0      1     39
40                  0.0     0.0       0.0    414.0      1     40
41                  0.0     0.0       0.0     60.0      1     41
42                  0.0     0.0       0.0     72.0      1     42
micro avg           0.0     0.0       0.0  32592.0      1      0
macro avg           0.0     0.0       0.0  32592.0      1      1
weighted avg        0.0     0.0       0.0  32592.0      1      2
samples avg         0.0     0.0       0.0  32592.0      1      3
device: cuda
Epoch: 1 of 1
[batch 20] samples: 320, Training Loss: 1.7063
   Time since start: 0:00:01.884999
[batch 40] samples: 640, Training Loss: 0.7745
   Time since start: 0:00:03.614886
[batch 60] samples: 960, Training Loss: 0.3400
   Time since start: 0:00:05.095492
[batch 80] samples: 1280, Training Loss: 0.2388
   Time since start: 0:00:06.366511
[batch 100] samples: 1600, Training Loss: 0.2538
   Time since start: 0:00:07.639342
[batch 120] samples: 1920, Training Loss: 0.2063
   Time since start: 0:00:08.922427
[batch 140] samples: 2240, Training Loss: 0.2379
   Time since start: 0:00:10.890807
[batch 160] samples: 2560, Training Loss: 0.1497
   Time since start: 0:00:12.861660
[batch 180] samples: 2880, Training Loss: 0.1687
   Time since start: 0:00:14.834778
[batch 200] samples: 3200, Training Loss: 0.1710
   Time since start: 0:00:16.816989
[batch 220] samples: 3520, Training Loss: 0.1797
   Time since start: 0:00:18.840449
[batch 240] samples: 3840, Training Loss: 0.1371
   Time since start: 0:00:20.753304
[batch 260] samples: 4160, Training Loss: 0.1504
   Time since start: 0:00:22.664151
[batch 280] samples: 4480, Training Loss: 0.1473
   Time since start: 0:00:24.577954
[batch 300] samples: 4800, Training Loss: 0.1236
   Time since start: 0:00:26.500344
[batch 320] samples: 5120, Training Loss: 0.1265
   Time since start: 0:00:28.419889
[batch 340] samples: 5440, Training Loss: 0.1296
   Time since start: 0:00:30.251936
[batch 360] samples: 5760, Training Loss: 0.1095
   Time since start: 0:00:31.899307
[batch 380] samples: 6080, Training Loss: 0.1403
   Time since start: 0:00:33.818252
[batch 400] samples: 6400, Training Loss: 0.1313
   Time since start: 0:00:35.739424
[batch 420] samples: 6720, Training Loss: 0.0524
   Time since start: 0:00:37.653409
[batch 440] samples: 7040, Training Loss: 0.0703
   Time since start: 0:00:39.575832
[batch 460] samples: 7360, Training Loss: 0.1247
   Time since start: 0:00:41.496986
[batch 480] samples: 7680, Training Loss: 0.1165
   Time since start: 0:00:43.417764
[batch 500] samples: 8000, Training Loss: 0.1606
   Time since start: 0:00:45.244519
[batch 520] samples: 8320, Training Loss: 0.1196
   Time since start: 0:00:47.051842
[batch 540] samples: 8640, Training Loss: 0.0893
   Time since start: 0:00:48.877109
[batch 560] samples: 8960, Training Loss: 0.1220
   Time since start: 0:00:50.249088
[batch 580] samples: 9280, Training Loss: 0.0972
   Time since start: 0:00:51.629387
[batch 600] samples: 9600, Training Loss: 0.0913
   Time since start: 0:00:53.008456
[batch 620] samples: 9920, Training Loss: 0.1134
   Time since start: 0:00:54.386969
[batch 640] samples: 10240, Training Loss: 0.0817
   Time since start: 0:00:55.764188
[batch 660] samples: 10560, Training Loss: 0.0977
   Time since start: 0:00:57.143404
[batch 680] samples: 10880, Training Loss: 0.1061
   Time since start: 0:00:58.524174
[batch 700] samples: 11200, Training Loss: 0.0473
   Time since start: 0:00:59.906685
[batch 720] samples: 11520, Training Loss: 0.1030
   Time since start: 0:01:01.292804
[batch 740] samples: 11840, Training Loss: 0.0927
   Time since start: 0:01:02.676306
[batch 760] samples: 12160, Training Loss: 0.0872
   Time since start: 0:01:04.244527
[batch 780] samples: 12480, Training Loss: 0.0936
   Time since start: 0:01:06.053150
[batch 800] samples: 12800, Training Loss: 0.1038
   Time since start: 0:01:07.844546
[batch 820] samples: 13120, Training Loss: 0.0971
   Time since start: 0:01:09.635683
[batch 840] samples: 13440, Training Loss: 0.0678
   Time since start: 0:01:11.425480
[batch 860] samples: 13760, Training Loss: 0.1140
   Time since start: 0:01:13.194473
[batch 880] samples: 14080, Training Loss: 0.0694
   Time since start: 0:01:14.961552
[batch 900] samples: 14400, Training Loss: 0.0747
   Time since start: 0:01:16.732960
[batch 920] samples: 14720, Training Loss: 0.1426
   Time since start: 0:01:18.651537
[batch 940] samples: 15040, Training Loss: 0.0917
   Time since start: 0:01:20.442359
[batch 960] samples: 15360, Training Loss: 0.0837
   Time since start: 0:01:22.211692
[batch 980] samples: 15680, Training Loss: 0.0887
   Time since start: 0:01:23.986887
[batch 1000] samples: 16000, Training Loss: 0.0573
   Time since start: 0:01:25.818344
[batch 1020] samples: 16320, Training Loss: 0.0750
   Time since start: 0:01:27.651380
[batch 1040] samples: 16640, Training Loss: 0.0580
   Time since start: 0:01:29.502010
[batch 1060] samples: 16960, Training Loss: 0.0851
   Time since start: 0:01:31.324427
[batch 1080] samples: 17280, Training Loss: 0.0731
   Time since start: 0:01:33.185394
[batch 1100] samples: 17600, Training Loss: 0.0677
   Time since start: 0:01:35.057755
[batch 1120] samples: 17920, Training Loss: 0.1085
   Time since start: 0:01:36.929447
[batch 1140] samples: 18240, Training Loss: 0.0681
   Time since start: 0:01:38.811022
[batch 1160] samples: 18560, Training Loss: 0.0839
   Time since start: 0:01:40.682390
[batch 1180] samples: 18880, Training Loss: 0.0790
   Time since start: 0:01:42.604524
[batch 1200] samples: 19200, Training Loss: 0.0648
   Time since start: 0:01:44.514255
[batch 1220] samples: 19520, Training Loss: 0.0751
   Time since start: 0:01:46.426114
[batch 1240] samples: 19840, Training Loss: 0.0756
   Time since start: 0:01:48.338246
[batch 1260] samples: 20160, Training Loss: 0.0602
   Time since start: 0:01:50.299717
[batch 1280] samples: 20480, Training Loss: 0.1178
   Time since start: 0:01:52.272469
[batch 1300] samples: 20800, Training Loss: 0.0905
   Time since start: 0:01:54.244461
[batch 1320] samples: 21120, Training Loss: 0.0845
   Time since start: 0:01:56.215078
[batch 1340] samples: 21440, Training Loss: 0.1108
   Time since start: 0:01:58.186881
[batch 1360] samples: 21760, Training Loss: 0.0669
   Time since start: 0:02:00.150202
[batch 1380] samples: 22080, Training Loss: 0.0556
   Time since start: 0:02:02.118825
[batch 1400] samples: 22400, Training Loss: 0.1003
   Time since start: 0:02:04.089848
[batch 1420] samples: 22720, Training Loss: 0.0841
   Time since start: 0:02:06.060361
[batch 1440] samples: 23040, Training Loss: 0.0479
   Time since start: 0:02:08.011108
[batch 1460] samples: 23360, Training Loss: 0.0478
   Time since start: 0:02:09.973339
[batch 1480] samples: 23680, Training Loss: 0.0468
   Time since start: 0:02:11.943669
[batch 1500] samples: 24000, Training Loss: 0.0704
   Time since start: 0:02:13.913933
[batch 1520] samples: 24320, Training Loss: 0.0630
   Time since start: 0:02:15.884266
[batch 1540] samples: 24640, Training Loss: 0.0567
   Time since start: 0:02:17.854420
[batch 1560] samples: 24960, Training Loss: 0.0613
   Time since start: 0:02:19.826581
[batch 1580] samples: 25280, Training Loss: 0.0738
   Time since start: 0:02:21.798171
[batch 1600] samples: 25600, Training Loss: 0.0475
   Time since start: 0:02:23.770331
[batch 1620] samples: 25920, Training Loss: 0.0547
   Time since start: 0:02:25.739637
[batch 1640] samples: 26240, Training Loss: 0.0432
   Time since start: 0:02:27.712724
[batch 1660] samples: 26560, Training Loss: 0.0706
   Time since start: 0:02:29.682842
[batch 1680] samples: 26880, Training Loss: 0.0513
   Time since start: 0:02:31.653124
[batch 1700] samples: 27200, Training Loss: 0.0789
   Time since start: 0:02:33.233454
[batch 1720] samples: 27520, Training Loss: 0.0680
   Time since start: 0:02:34.480456
[batch 1740] samples: 27840, Training Loss: 0.0474
   Time since start: 0:02:35.728812
[batch 1760] samples: 28160, Training Loss: 0.0544
   Time since start: 0:02:37.304256
[batch 1780] samples: 28480, Training Loss: 0.0534
   Time since start: 0:02:39.048465
[batch 1800] samples: 28800, Training Loss: 0.0429
   Time since start: 0:02:40.821646
[batch 1820] samples: 29120, Training Loss: 0.0546
   Time since start: 0:02:42.629757
[batch 1840] samples: 29440, Training Loss: 0.0869
   Time since start: 0:02:44.419791
[batch 1860] samples: 29760, Training Loss: 0.0800
   Time since start: 0:02:46.207424
[batch 1880] samples: 30080, Training Loss: 0.0271
   Time since start: 0:02:48.000779
[batch 1900] samples: 30400, Training Loss: 0.0779
   Time since start: 0:02:49.685261
[batch 1920] samples: 30720, Training Loss: 0.0710
   Time since start: 0:02:51.162937
[batch 1940] samples: 31040, Training Loss: 0.0472
   Time since start: 0:02:52.628155
[batch 1960] samples: 31360, Training Loss: 0.0668
   Time since start: 0:02:54.447002
--m-Epoch 1 done.
   Training Loss: 0.1370
   Validation Loss: 0.0460
              precision    recall  f1-score  support  epoch  class
0              0.990447  0.998986  0.994698   5916.0      1      0
1              0.997319  0.984127  0.990679    378.0      1      1
2              0.988351  0.977837  0.983066   1128.0      1      2
3              0.995050  0.957143  0.975728    420.0      1      3
4              0.994575  0.954861  0.974314    576.0      1      4
5              0.984738  0.998242  0.991444   5688.0      1      5
6              0.991261  0.990278  0.990769   5040.0      1      6
7              0.972185  0.989218  0.980628   2226.0      1      7
8              0.995050  0.957143  0.975728    420.0      1      8
9              1.000000  0.942308  0.970297    156.0      1      9
10             0.934331  0.970076  0.951868   2640.0      1     10
11             0.568123  0.775439  0.655786    570.0      1     11
12             0.426056  0.746914  0.542601    324.0      1     12
13             0.800885  0.407658  0.540299    444.0      1     13
14             0.522305  0.624444  0.568826    450.0      1     14
15             0.824000  0.365248  0.506143    282.0      1     15
16             0.993750  0.401515  0.571942    396.0      1     16
17             0.613445  0.160088  0.253913    456.0      1     17
18             0.822016  0.893736  0.856377    894.0      1     18
19             0.641960  0.956929  0.768421    534.0      1     19
20             1.000000  0.942308  0.970297    156.0      1     20
21             0.370000  0.711538  0.486842    156.0      1     21
22             0.661017  0.433333  0.523490     90.0      1     22
23             0.776471  0.611111  0.683938    108.0      1     23
24             0.641026  0.480769  0.549451    156.0      1     24
25             0.876033  0.706667  0.782288    300.0      1     25
26             0.843137  0.179167  0.295533    240.0      1     26
27             0.563910  0.625000  0.592885    120.0      1     27
28             0.000000  0.000000  0.000000     54.0      1     28
29             0.000000  0.000000  0.000000     54.0      1     29
30             0.692308  0.576923  0.629371     78.0      1     30
31             0.922747  0.942982  0.932755    228.0      1     31
32             0.693487  0.685606  0.689524    264.0      1     32
33             0.951327  0.968468  0.959821    222.0      1     33
34             0.000000  0.000000  0.000000     42.0      1     34
35             0.000000  0.000000  0.000000     72.0      1     35
36             0.000000  0.000000  0.000000     66.0      1     36
37             0.720588  0.907407  0.803279    216.0      1     37
38             1.000000  0.317460  0.481928    126.0      1     38
39             0.971831  0.958333  0.965035    360.0      1     39
40             0.848101  0.971014  0.905405    414.0      1     40
41             0.500000  0.916667  0.647059     60.0      1     41
42             0.790123  0.888889  0.836601     72.0      1     42
micro avg      0.917379  0.909272  0.913307  32592.0      1      0
macro avg      0.718092  0.671531  0.669280  32592.0      1      1
weighted avg   0.917336  0.909272  0.903854  32592.0      1      2
samples avg    0.928253  0.914376  0.915706  32592.0      1      3
device: cuda
Epoch: 1 of 1
[batch 20] samples: 320, Training Loss: 0.2312
   Time since start: 0:00:01.904093
[batch 40] samples: 640, Training Loss: 0.2141
   Time since start: 0:00:03.671873
[batch 60] samples: 960, Training Loss: 0.2555
   Time since start: 0:00:05.400668
[batch 80] samples: 1280, Training Loss: 0.1980
   Time since start: 0:00:07.170059
[batch 100] samples: 1600, Training Loss: 0.1587
   Time since start: 0:00:08.998283
[batch 120] samples: 1920, Training Loss: 0.1781
   Time since start: 0:00:10.880046
[batch 140] samples: 2240, Training Loss: 0.1907
   Time since start: 0:00:12.777515
[batch 160] samples: 2560, Training Loss: 0.1669
   Time since start: 0:00:14.649987
[batch 180] samples: 2880, Training Loss: 0.1923
   Time since start: 0:00:16.522150
[batch 200] samples: 3200, Training Loss: 0.1409
   Time since start: 0:00:18.380703
[batch 220] samples: 3520, Training Loss: 0.1447
   Time since start: 0:00:20.250540
[batch 240] samples: 3840, Training Loss: 0.1232
   Time since start: 0:00:22.121260
[batch 260] samples: 4160, Training Loss: 0.0950
   Time since start: 0:00:24.023349
[batch 280] samples: 4480, Training Loss: 0.0930
   Time since start: 0:00:25.836566
[batch 300] samples: 4800, Training Loss: 0.0943
   Time since start: 0:00:27.608754
[batch 320] samples: 5120, Training Loss: 0.0896
   Time since start: 0:00:29.381379
[batch 340] samples: 5440, Training Loss: 0.1255
   Time since start: 0:00:31.160192
[batch 360] samples: 5760, Training Loss: 0.0843
   Time since start: 0:00:32.934966
[batch 380] samples: 6080, Training Loss: 0.0928
   Time since start: 0:00:34.721040
[batch 400] samples: 6400, Training Loss: 0.0804
   Time since start: 0:00:36.490798
[batch 420] samples: 6720, Training Loss: 0.0847
   Time since start: 0:00:38.258979
[batch 440] samples: 7040, Training Loss: 0.0837
   Time since start: 0:00:40.028099
[batch 460] samples: 7360, Training Loss: 0.0900
   Time since start: 0:00:41.798489
[batch 480] samples: 7680, Training Loss: 0.1077
   Time since start: 0:00:43.569820
[batch 500] samples: 8000, Training Loss: 0.0871
   Time since start: 0:00:45.341296
[batch 520] samples: 8320, Training Loss: 0.0935
   Time since start: 0:00:47.262510
[batch 540] samples: 8640, Training Loss: 0.0759
   Time since start: 0:00:49.040424
[batch 560] samples: 8960, Training Loss: 0.0571
   Time since start: 0:00:50.819447
[batch 580] samples: 9280, Training Loss: 0.0603
   Time since start: 0:00:52.601915
[batch 600] samples: 9600, Training Loss: 0.0641
   Time since start: 0:00:54.378721
[batch 620] samples: 9920, Training Loss: 0.0746
   Time since start: 0:00:56.009789
[batch 640] samples: 10240, Training Loss: 0.0707
   Time since start: 0:00:57.328129
[batch 660] samples: 10560, Training Loss: 0.0657
   Time since start: 0:00:58.661183
[batch 680] samples: 10880, Training Loss: 0.0808
   Time since start: 0:00:59.994850
[batch 700] samples: 11200, Training Loss: 0.0973
   Time since start: 0:01:01.328895
[batch 720] samples: 11520, Training Loss: 0.0549
   Time since start: 0:01:03.111067
[batch 740] samples: 11840, Training Loss: 0.0624
   Time since start: 0:01:04.983411
[batch 760] samples: 12160, Training Loss: 0.0596
   Time since start: 0:01:06.859590
[batch 780] samples: 12480, Training Loss: 0.0472
   Time since start: 0:01:08.728055
[batch 800] samples: 12800, Training Loss: 0.0408
   Time since start: 0:01:10.597386
[batch 820] samples: 13120, Training Loss: 0.0612
   Time since start: 0:01:12.461969
[batch 840] samples: 13440, Training Loss: 0.0539
   Time since start: 0:01:14.328979
[batch 860] samples: 13760, Training Loss: 0.0471
   Time since start: 0:01:16.198700
[batch 880] samples: 14080, Training Loss: 0.0696
   Time since start: 0:01:18.066976
[batch 900] samples: 14400, Training Loss: 0.0564
   Time since start: 0:01:19.932091
[batch 920] samples: 14720, Training Loss: 0.0486
   Time since start: 0:01:21.796337
[batch 940] samples: 15040, Training Loss: 0.0417
   Time since start: 0:01:23.666301
[batch 960] samples: 15360, Training Loss: 0.0604
   Time since start: 0:01:25.540329
[batch 980] samples: 15680, Training Loss: 0.0328
   Time since start: 0:01:27.409842
[batch 1000] samples: 16000, Training Loss: 0.0571
   Time since start: 0:01:29.284804
[batch 1020] samples: 16320, Training Loss: 0.0803
   Time since start: 0:01:31.156696
[batch 1040] samples: 16640, Training Loss: 0.0350
   Time since start: 0:01:33.025899
[batch 1060] samples: 16960, Training Loss: 0.0345
   Time since start: 0:01:34.898491
[batch 1080] samples: 17280, Training Loss: 0.0466
   Time since start: 0:01:36.770259
[batch 1100] samples: 17600, Training Loss: 0.0769
   Time since start: 0:01:38.639043
[batch 1120] samples: 17920, Training Loss: 0.0426
   Time since start: 0:01:40.542213
[batch 1140] samples: 18240, Training Loss: 0.0311
   Time since start: 0:01:42.090109
[batch 1160] samples: 18560, Training Loss: 0.0498
   Time since start: 0:01:43.432877
[batch 1180] samples: 18880, Training Loss: 0.0504
   Time since start: 0:01:44.842166
[batch 1200] samples: 19200, Training Loss: 0.0462
   Time since start: 0:01:46.725981
[batch 1220] samples: 19520, Training Loss: 0.0440
   Time since start: 0:01:48.594421
[batch 1240] samples: 19840, Training Loss: 0.0578
   Time since start: 0:01:50.472332
[batch 1260] samples: 20160, Training Loss: 0.0568
   Time since start: 0:01:52.357414
[batch 1280] samples: 20480, Training Loss: 0.0490
   Time since start: 0:01:54.244670
[batch 1300] samples: 20800, Training Loss: 0.0598
   Time since start: 0:01:56.124763
[batch 1320] samples: 21120, Training Loss: 0.0840
   Time since start: 0:01:57.926155
[batch 1340] samples: 21440, Training Loss: 0.0850
   Time since start: 0:01:59.735194
[batch 1360] samples: 21760, Training Loss: 0.0490
   Time since start: 0:02:01.534594
[batch 1380] samples: 22080, Training Loss: 0.0280
   Time since start: 0:02:03.407605
[batch 1400] samples: 22400, Training Loss: 0.0239
   Time since start: 0:02:05.277046
[batch 1420] samples: 22720, Training Loss: 0.0296
   Time since start: 0:02:07.136986
[batch 1440] samples: 23040, Training Loss: 0.0438
   Time since start: 0:02:09.007216
[batch 1460] samples: 23360, Training Loss: 0.0629
   Time since start: 0:02:10.870373
[batch 1480] samples: 23680, Training Loss: 0.0823
   Time since start: 0:02:12.724107
[batch 1500] samples: 24000, Training Loss: 0.0435
   Time since start: 0:02:14.605968
[batch 1520] samples: 24320, Training Loss: 0.0301
   Time since start: 0:02:16.474192
[batch 1540] samples: 24640, Training Loss: 0.0276
   Time since start: 0:02:18.334853
[batch 1560] samples: 24960, Training Loss: 0.0178
   Time since start: 0:02:20.132635
[batch 1580] samples: 25280, Training Loss: 0.0362
   Time since start: 0:02:21.938430
[batch 1600] samples: 25600, Training Loss: 0.0318
   Time since start: 0:02:23.821174
[batch 1620] samples: 25920, Training Loss: 0.0462
   Time since start: 0:02:25.700234
[batch 1640] samples: 26240, Training Loss: 0.0312
   Time since start: 0:02:27.581079
[batch 1660] samples: 26560, Training Loss: 0.0239
   Time since start: 0:02:29.463734
[batch 1680] samples: 26880, Training Loss: 0.0250
   Time since start: 0:02:31.331412
[batch 1700] samples: 27200, Training Loss: 0.0257
   Time since start: 0:02:33.212882
[batch 1720] samples: 27520, Training Loss: 0.0429
   Time since start: 0:02:35.083685
[batch 1740] samples: 27840, Training Loss: 0.0495
   Time since start: 0:02:36.954940
[batch 1760] samples: 28160, Training Loss: 0.0444
   Time since start: 0:02:38.825198
[batch 1780] samples: 28480, Training Loss: 0.0220
   Time since start: 0:02:40.707085
[batch 1800] samples: 28800, Training Loss: 0.0447
   Time since start: 0:02:42.589395
[batch 1820] samples: 29120, Training Loss: 0.0280
   Time since start: 0:02:44.459664
[batch 1840] samples: 29440, Training Loss: 0.0215
   Time since start: 0:02:46.329426
[batch 1860] samples: 29760, Training Loss: 0.0581
   Time since start: 0:02:48.200619
[batch 1880] samples: 30080, Training Loss: 0.0573
   Time since start: 0:02:50.120730
[batch 1900] samples: 30400, Training Loss: 0.0283
   Time since start: 0:02:52.195395
[batch 1920] samples: 30720, Training Loss: 0.0199
   Time since start: 0:02:54.116767
[batch 1940] samples: 31040, Training Loss: 0.0299
   Time since start: 0:02:56.067274
[batch 1960] samples: 31360, Training Loss: 0.0397
   Time since start: 0:02:57.987741
--m-Epoch 1 done.
   Training Loss: 0.0741
   Validation Loss: 0.0242
              precision    recall  f1-score  support  epoch  class
0              0.999488  0.990703  0.995076   5916.0      1      0
1              0.994737  1.000000  0.997361    378.0      1      1
2              0.960000  1.000000  0.979592   1128.0      1      2
3              1.000000  0.983333  0.991597    420.0      1      3
4              0.981164  0.994792  0.987931    576.0      1      4
5              0.999468  0.991210  0.995322   5688.0      1      5
6              0.992291  0.996032  0.994158   5040.0      1      6
7              0.997709  0.977987  0.987750   2226.0      1      7
8              1.000000  0.983333  0.991597    420.0      1      8
9              0.951220  1.000000  0.975000    156.0      1      9
10             0.984481  0.985227  0.984854   2640.0      1     10
11             0.921603  0.928070  0.924825    570.0      1     11
12             0.576923  0.879630  0.696822    324.0      1     12
13             0.948171  0.700450  0.805699    444.0      1     13
14             0.855072  0.655556  0.742138    450.0      1     14
15             0.731214  0.897163  0.805732    282.0      1     15
16             0.824786  0.974747  0.893519    396.0      1     16
17             0.645796  0.791667  0.711330    456.0      1     17
18             0.967089  0.854586  0.907363    894.0      1     18
19             0.954092  0.895131  0.923671    534.0      1     19
20             0.951220  1.000000  0.975000    156.0      1     20
21             0.661836  0.878205  0.754821    156.0      1     21
22             0.851852  0.511111  0.638889     90.0      1     22
23             0.950617  0.712963  0.814815    108.0      1     23
24             0.984375  0.403846  0.572727    156.0      1     24
25             0.953333  0.953333  0.953333    300.0      1     25
26             0.885572  0.741667  0.807256    240.0      1     26
27             1.000000  0.483333  0.651685    120.0      1     27
28             0.308943  0.703704  0.429379     54.0      1     28
29             0.551282  0.796296  0.651515     54.0      1     29
30             1.000000  0.358974  0.528302     78.0      1     30
31             0.961702  0.991228  0.976242    228.0      1     31
32             0.892157  0.689394  0.777778    264.0      1     32
33             1.000000  0.977477  0.988610    222.0      1     33
34             1.000000  0.404762  0.576271     42.0      1     34
35             0.818182  0.625000  0.708661     72.0      1     35
36             1.000000  0.015152  0.029851     66.0      1     36
37             0.646341  0.981481  0.779412    216.0      1     37
38             0.801527  0.833333  0.817121    126.0      1     38
39             0.801822  0.977778  0.881101    360.0      1     39
40             0.979434  0.920290  0.948941    414.0      1     40
41             0.865385  0.750000  0.803571     60.0      1     41
42             0.857143  0.916667  0.885906     72.0      1     42
micro avg      0.955405  0.950509  0.952951  32592.0      1      0
macro avg      0.883908  0.816410  0.819594  32592.0      1      1
weighted avg   0.962520  0.950509  0.952021  32592.0      1      2
samples avg    0.959814  0.951556  0.952006  32592.0      1      3
device: cuda
Epoch: 1 of 1
[batch 20] samples: 320, Training Loss: 0.0948
   Time since start: 0:00:02.025433
[batch 40] samples: 640, Training Loss: 0.1135
   Time since start: 0:00:03.946785
[batch 60] samples: 960, Training Loss: 0.0761
   Time since start: 0:00:05.879907
[batch 80] samples: 1280, Training Loss: 0.0811
   Time since start: 0:00:07.811568
[batch 100] samples: 1600, Training Loss: 0.0628
   Time since start: 0:00:09.743447
[batch 120] samples: 1920, Training Loss: 0.0573
   Time since start: 0:00:11.683408
[batch 140] samples: 2240, Training Loss: 0.0738
   Time since start: 0:00:13.615725
[batch 160] samples: 2560, Training Loss: 0.0300
   Time since start: 0:00:15.711346
[batch 180] samples: 2880, Training Loss: 0.0450
   Time since start: 0:00:17.644658
[batch 200] samples: 3200, Training Loss: 0.0347
   Time since start: 0:00:19.585207
[batch 220] samples: 3520, Training Loss: 0.0243
   Time since start: 0:00:21.519630
[batch 240] samples: 3840, Training Loss: 0.0193
   Time since start: 0:00:23.459461
[batch 260] samples: 4160, Training Loss: 0.0257
   Time since start: 0:00:25.382400
[batch 280] samples: 4480, Training Loss: 0.0269
   Time since start: 0:00:27.303607
[batch 300] samples: 4800, Training Loss: 0.0145
   Time since start: 0:00:29.225235
[batch 320] samples: 5120, Training Loss: 0.0236
   Time since start: 0:00:31.145686
[batch 340] samples: 5440, Training Loss: 0.0194
   Time since start: 0:00:33.067827
[batch 360] samples: 5760, Training Loss: 0.0656
   Time since start: 0:00:34.897807
[batch 380] samples: 6080, Training Loss: 0.0182
   Time since start: 0:00:36.578132
[batch 400] samples: 6400, Training Loss: 0.0340
   Time since start: 0:00:38.275103
[batch 420] samples: 6720, Training Loss: 0.0213
   Time since start: 0:00:40.065862
[batch 440] samples: 7040, Training Loss: 0.0144
   Time since start: 0:00:41.669201
[batch 460] samples: 7360, Training Loss: 0.0176
   Time since start: 0:00:43.190464
[batch 480] samples: 7680, Training Loss: 0.0207
   Time since start: 0:00:44.997725
[batch 500] samples: 8000, Training Loss: 0.0083
   Time since start: 0:00:46.810983
[batch 520] samples: 8320, Training Loss: 0.0019
   Time since start: 0:00:48.622628
[batch 540] samples: 8640, Training Loss: 0.0068
   Time since start: 0:00:50.424546
[batch 560] samples: 8960, Training Loss: 0.0121
   Time since start: 0:00:52.209299
[batch 580] samples: 9280, Training Loss: 0.0265
   Time since start: 0:00:54.023318
[batch 600] samples: 9600, Training Loss: 0.0130
   Time since start: 0:00:55.893443
[batch 620] samples: 9920, Training Loss: 0.0181
   Time since start: 0:00:57.761755
[batch 640] samples: 10240, Training Loss: 0.0053
   Time since start: 0:00:59.615412
[batch 660] samples: 10560, Training Loss: 0.0151
   Time since start: 0:01:01.476073
[batch 680] samples: 10880, Training Loss: 0.0063
   Time since start: 0:01:03.348119
[batch 700] samples: 11200, Training Loss: 0.0236
   Time since start: 0:01:05.207920
[batch 720] samples: 11520, Training Loss: 0.0301
   Time since start: 0:01:07.077591
[batch 740] samples: 11840, Training Loss: 0.0038
   Time since start: 0:01:09.008423
[batch 760] samples: 12160, Training Loss: 0.0198
   Time since start: 0:01:10.928958
[batch 780] samples: 12480, Training Loss: 0.0209
   Time since start: 0:01:12.849918
[batch 800] samples: 12800, Training Loss: 0.0100
   Time since start: 0:01:14.770366
[batch 820] samples: 13120, Training Loss: 0.0180
   Time since start: 0:01:16.700127
[batch 840] samples: 13440, Training Loss: 0.0053
   Time since start: 0:01:18.620391
[batch 860] samples: 13760, Training Loss: 0.0035
   Time since start: 0:01:20.538844
[batch 880] samples: 14080, Training Loss: 0.0028
   Time since start: 0:01:22.460478
[batch 900] samples: 14400, Training Loss: 0.0282
   Time since start: 0:01:24.379484
[batch 920] samples: 14720, Training Loss: 0.0009
   Time since start: 0:01:26.300540
[batch 940] samples: 15040, Training Loss: 0.0029
   Time since start: 0:01:28.220826
[batch 960] samples: 15360, Training Loss: 0.0018
   Time since start: 0:01:30.140828
[batch 980] samples: 15680, Training Loss: 0.0103
   Time since start: 0:01:32.032825
[batch 1000] samples: 16000, Training Loss: 0.0055
   Time since start: 0:01:33.885978
[batch 1020] samples: 16320, Training Loss: 0.0015
   Time since start: 0:01:35.739197
[batch 1040] samples: 16640, Training Loss: 0.0024
   Time since start: 0:01:37.590960
[batch 1060] samples: 16960, Training Loss: 0.0015
   Time since start: 0:01:39.423073
[batch 1080] samples: 17280, Training Loss: 0.0020
   Time since start: 0:01:41.232083
[batch 1100] samples: 17600, Training Loss: 0.0052
   Time since start: 0:01:43.104860
[batch 1120] samples: 17920, Training Loss: 0.0170
   Time since start: 0:01:44.976047
[batch 1140] samples: 18240, Training Loss: 0.0129
   Time since start: 0:01:46.826952
[batch 1160] samples: 18560, Training Loss: 0.0151
   Time since start: 0:01:48.697329
[batch 1180] samples: 18880, Training Loss: 0.0020
   Time since start: 0:01:50.569979
[batch 1200] samples: 19200, Training Loss: 0.0211
   Time since start: 0:01:52.431217
[batch 1220] samples: 19520, Training Loss: 0.0128
   Time since start: 0:01:54.300239
[batch 1240] samples: 19840, Training Loss: 0.0095
   Time since start: 0:01:56.159764
[batch 1260] samples: 20160, Training Loss: 0.0041
   Time since start: 0:01:58.013521
[batch 1280] samples: 20480, Training Loss: 0.0037
   Time since start: 0:01:59.825517
[batch 1300] samples: 20800, Training Loss: 0.0019
   Time since start: 0:02:01.646831
[batch 1320] samples: 21120, Training Loss: 0.0110
   Time since start: 0:02:03.447797
[batch 1340] samples: 21440, Training Loss: 0.0008
   Time since start: 0:02:05.259392
[batch 1360] samples: 21760, Training Loss: 0.0009
   Time since start: 0:02:07.059450
[batch 1380] samples: 22080, Training Loss: 0.0013
   Time since start: 0:02:08.850490
[batch 1400] samples: 22400, Training Loss: 0.0009
   Time since start: 0:02:10.649752
[batch 1420] samples: 22720, Training Loss: 0.0007
   Time since start: 0:02:12.439198
[batch 1440] samples: 23040, Training Loss: 0.0011
   Time since start: 0:02:14.240986
[batch 1460] samples: 23360, Training Loss: 0.0007
   Time since start: 0:02:15.930641
[batch 1480] samples: 23680, Training Loss: 0.0045
   Time since start: 0:02:17.810705
[batch 1500] samples: 24000, Training Loss: 0.0029
   Time since start: 0:02:19.681578
[batch 1520] samples: 24320, Training Loss: 0.0022
   Time since start: 0:02:21.542546
[batch 1540] samples: 24640, Training Loss: 0.0037
   Time since start: 0:02:23.546636
[batch 1560] samples: 24960, Training Loss: 0.0106
   Time since start: 0:02:25.428040
[batch 1580] samples: 25280, Training Loss: 0.0013
   Time since start: 0:02:27.240221
[batch 1600] samples: 25600, Training Loss: 0.0083
   Time since start: 0:02:29.024371
[batch 1620] samples: 25920, Training Loss: 0.0006
   Time since start: 0:02:30.807317
[batch 1640] samples: 26240, Training Loss: 0.0025
   Time since start: 0:02:32.564833
[batch 1660] samples: 26560, Training Loss: 0.0006
   Time since start: 0:02:34.334740
[batch 1680] samples: 26880, Training Loss: 0.0026
   Time since start: 0:02:36.116193
[batch 1700] samples: 27200, Training Loss: 0.0005
   Time since start: 0:02:37.966632
[batch 1720] samples: 27520, Training Loss: 0.0187
   Time since start: 0:02:39.835021
[batch 1740] samples: 27840, Training Loss: 0.0027
   Time since start: 0:02:41.704439
[batch 1760] samples: 28160, Training Loss: 0.0029
   Time since start: 0:02:43.577722
[batch 1780] samples: 28480, Training Loss: 0.0003
   Time since start: 0:02:45.454854
[batch 1800] samples: 28800, Training Loss: 0.0016
   Time since start: 0:02:47.333616
[batch 1820] samples: 29120, Training Loss: 0.0006
   Time since start: 0:02:49.192391
[batch 1840] samples: 29440, Training Loss: 0.0011
   Time since start: 0:02:51.066314
[batch 1860] samples: 29760, Training Loss: 0.0010
   Time since start: 0:02:52.948406
[batch 1880] samples: 30080, Training Loss: 0.0046
   Time since start: 0:02:54.840322
[batch 1900] samples: 30400, Training Loss: 0.0074
   Time since start: 0:02:56.731732
[batch 1920] samples: 30720, Training Loss: 0.0002
   Time since start: 0:02:58.654053
[batch 1940] samples: 31040, Training Loss: 0.0045
   Time since start: 0:03:00.596497
[batch 1960] samples: 31360, Training Loss: 0.0007
   Time since start: 0:03:02.483499
--m-Epoch 1 done.
   Training Loss: 0.0182
   Validation Loss: 0.0016
              precision    recall  f1-score  support  epoch  class
0              1.000000  1.000000  1.000000   5916.0      1      0
1              1.000000  1.000000  1.000000    378.0      1      1
2              1.000000  1.000000  1.000000   1128.0      1      2
3              0.997625  1.000000  0.998811    420.0      1      3
4              0.998267  1.000000  0.999133    576.0      1      4
5              1.000000  1.000000  1.000000   5688.0      1      5
6              1.000000  0.999802  0.999901   5040.0      1      6
7              1.000000  1.000000  1.000000   2226.0      1      7
8              0.997625  1.000000  0.998811    420.0      1      8
9              1.000000  1.000000  1.000000    156.0      1      9
10             1.000000  0.998106  0.999052   2640.0      1     10
11             0.998106  0.924561  0.959927    570.0      1     11
12             0.993506  0.944444  0.968354    324.0      1     12
13             1.000000  0.997748  0.998873    444.0      1     13
14             0.995575  1.000000  0.997783    450.0      1     14
15             0.992857  0.985816  0.989324    282.0      1     15
16             1.000000  1.000000  1.000000    396.0      1     16
17             0.974194  0.993421  0.983713    456.0      1     17
18             0.995526  0.995526  0.995526    894.0      1     18
19             1.000000  0.917603  0.957031    534.0      1     19
20             1.000000  1.000000  1.000000    156.0      1     20
21             0.993631  1.000000  0.996805    156.0      1     21
22             1.000000  1.000000  1.000000     90.0      1     22
23             1.000000  0.962963  0.981132    108.0      1     23
24             1.000000  0.987179  0.993548    156.0      1     24
25             1.000000  1.000000  1.000000    300.0      1     25
26             1.000000  1.000000  1.000000    240.0      1     26
27             1.000000  1.000000  1.000000    120.0      1     27
28             0.981818  1.000000  0.990826     54.0      1     28
29             0.964286  1.000000  0.981818     54.0      1     29
30             1.000000  1.000000  1.000000     78.0      1     30
31             1.000000  1.000000  1.000000    228.0      1     31
32             1.000000  1.000000  1.000000    264.0      1     32
33             1.000000  1.000000  1.000000    222.0      1     33
34             1.000000  1.000000  1.000000     42.0      1     34
35             1.000000  1.000000  1.000000     72.0      1     35
36             1.000000  0.984848  0.992366     66.0      1     36
37             0.986301  1.000000  0.993103    216.0      1     37
38             1.000000  1.000000  1.000000    126.0      1     38
39             0.997230  1.000000  0.998613    360.0      1     39
40             1.000000  1.000000  1.000000    414.0      1     40
41             1.000000  1.000000  1.000000     60.0      1     41
42             1.000000  1.000000  1.000000     72.0      1     42
micro avg      0.998954  0.996011  0.997480  32592.0      1      0
macro avg      0.996896  0.992838  0.994755  32592.0      1      1
weighted avg   0.998963  0.996011  0.997423  32592.0      1      2
samples avg    0.999146  0.996893  0.997829  32592.0      1      3
device: cuda
Epoch: 1 of 1
[batch 20] samples: 320, Training Loss: 0.5049
   Time since start: 0:00:01.995064
[batch 40] samples: 640, Training Loss: 0.2569
   Time since start: 0:00:03.854809
[batch 60] samples: 960, Training Loss: 0.1904
   Time since start: 0:00:05.746732
[batch 80] samples: 1280, Training Loss: 0.1241
   Time since start: 0:00:07.668850
[batch 100] samples: 1600, Training Loss: 0.1142
   Time since start: 0:00:09.592658
[batch 120] samples: 1920, Training Loss: 0.0977
   Time since start: 0:00:11.516134
[batch 140] samples: 2240, Training Loss: 0.0939
   Time since start: 0:00:13.439257
[batch 160] samples: 2560, Training Loss: 0.0917
   Time since start: 0:00:15.362095
[batch 180] samples: 2880, Training Loss: 0.0741
   Time since start: 0:00:17.294737
[batch 200] samples: 3200, Training Loss: 0.0767
   Time since start: 0:00:19.227749
[batch 220] samples: 3520, Training Loss: 0.0654
   Time since start: 0:00:21.149693
[batch 240] samples: 3840, Training Loss: 0.0749
   Time since start: 0:00:23.083024
[batch 260] samples: 4160, Training Loss: 0.0468
   Time since start: 0:00:24.946648
[batch 280] samples: 4480, Training Loss: 0.0654
   Time since start: 0:00:26.790831
[batch 300] samples: 4800, Training Loss: 0.0407
   Time since start: 0:00:28.649406
[batch 320] samples: 5120, Training Loss: 0.0590
   Time since start: 0:00:30.531354
[batch 340] samples: 5440, Training Loss: 0.0387
   Time since start: 0:00:32.374053
[batch 360] samples: 5760, Training Loss: 0.0317
   Time since start: 0:00:34.215474
[batch 380] samples: 6080, Training Loss: 0.0307
   Time since start: 0:00:36.056818
[batch 400] samples: 6400, Training Loss: 0.0299
   Time since start: 0:00:37.898823
[batch 420] samples: 6720, Training Loss: 0.0441
   Time since start: 0:00:39.729272
[batch 440] samples: 7040, Training Loss: 0.0298
   Time since start: 0:00:41.570756
[batch 460] samples: 7360, Training Loss: 0.0227
   Time since start: 0:00:43.411912
[batch 480] samples: 7680, Training Loss: 0.0261
   Time since start: 0:00:45.253845
[batch 500] samples: 8000, Training Loss: 0.0512
   Time since start: 0:00:47.104940
[batch 520] samples: 8320, Training Loss: 0.0265
   Time since start: 0:00:48.955961
[batch 540] samples: 8640, Training Loss: 0.0154
   Time since start: 0:00:50.815148
[batch 560] samples: 8960, Training Loss: 0.0218
   Time since start: 0:00:52.667695
[batch 580] samples: 9280, Training Loss: 0.0301
   Time since start: 0:00:54.517809
[batch 600] samples: 9600, Training Loss: 0.0183
   Time since start: 0:00:56.418040
[batch 620] samples: 9920, Training Loss: 0.0125
   Time since start: 0:00:58.339331
[batch 640] samples: 10240, Training Loss: 0.0190
   Time since start: 0:01:00.281821
[batch 660] samples: 10560, Training Loss: 0.0122
   Time since start: 0:01:02.194101
[batch 680] samples: 10880, Training Loss: 0.0099
   Time since start: 0:01:04.065378
[batch 700] samples: 11200, Training Loss: 0.0182
   Time since start: 0:01:05.936343
[batch 720] samples: 11520, Training Loss: 0.0116
   Time since start: 0:01:07.806631
[batch 740] samples: 11840, Training Loss: 0.0119
   Time since start: 0:01:09.688114
[batch 760] samples: 12160, Training Loss: 0.0092
   Time since start: 0:01:11.577222
[batch 780] samples: 12480, Training Loss: 0.0138
   Time since start: 0:01:13.451693
[batch 800] samples: 12800, Training Loss: 0.0140
   Time since start: 0:01:15.322871
[batch 820] samples: 13120, Training Loss: 0.0135
   Time since start: 0:01:17.205964
[batch 840] samples: 13440, Training Loss: 0.0117
   Time since start: 0:01:19.087595
[batch 860] samples: 13760, Training Loss: 0.0122
   Time since start: 0:01:20.958277
[batch 880] samples: 14080, Training Loss: 0.0068
   Time since start: 0:01:22.830425
[batch 900] samples: 14400, Training Loss: 0.0051
   Time since start: 0:01:24.713583
[batch 920] samples: 14720, Training Loss: 0.0084
   Time since start: 0:01:26.585021
[batch 940] samples: 15040, Training Loss: 0.0074
   Time since start: 0:01:28.471861
[batch 960] samples: 15360, Training Loss: 0.0079
   Time since start: 0:01:30.347668
[batch 980] samples: 15680, Training Loss: 0.0106
   Time since start: 0:01:32.218853
[batch 1000] samples: 16000, Training Loss: 0.0138
   Time since start: 0:01:34.090416
[batch 1020] samples: 16320, Training Loss: 0.0072
   Time since start: 0:01:35.972648
[batch 1040] samples: 16640, Training Loss: 0.0095
   Time since start: 0:01:37.895095
[batch 1060] samples: 16960, Training Loss: 0.0048
   Time since start: 0:01:39.829323
[batch 1080] samples: 17280, Training Loss: 0.0044
   Time since start: 0:01:41.772809
[batch 1100] samples: 17600, Training Loss: 0.0052
   Time since start: 0:01:43.694118
[batch 1120] samples: 17920, Training Loss: 0.0042
   Time since start: 0:01:45.627715
[batch 1140] samples: 18240, Training Loss: 0.0078
   Time since start: 0:01:47.559328
[batch 1160] samples: 18560, Training Loss: 0.0167
   Time since start: 0:01:49.499973
[batch 1180] samples: 18880, Training Loss: 0.0111
   Time since start: 0:01:51.432382
[batch 1200] samples: 19200, Training Loss: 0.0098
   Time since start: 0:01:53.375376
[batch 1220] samples: 19520, Training Loss: 0.0059
   Time since start: 0:01:55.480977
[batch 1240] samples: 19840, Training Loss: 0.0072
   Time since start: 0:01:57.414649
[batch 1260] samples: 20160, Training Loss: 0.0037
   Time since start: 0:01:59.368583
[batch 1280] samples: 20480, Training Loss: 0.0053
   Time since start: 0:02:01.318998
[batch 1300] samples: 20800, Training Loss: 0.0072
   Time since start: 0:02:03.262723
[batch 1320] samples: 21120, Training Loss: 0.0083
   Time since start: 0:02:05.127892
[batch 1340] samples: 21440, Training Loss: 0.0049
   Time since start: 0:02:07.000525
[batch 1360] samples: 21760, Training Loss: 0.0060
   Time since start: 0:02:08.872646
[batch 1380] samples: 22080, Training Loss: 0.0031
   Time since start: 0:02:10.742781
[batch 1400] samples: 22400, Training Loss: 0.0035
   Time since start: 0:02:12.662363
[batch 1420] samples: 22720, Training Loss: 0.0098
   Time since start: 0:02:14.587031
[batch 1440] samples: 23040, Training Loss: 0.0067
   Time since start: 0:02:16.538709
[batch 1460] samples: 23360, Training Loss: 0.0135
   Time since start: 0:02:18.470839
[batch 1480] samples: 23680, Training Loss: 0.0029
   Time since start: 0:02:20.394907
[batch 1500] samples: 24000, Training Loss: 0.0027
   Time since start: 0:02:22.336942
[batch 1520] samples: 24320, Training Loss: 0.0049
   Time since start: 0:02:24.269475
[batch 1540] samples: 24640, Training Loss: 0.0091
   Time since start: 0:02:26.212164
[batch 1560] samples: 24960, Training Loss: 0.0018
   Time since start: 0:02:28.134408
[batch 1580] samples: 25280, Training Loss: 0.0016
   Time since start: 0:02:30.077270
[batch 1600] samples: 25600, Training Loss: 0.0028
   Time since start: 0:02:32.010308
[batch 1620] samples: 25920, Training Loss: 0.0022
   Time since start: 0:02:33.964263
[batch 1640] samples: 26240, Training Loss: 0.0029
   Time since start: 0:02:35.886926
[batch 1660] samples: 26560, Training Loss: 0.0020
   Time since start: 0:02:37.808615
[batch 1680] samples: 26880, Training Loss: 0.0023
   Time since start: 0:02:39.732365
[batch 1700] samples: 27200, Training Loss: 0.0016
   Time since start: 0:02:41.675293
[batch 1720] samples: 27520, Training Loss: 0.0033
   Time since start: 0:02:43.608021
[batch 1740] samples: 27840, Training Loss: 0.0019
   Time since start: 0:02:45.529434
[batch 1760] samples: 28160, Training Loss: 0.0047
   Time since start: 0:02:47.481109
[batch 1780] samples: 28480, Training Loss: 0.0016
   Time since start: 0:02:49.294029
[batch 1800] samples: 28800, Training Loss: 0.0042
   Time since start: 0:02:50.601292
[batch 1820] samples: 29120, Training Loss: 0.0025
   Time since start: 0:02:51.903574
[batch 1840] samples: 29440, Training Loss: 0.0016
   Time since start: 0:02:53.204487
[batch 1860] samples: 29760, Training Loss: 0.0013
   Time since start: 0:02:54.506545
[batch 1880] samples: 30080, Training Loss: 0.0024
   Time since start: 0:02:55.808883
[batch 1900] samples: 30400, Training Loss: 0.0058
   Time since start: 0:02:57.109823
[batch 1920] samples: 30720, Training Loss: 0.0046
   Time since start: 0:02:58.410242
[batch 1940] samples: 31040, Training Loss: 0.0023
   Time since start: 0:02:59.713308
[batch 1960] samples: 31360, Training Loss: 0.0020
   Time since start: 0:03:00.988019
--m-Epoch 1 done.
   Training Loss: 0.0332
   Validation Loss: 0.0012
              precision    recall  f1-score  support  epoch  class
0              1.000000  1.000000  1.000000   5916.0      1      0
1              1.000000  1.000000  1.000000    378.0      1      1
2              1.000000  1.000000  1.000000   1128.0      1      2
3              1.000000  1.000000  1.000000    420.0      1      3
4              1.000000  1.000000  1.000000    576.0      1      4
5              1.000000  1.000000  1.000000   5688.0      1      5
6              1.000000  1.000000  1.000000   5040.0      1      6
7              1.000000  1.000000  1.000000   2226.0      1      7
8              1.000000  1.000000  1.000000    420.0      1      8
9              1.000000  1.000000  1.000000    156.0      1      9
10             1.000000  1.000000  1.000000   2640.0      1     10
11             0.998243  0.996491  0.997366    570.0      1     11
12             0.996885  0.987654  0.992248    324.0      1     12
13             0.993289  1.000000  0.996633    444.0      1     13
14             0.997778  0.997778  0.997778    450.0      1     14
15             0.986014  1.000000  0.992958    282.0      1     15
16             1.000000  1.000000  1.000000    396.0      1     16
17             1.000000  0.991228  0.995595    456.0      1     17
18             1.000000  0.998881  0.999440    894.0      1     18
19             0.998131  1.000000  0.999065    534.0      1     19
20             1.000000  1.000000  1.000000    156.0      1     20
21             1.000000  1.000000  1.000000    156.0      1     21
22             1.000000  1.000000  1.000000     90.0      1     22
23             1.000000  1.000000  1.000000    108.0      1     23
24             0.993631  1.000000  0.996805    156.0      1     24
25             1.000000  1.000000  1.000000    300.0      1     25
26             1.000000  1.000000  1.000000    240.0      1     26
27             0.991736  1.000000  0.995851    120.0      1     27
28             1.000000  1.000000  1.000000     54.0      1     28
29             1.000000  1.000000  1.000000     54.0      1     29
30             1.000000  1.000000  1.000000     78.0      1     30
31             1.000000  1.000000  1.000000    228.0      1     31
32             1.000000  0.996212  0.998102    264.0      1     32
33             1.000000  1.000000  1.000000    222.0      1     33
34             1.000000  1.000000  1.000000     42.0      1     34
35             1.000000  1.000000  1.000000     72.0      1     35
36             1.000000  0.984848  0.992366     66.0      1     36
37             1.000000  1.000000  1.000000    216.0      1     37
38             0.992126  1.000000  0.996047    126.0      1     38
39             1.000000  1.000000  1.000000    360.0      1     39
40             1.000000  0.997585  0.998791    414.0      1     40
41             1.000000  1.000000  1.000000     60.0      1     41
42             1.000000  1.000000  1.000000     72.0      1     42
micro avg      0.999570  0.999540  0.999555  32592.0      1      0
macro avg      0.998787  0.998853  0.998815  32592.0      1      1
weighted avg   0.999573  0.999540  0.999555  32592.0      1      2
samples avg    0.999628  0.999594  0.999595  32592.0      1      3
