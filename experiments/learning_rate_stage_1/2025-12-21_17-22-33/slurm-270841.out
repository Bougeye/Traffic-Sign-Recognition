Requirement already satisfied: pip in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.3)
Requirement already satisfied: wheel in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (0.45.1)
Requirement already satisfied: setuptools in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (80.9.0)
Requirement already satisfied: packaging in /vol/fob-vol1/mi23/ziglowsa/venvs/payload_venv/lib/python3.10/site-packages (25.0)
All required imports already available in venv.
Import check passed (including torchvision).
=== ENV ===
HOSTNAME: gruenau1
PARTITION: gpu
CUDA_VISIBLE_DEVICES: 0
PWD: /vol/fob-vol1/mi23/ziglowsa/Payload
numpy: 2.2.6
torch: 2.4.1+cu121 cuda: 12.1 is_available: True
torchvision: 0.19.1+cu121
nvidia-smi -L: GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-502de52b-61ca-040e-e44d-5cee4e192a9d)
device: cuda
Epoch: 1 of 30
[batch 20] samples: 320, Training Loss: 0.2896
   Time since start: 0:00:03.252748
[batch 40] samples: 640, Training Loss: 0.2042
   Time since start: 0:00:05.173565
[batch 60] samples: 960, Training Loss: 0.2202
   Time since start: 0:00:07.105825
[batch 80] samples: 1280, Training Loss: 0.2093
   Time since start: 0:00:09.026837
[batch 100] samples: 1600, Training Loss: 0.1980
   Time since start: 0:00:10.967646
[batch 120] samples: 1920, Training Loss: 0.1592
   Time since start: 0:00:12.897448
[batch 140] samples: 2240, Training Loss: 0.1439
   Time since start: 0:00:14.730811
[batch 160] samples: 2560, Training Loss: 0.1604
   Time since start: 0:00:16.446527
[batch 180] samples: 2880, Training Loss: 0.1136
   Time since start: 0:00:17.847168
[batch 200] samples: 3200, Training Loss: 0.0774
   Time since start: 0:00:19.367109
[batch 220] samples: 3520, Training Loss: 0.0996
   Time since start: 0:00:21.077418
[batch 240] samples: 3840, Training Loss: 0.1406
   Time since start: 0:00:22.791226
[batch 260] samples: 4160, Training Loss: 0.1391
   Time since start: 0:00:24.510219
[batch 280] samples: 4480, Training Loss: 0.0866
   Time since start: 0:00:26.231605
[batch 300] samples: 4800, Training Loss: 0.0904
   Time since start: 0:00:27.946457
[batch 320] samples: 5120, Training Loss: 0.0955
   Time since start: 0:00:29.619707
[batch 340] samples: 5440, Training Loss: 0.0887
   Time since start: 0:00:31.255130
[batch 360] samples: 5760, Training Loss: 0.1058
   Time since start: 0:00:32.868152
[batch 380] samples: 6080, Training Loss: 0.0758
   Time since start: 0:00:34.609502
[batch 400] samples: 6400, Training Loss: 0.0771
   Time since start: 0:00:35.999253
[batch 420] samples: 6720, Training Loss: 0.1173
   Time since start: 0:00:37.378969
[batch 440] samples: 7040, Training Loss: 0.0886
   Time since start: 0:00:38.716084
[batch 460] samples: 7360, Training Loss: 0.0871
   Time since start: 0:00:40.092559
[batch 480] samples: 7680, Training Loss: 0.0959
   Time since start: 0:00:41.457179
[batch 500] samples: 8000, Training Loss: 0.0770
   Time since start: 0:00:42.931862
[batch 520] samples: 8320, Training Loss: 0.0896
   Time since start: 0:00:44.374069
[batch 540] samples: 8640, Training Loss: 0.0805
   Time since start: 0:00:45.954657
[batch 560] samples: 8960, Training Loss: 0.0753
   Time since start: 0:00:47.974699
[batch 580] samples: 9280, Training Loss: 0.0806
   Time since start: 0:00:49.993970
[batch 600] samples: 9600, Training Loss: 0.0490
   Time since start: 0:00:52.004598
[batch 620] samples: 9920, Training Loss: 0.0665
   Time since start: 0:00:54.024818
[batch 640] samples: 10240, Training Loss: 0.0699
   Time since start: 0:00:55.983991
[batch 660] samples: 10560, Training Loss: 0.0976
   Time since start: 0:00:58.008106
[batch 680] samples: 10880, Training Loss: 0.0762
   Time since start: 0:01:00.049665
[batch 700] samples: 11200, Training Loss: 0.0644
   Time since start: 0:01:02.056894
[batch 720] samples: 11520, Training Loss: 0.0597
   Time since start: 0:01:03.955761
[batch 740] samples: 11840, Training Loss: 0.0592
   Time since start: 0:01:05.481657
[batch 760] samples: 12160, Training Loss: 0.0587
   Time since start: 0:01:07.119685
[batch 780] samples: 12480, Training Loss: 0.0631
   Time since start: 0:01:08.760015
[batch 800] samples: 12800, Training Loss: 0.0844
   Time since start: 0:01:10.386490
[batch 820] samples: 13120, Training Loss: 0.0682
   Time since start: 0:01:12.004115
[batch 840] samples: 13440, Training Loss: 0.0784
   Time since start: 0:01:13.581424
[batch 860] samples: 13760, Training Loss: 0.0619
   Time since start: 0:01:15.086306
[batch 880] samples: 14080, Training Loss: 0.0455
   Time since start: 0:01:16.608926
[batch 900] samples: 14400, Training Loss: 0.0625
   Time since start: 0:01:18.207284
[batch 920] samples: 14720, Training Loss: 0.1007
   Time since start: 0:01:19.737547
[batch 940] samples: 15040, Training Loss: 0.0689
   Time since start: 0:01:21.428687
[batch 960] samples: 15360, Training Loss: 0.0664
   Time since start: 0:01:23.102166
[batch 980] samples: 15680, Training Loss: 0.0638
   Time since start: 0:01:25.124951
[batch 1000] samples: 16000, Training Loss: 0.0617
   Time since start: 0:01:27.137050
[batch 1020] samples: 16320, Training Loss: 0.0329
   Time since start: 0:01:28.957882
[batch 1040] samples: 16640, Training Loss: 0.0404
   Time since start: 0:01:30.791375
[batch 1060] samples: 16960, Training Loss: 0.0773
   Time since start: 0:01:32.525331
[batch 1080] samples: 17280, Training Loss: 0.0488
   Time since start: 0:01:34.250310
[batch 1100] samples: 17600, Training Loss: 0.0294
   Time since start: 0:01:36.000921
[batch 1120] samples: 17920, Training Loss: 0.0492
   Time since start: 0:01:37.728184
[batch 1140] samples: 18240, Training Loss: 0.0516
   Time since start: 0:01:39.507573
[batch 1160] samples: 18560, Training Loss: 0.0546
   Time since start: 0:01:41.245222
[batch 1180] samples: 18880, Training Loss: 0.0364
   Time since start: 0:01:42.965765
[batch 1200] samples: 19200, Training Loss: 0.0634
   Time since start: 0:01:44.650701
[batch 1220] samples: 19520, Training Loss: 0.0268
   Time since start: 0:01:46.276188
[batch 1240] samples: 19840, Training Loss: 0.0509
   Time since start: 0:01:47.651218
[batch 1260] samples: 20160, Training Loss: 0.0328
   Time since start: 0:01:48.994935
[batch 1280] samples: 20480, Training Loss: 0.0393
   Time since start: 0:01:50.567619
[batch 1300] samples: 20800, Training Loss: 0.0305
   Time since start: 0:01:52.238297
[batch 1320] samples: 21120, Training Loss: 0.0167
   Time since start: 0:01:53.868171
[batch 1340] samples: 21440, Training Loss: 0.0374
   Time since start: 0:01:55.507823
[batch 1360] samples: 21760, Training Loss: 0.0329
   Time since start: 0:01:57.114862
[batch 1380] samples: 22080, Training Loss: 0.0192
   Time since start: 0:01:58.753166
[batch 1400] samples: 22400, Training Loss: 0.0452
   Time since start: 0:02:00.636207
[batch 1420] samples: 22720, Training Loss: 0.0176
   Time since start: 0:02:02.366327
[batch 1440] samples: 23040, Training Loss: 0.0243
   Time since start: 0:02:03.983158
[batch 1460] samples: 23360, Training Loss: 0.0299
   Time since start: 0:02:06.004965
[batch 1480] samples: 23680, Training Loss: 0.0192
   Time since start: 0:02:08.016029
[batch 1500] samples: 24000, Training Loss: 0.0135
   Time since start: 0:02:10.028109
[batch 1520] samples: 24320, Training Loss: 0.0135
   Time since start: 0:02:12.047791
[batch 1540] samples: 24640, Training Loss: 0.0241
   Time since start: 0:02:13.988659
[batch 1560] samples: 24960, Training Loss: 0.0131
   Time since start: 0:02:15.950000
[batch 1580] samples: 25280, Training Loss: 0.0332
   Time since start: 0:02:17.972156
[batch 1600] samples: 25600, Training Loss: 0.0370
   Time since start: 0:02:19.964502
[batch 1620] samples: 25920, Training Loss: 0.0367
   Time since start: 0:02:21.786148
[batch 1640] samples: 26240, Training Loss: 0.0321
   Time since start: 0:02:23.360301
[batch 1660] samples: 26560, Training Loss: 0.0097
   Time since start: 0:02:25.266123
[batch 1680] samples: 26880, Training Loss: 0.0397
   Time since start: 0:02:27.194914
[batch 1700] samples: 27200, Training Loss: 0.0313
   Time since start: 0:02:29.119273
[batch 1720] samples: 27520, Training Loss: 0.0336
   Time since start: 0:02:31.028540
[batch 1740] samples: 27840, Training Loss: 0.0262
   Time since start: 0:02:32.944455
[batch 1760] samples: 28160, Training Loss: 0.0322
   Time since start: 0:02:34.599300
[batch 1780] samples: 28480, Training Loss: 0.0364
   Time since start: 0:02:36.248979
[batch 1800] samples: 28800, Training Loss: 0.0320
   Time since start: 0:02:38.056277
[batch 1820] samples: 29120, Training Loss: 0.0251
   Time since start: 0:02:39.865830
[batch 1840] samples: 29440, Training Loss: 0.0170
   Time since start: 0:02:41.612739
[batch 1860] samples: 29760, Training Loss: 0.0465
   Time since start: 0:02:43.390914
[batch 1880] samples: 30080, Training Loss: 0.0273
   Time since start: 0:02:45.130840
[batch 1900] samples: 30400, Training Loss: 0.0339
   Time since start: 0:02:46.488642
[batch 1920] samples: 30720, Training Loss: 0.0205
   Time since start: 0:02:47.844354
[batch 1940] samples: 31040, Training Loss: 0.0177
   Time since start: 0:02:49.208860
[batch 1960] samples: 31360, Training Loss: 0.0124
   Time since start: 0:02:50.553452
--m-Epoch 1 done.
   Training Loss: 0.0666
   Validation Loss: 0.0213
Epoch: 2 of 30
[batch 20] samples: 320, Training Loss: 0.0869
   Time since start: 0:03:04.201021
[batch 40] samples: 640, Training Loss: 0.0139
   Time since start: 0:03:06.083544
[batch 60] samples: 960, Training Loss: 0.0124
   Time since start: 0:03:07.815903
[batch 80] samples: 1280, Training Loss: 0.0057
   Time since start: 0:03:09.644344
[batch 100] samples: 1600, Training Loss: 0.0116
   Time since start: 0:03:11.572863
[batch 120] samples: 1920, Training Loss: 0.0121
   Time since start: 0:03:13.383090
[batch 140] samples: 2240, Training Loss: 0.0172
   Time since start: 0:03:15.195177
[batch 160] samples: 2560, Training Loss: 0.0126
   Time since start: 0:03:16.847480
[batch 180] samples: 2880, Training Loss: 0.0137
   Time since start: 0:03:18.601147
[batch 200] samples: 3200, Training Loss: 0.0150
   Time since start: 0:03:20.390773
[batch 220] samples: 3520, Training Loss: 0.0256
   Time since start: 0:03:22.134624
[batch 240] samples: 3840, Training Loss: 0.0066
   Time since start: 0:03:23.929219
[batch 260] samples: 4160, Training Loss: 0.0093
   Time since start: 0:03:25.786435
[batch 280] samples: 4480, Training Loss: 0.0298
   Time since start: 0:03:27.767488
[batch 300] samples: 4800, Training Loss: 0.0207
   Time since start: 0:03:29.486711
[batch 320] samples: 5120, Training Loss: 0.0105
   Time since start: 0:03:31.273722
[batch 340] samples: 5440, Training Loss: 0.0143
   Time since start: 0:03:33.182320
[batch 360] samples: 5760, Training Loss: 0.0195
   Time since start: 0:03:34.827426
[batch 380] samples: 6080, Training Loss: 0.0144
   Time since start: 0:03:36.706515
[batch 400] samples: 6400, Training Loss: 0.0380
   Time since start: 0:03:38.596966
[batch 420] samples: 6720, Training Loss: 0.0266
   Time since start: 0:03:40.397473
[batch 440] samples: 7040, Training Loss: 0.0124
   Time since start: 0:03:42.120387
[batch 460] samples: 7360, Training Loss: 0.0310
   Time since start: 0:03:43.926014
[batch 480] samples: 7680, Training Loss: 0.0167
   Time since start: 0:03:45.728701
[batch 500] samples: 8000, Training Loss: 0.0138
   Time since start: 0:03:47.612545
[batch 520] samples: 8320, Training Loss: 0.0150
   Time since start: 0:03:49.633568
[batch 540] samples: 8640, Training Loss: 0.0064
   Time since start: 0:03:51.602937
[batch 560] samples: 8960, Training Loss: 0.0156
   Time since start: 0:03:53.383096
[batch 580] samples: 9280, Training Loss: 0.0111
   Time since start: 0:03:55.312956
[batch 600] samples: 9600, Training Loss: 0.0238
   Time since start: 0:03:57.231679
[batch 620] samples: 9920, Training Loss: 0.0127
   Time since start: 0:03:58.714062
[batch 640] samples: 10240, Training Loss: 0.0201
   Time since start: 0:04:00.289183
[batch 660] samples: 10560, Training Loss: 0.0088
   Time since start: 0:04:02.119109
[batch 680] samples: 10880, Training Loss: 0.0049
   Time since start: 0:04:03.925913
[batch 700] samples: 11200, Training Loss: 0.0208
   Time since start: 0:04:05.499161
[batch 720] samples: 11520, Training Loss: 0.0204
   Time since start: 0:04:06.858531
[batch 740] samples: 11840, Training Loss: 0.0063
   Time since start: 0:04:08.573991
[batch 760] samples: 12160, Training Loss: 0.0125
   Time since start: 0:04:09.988761
[batch 780] samples: 12480, Training Loss: 0.0074
   Time since start: 0:04:11.391376
[batch 800] samples: 12800, Training Loss: 0.0198
   Time since start: 0:04:13.112494
[batch 820] samples: 13120, Training Loss: 0.0020
   Time since start: 0:04:14.845767
[batch 840] samples: 13440, Training Loss: 0.0019
   Time since start: 0:04:16.552071
[batch 860] samples: 13760, Training Loss: 0.0104
   Time since start: 0:04:18.421865
[batch 880] samples: 14080, Training Loss: 0.0025
   Time since start: 0:04:19.950710
[batch 900] samples: 14400, Training Loss: 0.0097
   Time since start: 0:04:21.266978
[batch 920] samples: 14720, Training Loss: 0.0349
   Time since start: 0:04:22.610537
[batch 940] samples: 15040, Training Loss: 0.0144
   Time since start: 0:04:23.970446
[batch 960] samples: 15360, Training Loss: 0.0093
   Time since start: 0:04:25.334249
[batch 980] samples: 15680, Training Loss: 0.0077
   Time since start: 0:04:26.892316
[batch 1000] samples: 16000, Training Loss: 0.0099
   Time since start: 0:04:28.581138
[batch 1020] samples: 16320, Training Loss: 0.0015
   Time since start: 0:04:30.007142
[batch 1040] samples: 16640, Training Loss: 0.0318
   Time since start: 0:04:31.606317
[batch 1060] samples: 16960, Training Loss: 0.0314
   Time since start: 0:04:33.373544
[batch 1080] samples: 17280, Training Loss: 0.0160
   Time since start: 0:04:35.069731
[batch 1100] samples: 17600, Training Loss: 0.0205
   Time since start: 0:04:36.736022
[batch 1120] samples: 17920, Training Loss: 0.0242
   Time since start: 0:04:38.365287
[batch 1140] samples: 18240, Training Loss: 0.0079
   Time since start: 0:04:40.122370
[batch 1160] samples: 18560, Training Loss: 0.0090
   Time since start: 0:04:41.744931
[batch 1180] samples: 18880, Training Loss: 0.0209
   Time since start: 0:04:43.437768
[batch 1200] samples: 19200, Training Loss: 0.0062
   Time since start: 0:04:45.363937
[batch 1220] samples: 19520, Training Loss: 0.0146
   Time since start: 0:04:47.327450
[batch 1240] samples: 19840, Training Loss: 0.0052
   Time since start: 0:04:49.351368
[batch 1260] samples: 20160, Training Loss: 0.1551
   Time since start: 0:04:51.049295
[batch 1280] samples: 20480, Training Loss: 0.0113
   Time since start: 0:04:52.789613
[batch 1300] samples: 20800, Training Loss: 0.0082
   Time since start: 0:04:54.255828
[batch 1320] samples: 21120, Training Loss: 0.0240
   Time since start: 0:04:55.687726
[batch 1340] samples: 21440, Training Loss: 0.0130
   Time since start: 0:04:57.088701
[batch 1360] samples: 21760, Training Loss: 0.0203
   Time since start: 0:04:58.694563
[batch 1380] samples: 22080, Training Loss: 0.1060
   Time since start: 0:05:00.337360
[batch 1400] samples: 22400, Training Loss: 0.0470
   Time since start: 0:05:01.965428
[batch 1420] samples: 22720, Training Loss: 0.0067
   Time since start: 0:05:03.625253
[batch 1440] samples: 23040, Training Loss: 0.0021
   Time since start: 0:05:05.253658
[batch 1460] samples: 23360, Training Loss: 0.0057
   Time since start: 0:05:06.982269
[batch 1480] samples: 23680, Training Loss: 0.0184
   Time since start: 0:05:08.678442
[batch 1500] samples: 24000, Training Loss: 0.0026
   Time since start: 0:05:10.083918
[batch 1520] samples: 24320, Training Loss: 0.0035
   Time since start: 0:05:11.818163
[batch 1540] samples: 24640, Training Loss: 0.0032
   Time since start: 0:05:13.671385
[batch 1560] samples: 24960, Training Loss: 0.0043
   Time since start: 0:05:15.542613
[batch 1580] samples: 25280, Training Loss: 0.0160
   Time since start: 0:05:17.416155
[batch 1600] samples: 25600, Training Loss: 0.0077
   Time since start: 0:05:19.287885
[batch 1620] samples: 25920, Training Loss: 0.0084
   Time since start: 0:05:21.138269
[batch 1640] samples: 26240, Training Loss: 0.0048
   Time since start: 0:05:22.938062
[batch 1660] samples: 26560, Training Loss: 0.0254
   Time since start: 0:05:24.738364
[batch 1680] samples: 26880, Training Loss: 0.0018
   Time since start: 0:05:26.543492
[batch 1700] samples: 27200, Training Loss: 0.0124
   Time since start: 0:05:27.896672
[batch 1720] samples: 27520, Training Loss: 0.0036
   Time since start: 0:05:29.256899
[batch 1740] samples: 27840, Training Loss: 0.0124
   Time since start: 0:05:30.573448
[batch 1760] samples: 28160, Training Loss: 0.0055
   Time since start: 0:05:32.252009
[batch 1780] samples: 28480, Training Loss: 0.0095
   Time since start: 0:05:34.098763
[batch 1800] samples: 28800, Training Loss: 0.0090
   Time since start: 0:05:35.932032
[batch 1820] samples: 29120, Training Loss: 0.0199
   Time since start: 0:05:37.449142
[batch 1840] samples: 29440, Training Loss: 0.0023
   Time since start: 0:05:38.812216
[batch 1860] samples: 29760, Training Loss: 0.0025
   Time since start: 0:05:40.307204
[batch 1880] samples: 30080, Training Loss: 0.0107
   Time since start: 0:05:42.039161
[batch 1900] samples: 30400, Training Loss: 0.0055
   Time since start: 0:05:43.504449
[batch 1920] samples: 30720, Training Loss: 0.0125
   Time since start: 0:05:44.905299
[batch 1940] samples: 31040, Training Loss: 0.0016
   Time since start: 0:05:46.298095
[batch 1960] samples: 31360, Training Loss: 0.0170
   Time since start: 0:05:48.097143
--m-Epoch 2 done.
   Training Loss: 0.0147
   Validation Loss: 0.0105
Epoch: 3 of 30
[batch 20] samples: 320, Training Loss: 0.0022
   Time since start: 0:06:02.563985
[batch 40] samples: 640, Training Loss: 0.0085
   Time since start: 0:06:04.351009
[batch 60] samples: 960, Training Loss: 0.0198
   Time since start: 0:06:06.107756
[batch 80] samples: 1280, Training Loss: 0.0050
   Time since start: 0:06:07.841527
[batch 100] samples: 1600, Training Loss: 0.0125
   Time since start: 0:06:09.581214
[batch 120] samples: 1920, Training Loss: 0.0217
   Time since start: 0:06:11.297446
[batch 140] samples: 2240, Training Loss: 0.0107
   Time since start: 0:06:12.979052
[batch 160] samples: 2560, Training Loss: 0.0048
   Time since start: 0:06:14.697369
[batch 180] samples: 2880, Training Loss: 0.0252
   Time since start: 0:06:16.432263
[batch 200] samples: 3200, Training Loss: 0.0027
   Time since start: 0:06:18.164263
[batch 220] samples: 3520, Training Loss: 0.0259
   Time since start: 0:06:19.895153
[batch 240] samples: 3840, Training Loss: 0.0166
   Time since start: 0:06:21.609571
[batch 260] samples: 4160, Training Loss: 0.0189
   Time since start: 0:06:23.057499
[batch 280] samples: 4480, Training Loss: 0.0089
   Time since start: 0:06:24.503795
[batch 300] samples: 4800, Training Loss: 0.0075
   Time since start: 0:06:25.942037
[batch 320] samples: 5120, Training Loss: 0.0008
   Time since start: 0:06:27.386729
[batch 340] samples: 5440, Training Loss: 0.0388
   Time since start: 0:06:28.843995
[batch 360] samples: 5760, Training Loss: 0.0158
   Time since start: 0:06:30.306439
[batch 380] samples: 6080, Training Loss: 0.0009
   Time since start: 0:06:31.765315
[batch 400] samples: 6400, Training Loss: 0.0054
   Time since start: 0:06:33.334651
[batch 420] samples: 6720, Training Loss: 0.0050
   Time since start: 0:06:34.840376
[batch 440] samples: 7040, Training Loss: 0.0013
   Time since start: 0:06:36.476825
[batch 460] samples: 7360, Training Loss: 0.0027
   Time since start: 0:06:38.102975
[batch 480] samples: 7680, Training Loss: 0.0095
   Time since start: 0:06:39.766650
[batch 500] samples: 8000, Training Loss: 0.0078
   Time since start: 0:06:41.406938
[batch 520] samples: 8320, Training Loss: 0.0068
   Time since start: 0:06:42.971288
[batch 540] samples: 8640, Training Loss: 0.0017
   Time since start: 0:06:44.292904
[batch 560] samples: 8960, Training Loss: 0.0194
   Time since start: 0:06:45.607255
[batch 580] samples: 9280, Training Loss: 0.0271
   Time since start: 0:06:46.937648
[batch 600] samples: 9600, Training Loss: 0.0379
   Time since start: 0:06:48.251370
[batch 620] samples: 9920, Training Loss: 0.0062
   Time since start: 0:06:49.518351
[batch 640] samples: 10240, Training Loss: 0.0118
   Time since start: 0:06:50.789230
[batch 660] samples: 10560, Training Loss: 0.0029
   Time since start: 0:06:52.056382
[batch 680] samples: 10880, Training Loss: 0.0008
   Time since start: 0:06:53.329975
[batch 700] samples: 11200, Training Loss: 0.0035
   Time since start: 0:06:54.595754
[batch 720] samples: 11520, Training Loss: 0.0040
   Time since start: 0:06:55.866720
[batch 740] samples: 11840, Training Loss: 0.0073
   Time since start: 0:06:57.135710
[batch 760] samples: 12160, Training Loss: 0.0044
   Time since start: 0:06:58.404461
[batch 780] samples: 12480, Training Loss: 0.0080
   Time since start: 0:06:59.681509
[batch 800] samples: 12800, Training Loss: 0.0050
   Time since start: 0:07:01.408924
[batch 820] samples: 13120, Training Loss: 0.0058
   Time since start: 0:07:03.132366
[batch 840] samples: 13440, Training Loss: 0.0484
   Time since start: 0:07:04.824540
[batch 860] samples: 13760, Training Loss: 0.0113
   Time since start: 0:07:06.574465
[batch 880] samples: 14080, Training Loss: 0.0007
   Time since start: 0:07:08.373603
[batch 900] samples: 14400, Training Loss: 0.0087
   Time since start: 0:07:10.171220
[batch 920] samples: 14720, Training Loss: 0.0026
   Time since start: 0:07:11.739911
[batch 940] samples: 15040, Training Loss: 0.0045
   Time since start: 0:07:13.371860
[batch 960] samples: 15360, Training Loss: 0.0058
   Time since start: 0:07:14.734705
[batch 980] samples: 15680, Training Loss: 0.0018
   Time since start: 0:07:16.327648
[batch 1000] samples: 16000, Training Loss: 0.0034
   Time since start: 0:07:18.199428
[batch 1020] samples: 16320, Training Loss: 0.0068
   Time since start: 0:07:20.002565
[batch 1040] samples: 16640, Training Loss: 0.0082
   Time since start: 0:07:21.791142
[batch 1060] samples: 16960, Training Loss: 0.0095
   Time since start: 0:07:23.697671
[batch 1080] samples: 17280, Training Loss: 0.0180
   Time since start: 0:07:25.589032
[batch 1100] samples: 17600, Training Loss: 0.0010
   Time since start: 0:07:27.479706
[batch 1120] samples: 17920, Training Loss: 0.0011
   Time since start: 0:07:29.381945
[batch 1140] samples: 18240, Training Loss: 0.0064
   Time since start: 0:07:31.301912
[batch 1160] samples: 18560, Training Loss: 0.0016
   Time since start: 0:07:33.192095
[batch 1180] samples: 18880, Training Loss: 0.0181
   Time since start: 0:07:35.089198
[batch 1200] samples: 19200, Training Loss: 0.0010
   Time since start: 0:07:36.953516
[batch 1220] samples: 19520, Training Loss: 0.0004
   Time since start: 0:07:38.815649
[batch 1240] samples: 19840, Training Loss: 0.0036
   Time since start: 0:07:40.687182
[batch 1260] samples: 20160, Training Loss: 0.0089
   Time since start: 0:07:42.412023
[batch 1280] samples: 20480, Training Loss: 0.0047
   Time since start: 0:07:43.718115
[batch 1300] samples: 20800, Training Loss: 0.0008
   Time since start: 0:07:45.023491
[batch 1320] samples: 21120, Training Loss: 0.0004
   Time since start: 0:07:46.334585
[batch 1340] samples: 21440, Training Loss: 0.0002
   Time since start: 0:07:47.641421
[batch 1360] samples: 21760, Training Loss: 0.0230
   Time since start: 0:07:48.945945
[batch 1380] samples: 22080, Training Loss: 0.0060
   Time since start: 0:07:50.252374
[batch 1400] samples: 22400, Training Loss: 0.0285
   Time since start: 0:07:51.556666
[batch 1420] samples: 22720, Training Loss: 0.0016
   Time since start: 0:07:52.864450
[batch 1440] samples: 23040, Training Loss: 0.0018
   Time since start: 0:07:54.172399
[batch 1460] samples: 23360, Training Loss: 0.0073
   Time since start: 0:07:55.477776
[batch 1480] samples: 23680, Training Loss: 0.0015
   Time since start: 0:07:56.783767
[batch 1500] samples: 24000, Training Loss: 0.0143
   Time since start: 0:07:58.089509
[batch 1520] samples: 24320, Training Loss: 0.0041
   Time since start: 0:07:59.395865
[batch 1540] samples: 24640, Training Loss: 0.0184
   Time since start: 0:08:00.701179
[batch 1560] samples: 24960, Training Loss: 0.0103
   Time since start: 0:08:02.009672
[batch 1580] samples: 25280, Training Loss: 0.0010
   Time since start: 0:08:03.314692
[batch 1600] samples: 25600, Training Loss: 0.0089
   Time since start: 0:08:04.619056
[batch 1620] samples: 25920, Training Loss: 0.0068
   Time since start: 0:08:05.924682
[batch 1640] samples: 26240, Training Loss: 0.0227
   Time since start: 0:08:07.352247
[batch 1660] samples: 26560, Training Loss: 0.0579
   Time since start: 0:08:08.664966
[batch 1680] samples: 26880, Training Loss: 0.0356
   Time since start: 0:08:09.974753
[batch 1700] samples: 27200, Training Loss: 0.0084
   Time since start: 0:08:11.284711
[batch 1720] samples: 27520, Training Loss: 0.0147
   Time since start: 0:08:12.596358
[batch 1740] samples: 27840, Training Loss: 0.0080
   Time since start: 0:08:13.907324
[batch 1760] samples: 28160, Training Loss: 0.0237
   Time since start: 0:08:15.220058
[batch 1780] samples: 28480, Training Loss: 0.0020
   Time since start: 0:08:16.533987
[batch 1800] samples: 28800, Training Loss: 0.0010
   Time since start: 0:08:17.844994
[batch 1820] samples: 29120, Training Loss: 0.0201
   Time since start: 0:08:19.153814
[batch 1840] samples: 29440, Training Loss: 0.0025
   Time since start: 0:08:20.471755
[batch 1860] samples: 29760, Training Loss: 0.0058
   Time since start: 0:08:22.282124
[batch 1880] samples: 30080, Training Loss: 0.0045
   Time since start: 0:08:24.104467
[batch 1900] samples: 30400, Training Loss: 0.0003
   Time since start: 0:08:25.923224
[batch 1920] samples: 30720, Training Loss: 0.0004
   Time since start: 0:08:27.711915
[batch 1940] samples: 31040, Training Loss: 0.0034
   Time since start: 0:08:29.481238
[batch 1960] samples: 31360, Training Loss: 0.0011
   Time since start: 0:08:31.270595
--m-Epoch 3 done.
   Training Loss: 0.0079
   Validation Loss: 0.0045
Epoch: 4 of 30
[batch 20] samples: 320, Training Loss: 0.0026
   Time since start: 0:08:44.639742
[batch 40] samples: 640, Training Loss: 0.0013
   Time since start: 0:08:46.510333
[batch 60] samples: 960, Training Loss: 0.0076
   Time since start: 0:08:48.377650
[batch 80] samples: 1280, Training Loss: 0.0005
   Time since start: 0:08:50.248162
[batch 100] samples: 1600, Training Loss: 0.0034
   Time since start: 0:08:52.108127
[batch 120] samples: 1920, Training Loss: 0.0021
   Time since start: 0:08:53.977771
[batch 140] samples: 2240, Training Loss: 0.0027
   Time since start: 0:08:55.845159
[batch 160] samples: 2560, Training Loss: 0.0073
   Time since start: 0:08:57.710737
[batch 180] samples: 2880, Training Loss: 0.0138
   Time since start: 0:08:59.578302
[batch 200] samples: 3200, Training Loss: 0.0006
   Time since start: 0:09:01.448743
[batch 220] samples: 3520, Training Loss: 0.0006
   Time since start: 0:09:03.317551
[batch 240] samples: 3840, Training Loss: 0.0013
   Time since start: 0:09:05.198316
[batch 260] samples: 4160, Training Loss: 0.0003
   Time since start: 0:09:07.067465
[batch 280] samples: 4480, Training Loss: 0.0008
   Time since start: 0:09:08.960510
[batch 300] samples: 4800, Training Loss: 0.0044
   Time since start: 0:09:10.854577
[batch 320] samples: 5120, Training Loss: 0.0014
   Time since start: 0:09:12.764988
[batch 340] samples: 5440, Training Loss: 0.0027
   Time since start: 0:09:14.707565
[batch 360] samples: 5760, Training Loss: 0.0018
   Time since start: 0:09:16.627546
[batch 380] samples: 6080, Training Loss: 0.0091
   Time since start: 0:09:18.556942
[batch 400] samples: 6400, Training Loss: 0.0148
   Time since start: 0:09:20.495833
[batch 420] samples: 6720, Training Loss: 0.0036
   Time since start: 0:09:22.405965
[batch 440] samples: 7040, Training Loss: 0.0002
   Time since start: 0:09:24.326050
[batch 460] samples: 7360, Training Loss: 0.0044
   Time since start: 0:09:26.257550
[batch 480] samples: 7680, Training Loss: 0.0024
   Time since start: 0:09:28.189599
[batch 500] samples: 8000, Training Loss: 0.0007
   Time since start: 0:09:30.130338
[batch 520] samples: 8320, Training Loss: 0.0022
   Time since start: 0:09:32.071781
[batch 540] samples: 8640, Training Loss: 0.0025
   Time since start: 0:09:34.041094
[batch 560] samples: 8960, Training Loss: 0.0006
   Time since start: 0:09:35.998932
[batch 580] samples: 9280, Training Loss: 0.0006
   Time since start: 0:09:37.941549
[batch 600] samples: 9600, Training Loss: 0.0011
   Time since start: 0:09:39.891850
[batch 620] samples: 9920, Training Loss: 0.0122
   Time since start: 0:09:41.842285
[batch 640] samples: 10240, Training Loss: 0.0004
   Time since start: 0:09:43.476624
[batch 660] samples: 10560, Training Loss: 0.0057
   Time since start: 0:09:44.859382
[batch 680] samples: 10880, Training Loss: 0.0051
   Time since start: 0:09:46.243751
[batch 700] samples: 11200, Training Loss: 0.0028
   Time since start: 0:09:47.628206
[batch 720] samples: 11520, Training Loss: 0.0017
   Time since start: 0:09:49.012935
[batch 740] samples: 11840, Training Loss: 0.0008
   Time since start: 0:09:50.558573
[batch 760] samples: 12160, Training Loss: 0.0129
   Time since start: 0:09:52.506325
[batch 780] samples: 12480, Training Loss: 0.0117
   Time since start: 0:09:54.401888
[batch 800] samples: 12800, Training Loss: 0.0144
   Time since start: 0:09:56.207612
[batch 820] samples: 13120, Training Loss: 0.0020
   Time since start: 0:09:58.198711
[batch 840] samples: 13440, Training Loss: 0.0044
   Time since start: 0:10:00.160970
[batch 860] samples: 13760, Training Loss: 0.0094
   Time since start: 0:10:01.920882
[batch 880] samples: 14080, Training Loss: 0.0002
   Time since start: 0:10:03.434323
[batch 900] samples: 14400, Training Loss: 0.0027
   Time since start: 0:10:05.341610
[batch 920] samples: 14720, Training Loss: 0.0034
   Time since start: 0:10:07.130764
[batch 940] samples: 15040, Training Loss: 0.0023
   Time since start: 0:10:08.882855
[batch 960] samples: 15360, Training Loss: 0.0247
   Time since start: 0:10:10.200460
[batch 980] samples: 15680, Training Loss: 0.0020
   Time since start: 0:10:11.516712
[batch 1000] samples: 16000, Training Loss: 0.0002
   Time since start: 0:10:12.835579
[batch 1020] samples: 16320, Training Loss: 0.0122
   Time since start: 0:10:14.153320
[batch 1040] samples: 16640, Training Loss: 0.0024
   Time since start: 0:10:15.470891
[batch 1060] samples: 16960, Training Loss: 0.0008
   Time since start: 0:10:16.786975
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:10:18.204996
[batch 1100] samples: 17600, Training Loss: 0.0126
   Time since start: 0:10:19.755323
[batch 1120] samples: 17920, Training Loss: 0.0058
   Time since start: 0:10:21.298162
[batch 1140] samples: 18240, Training Loss: 0.0049
   Time since start: 0:10:23.049657
[batch 1160] samples: 18560, Training Loss: 0.0080
   Time since start: 0:10:24.885664
[batch 1180] samples: 18880, Training Loss: 0.0064
   Time since start: 0:10:26.512009
[batch 1200] samples: 19200, Training Loss: 0.0002
   Time since start: 0:10:27.823586
[batch 1220] samples: 19520, Training Loss: 0.0030
   Time since start: 0:10:29.629365
[batch 1240] samples: 19840, Training Loss: 0.0041
   Time since start: 0:10:31.449663
[batch 1260] samples: 20160, Training Loss: 0.0127
   Time since start: 0:10:33.273508
[batch 1280] samples: 20480, Training Loss: 0.0097
   Time since start: 0:10:34.796930
[batch 1300] samples: 20800, Training Loss: 0.0015
   Time since start: 0:10:36.179128
[batch 1320] samples: 21120, Training Loss: 0.0027
   Time since start: 0:10:37.561196
[batch 1340] samples: 21440, Training Loss: 0.0004
   Time since start: 0:10:38.944509
[batch 1360] samples: 21760, Training Loss: 0.0252
   Time since start: 0:10:40.323766
[batch 1380] samples: 22080, Training Loss: 0.0033
   Time since start: 0:10:41.701031
[batch 1400] samples: 22400, Training Loss: 0.0088
   Time since start: 0:10:43.081293
[batch 1420] samples: 22720, Training Loss: 0.0160
   Time since start: 0:10:44.458417
[batch 1440] samples: 23040, Training Loss: 0.0076
   Time since start: 0:10:45.841823
[batch 1460] samples: 23360, Training Loss: 0.0043
   Time since start: 0:10:47.588907
[batch 1480] samples: 23680, Training Loss: 0.0250
   Time since start: 0:10:49.530193
[batch 1500] samples: 24000, Training Loss: 0.0035
   Time since start: 0:10:51.462058
[batch 1520] samples: 24320, Training Loss: 0.0092
   Time since start: 0:10:52.874098
[batch 1540] samples: 24640, Training Loss: 0.0057
   Time since start: 0:10:54.253567
[batch 1560] samples: 24960, Training Loss: 0.0061
   Time since start: 0:10:55.630658
[batch 1580] samples: 25280, Training Loss: 0.0127
   Time since start: 0:10:57.007057
[batch 1600] samples: 25600, Training Loss: 0.0006
   Time since start: 0:10:58.384869
[batch 1620] samples: 25920, Training Loss: 0.0060
   Time since start: 0:10:59.770072
[batch 1640] samples: 26240, Training Loss: 0.0022
   Time since start: 0:11:01.161820
[batch 1660] samples: 26560, Training Loss: 0.0002
   Time since start: 0:11:02.549668
[batch 1680] samples: 26880, Training Loss: 0.0149
   Time since start: 0:11:03.907221
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:11:05.247080
[batch 1720] samples: 27520, Training Loss: 0.0411
   Time since start: 0:11:06.588645
[batch 1740] samples: 27840, Training Loss: 0.0056
   Time since start: 0:11:07.930601
[batch 1760] samples: 28160, Training Loss: 0.0125
   Time since start: 0:11:09.277310
[batch 1780] samples: 28480, Training Loss: 0.0009
   Time since start: 0:11:10.616984
[batch 1800] samples: 28800, Training Loss: 0.0326
   Time since start: 0:11:11.956884
[batch 1820] samples: 29120, Training Loss: 0.0194
   Time since start: 0:11:13.297728
[batch 1840] samples: 29440, Training Loss: 0.0068
   Time since start: 0:11:14.637520
[batch 1860] samples: 29760, Training Loss: 0.0150
   Time since start: 0:11:16.344770
[batch 1880] samples: 30080, Training Loss: 0.0024
   Time since start: 0:11:18.210802
[batch 1900] samples: 30400, Training Loss: 0.0021
   Time since start: 0:11:20.074623
[batch 1920] samples: 30720, Training Loss: 0.0021
   Time since start: 0:11:21.902296
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:11:23.587545
[batch 1960] samples: 31360, Training Loss: 0.0019
   Time since start: 0:11:24.858048
--m-Epoch 4 done.
   Training Loss: 0.0064
   Validation Loss: 0.0033
Epoch: 5 of 30
[batch 20] samples: 320, Training Loss: 0.0044
   Time since start: 0:11:36.983636
[batch 40] samples: 640, Training Loss: 0.0002
   Time since start: 0:11:38.862674
[batch 60] samples: 960, Training Loss: 0.0008
   Time since start: 0:11:40.395127
[batch 80] samples: 1280, Training Loss: 0.0020
   Time since start: 0:11:41.733052
[batch 100] samples: 1600, Training Loss: 0.0135
   Time since start: 0:11:43.072841
[batch 120] samples: 1920, Training Loss: 0.0003
   Time since start: 0:11:44.410884
[batch 140] samples: 2240, Training Loss: 0.0003
   Time since start: 0:11:45.777038
[batch 160] samples: 2560, Training Loss: 0.0038
   Time since start: 0:11:47.272038
[batch 180] samples: 2880, Training Loss: 0.0170
   Time since start: 0:11:48.896388
[batch 200] samples: 3200, Training Loss: 0.0003
   Time since start: 0:11:50.652397
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:11:52.385501
[batch 240] samples: 3840, Training Loss: 0.0029
   Time since start: 0:11:54.130858
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:11:55.860016
[batch 280] samples: 4480, Training Loss: 0.0002
   Time since start: 0:11:57.573907
[batch 300] samples: 4800, Training Loss: 0.0002
   Time since start: 0:11:58.936699
[batch 320] samples: 5120, Training Loss: 0.0003
   Time since start: 0:12:00.208350
[batch 340] samples: 5440, Training Loss: 0.0004
   Time since start: 0:12:01.480067
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:12:02.761966
[batch 380] samples: 6080, Training Loss: 0.0021
   Time since start: 0:12:04.538194
[batch 400] samples: 6400, Training Loss: 0.0013
   Time since start: 0:12:06.353410
[batch 420] samples: 6720, Training Loss: 0.0008
   Time since start: 0:12:07.808727
[batch 440] samples: 7040, Training Loss: 0.0003
   Time since start: 0:12:09.368168
[batch 460] samples: 7360, Training Loss: 0.0038
   Time since start: 0:12:11.209074
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:12:13.026427
[batch 500] samples: 8000, Training Loss: 0.0013
   Time since start: 0:12:14.672844
[batch 520] samples: 8320, Training Loss: 0.0002
   Time since start: 0:12:15.986963
[batch 540] samples: 8640, Training Loss: 0.0052
   Time since start: 0:12:17.333054
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:12:18.648627
[batch 580] samples: 9280, Training Loss: 0.0003
   Time since start: 0:12:19.959677
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:12:21.270083
[batch 620] samples: 9920, Training Loss: 0.0046
   Time since start: 0:12:22.581346
[batch 640] samples: 10240, Training Loss: 0.0142
   Time since start: 0:12:23.902785
[batch 660] samples: 10560, Training Loss: 0.0051
   Time since start: 0:12:25.808602
[batch 680] samples: 10880, Training Loss: 0.0033
   Time since start: 0:12:27.727898
[batch 700] samples: 11200, Training Loss: 0.0104
   Time since start: 0:12:29.666890
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:12:31.604322
[batch 740] samples: 11840, Training Loss: 0.0022
   Time since start: 0:12:33.414027
[batch 760] samples: 12160, Training Loss: 0.0009
   Time since start: 0:12:34.817278
[batch 780] samples: 12480, Training Loss: 0.0072
   Time since start: 0:12:36.341888
[batch 800] samples: 12800, Training Loss: 0.0068
   Time since start: 0:12:38.231946
[batch 820] samples: 13120, Training Loss: 0.0006
   Time since start: 0:12:40.120874
[batch 840] samples: 13440, Training Loss: 0.0022
   Time since start: 0:12:42.022109
[batch 860] samples: 13760, Training Loss: 0.0215
   Time since start: 0:12:43.923244
[batch 880] samples: 14080, Training Loss: 0.0006
   Time since start: 0:12:45.491524
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:12:47.138300
[batch 920] samples: 14720, Training Loss: 0.0062
   Time since start: 0:12:49.010380
[batch 940] samples: 15040, Training Loss: 0.0072
   Time since start: 0:12:50.870144
[batch 960] samples: 15360, Training Loss: 0.0225
   Time since start: 0:12:52.739748
[batch 980] samples: 15680, Training Loss: 0.0059
   Time since start: 0:12:54.611121
[batch 1000] samples: 16000, Training Loss: 0.0060
   Time since start: 0:12:56.471511
[batch 1020] samples: 16320, Training Loss: 0.0010
   Time since start: 0:12:58.341776
[batch 1040] samples: 16640, Training Loss: 0.0166
   Time since start: 0:13:00.222292
[batch 1060] samples: 16960, Training Loss: 0.0064
   Time since start: 0:13:02.044707
[batch 1080] samples: 17280, Training Loss: 0.0002
   Time since start: 0:13:03.834452
[batch 1100] samples: 17600, Training Loss: 0.0041
   Time since start: 0:13:05.637977
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:13:07.330633
[batch 1140] samples: 18240, Training Loss: 0.0028
   Time since start: 0:13:08.639612
[batch 1160] samples: 18560, Training Loss: 0.0002
   Time since start: 0:13:10.268364
[batch 1180] samples: 18880, Training Loss: 0.0014
   Time since start: 0:13:11.726477
[batch 1200] samples: 19200, Training Loss: 0.0083
   Time since start: 0:13:12.989925
[batch 1220] samples: 19520, Training Loss: 0.0021
   Time since start: 0:13:14.246937
[batch 1240] samples: 19840, Training Loss: 0.0027
   Time since start: 0:13:15.503906
[batch 1260] samples: 20160, Training Loss: 0.0012
   Time since start: 0:13:16.761460
[batch 1280] samples: 20480, Training Loss: 0.0034
   Time since start: 0:13:18.018892
[batch 1300] samples: 20800, Training Loss: 0.0012
   Time since start: 0:13:19.276136
[batch 1320] samples: 21120, Training Loss: 0.0007
   Time since start: 0:13:20.540704
[batch 1340] samples: 21440, Training Loss: 0.0011
   Time since start: 0:13:21.799488
[batch 1360] samples: 21760, Training Loss: 0.0149
   Time since start: 0:13:23.076396
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:13:24.450425
[batch 1400] samples: 22400, Training Loss: 0.0272
   Time since start: 0:13:25.718590
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:13:26.982299
[batch 1440] samples: 23040, Training Loss: 0.0004
   Time since start: 0:13:28.246794
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:13:29.510405
[batch 1480] samples: 23680, Training Loss: 0.0004
   Time since start: 0:13:30.774122
[batch 1500] samples: 24000, Training Loss: 0.0023
   Time since start: 0:13:32.042861
[batch 1520] samples: 24320, Training Loss: 0.0143
   Time since start: 0:13:33.319584
[batch 1540] samples: 24640, Training Loss: 0.0046
   Time since start: 0:13:34.594463
[batch 1560] samples: 24960, Training Loss: 0.0012
   Time since start: 0:13:35.862134
[batch 1580] samples: 25280, Training Loss: 0.0005
   Time since start: 0:13:37.129699
[batch 1600] samples: 25600, Training Loss: 0.0010
   Time since start: 0:13:38.395498
[batch 1620] samples: 25920, Training Loss: 0.0009
   Time since start: 0:13:39.659315
[batch 1640] samples: 26240, Training Loss: 0.0072
   Time since start: 0:13:40.923515
[batch 1660] samples: 26560, Training Loss: 0.0003
   Time since start: 0:13:42.187522
[batch 1680] samples: 26880, Training Loss: 0.0022
   Time since start: 0:13:43.460736
[batch 1700] samples: 27200, Training Loss: 0.0002
   Time since start: 0:13:44.728999
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:13:45.989843
[batch 1740] samples: 27840, Training Loss: 0.0015
   Time since start: 0:13:47.251360
[batch 1760] samples: 28160, Training Loss: 0.0004
   Time since start: 0:13:48.510729
[batch 1780] samples: 28480, Training Loss: 0.0009
   Time since start: 0:13:49.770554
[batch 1800] samples: 28800, Training Loss: 0.0030
   Time since start: 0:13:51.088449
[batch 1820] samples: 29120, Training Loss: 0.0482
   Time since start: 0:13:52.411262
[batch 1840] samples: 29440, Training Loss: 0.0002
   Time since start: 0:13:53.676490
[batch 1860] samples: 29760, Training Loss: 0.0011
   Time since start: 0:13:54.935845
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:13:56.201174
[batch 1900] samples: 30400, Training Loss: 0.0031
   Time since start: 0:13:57.461349
[batch 1920] samples: 30720, Training Loss: 0.0040
   Time since start: 0:13:58.723812
[batch 1940] samples: 31040, Training Loss: 0.0058
   Time since start: 0:13:59.983278
[batch 1960] samples: 31360, Training Loss: 0.0058
   Time since start: 0:14:01.230352
--m-Epoch 5 done.
   Training Loss: 0.0043
   Validation Loss: 0.0026
Epoch: 6 of 30
[batch 20] samples: 320, Training Loss: 0.0003
   Time since start: 0:14:15.141548
[batch 40] samples: 640, Training Loss: 0.0252
   Time since start: 0:14:17.025958
[batch 60] samples: 960, Training Loss: 0.0004
   Time since start: 0:14:18.413594
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:14:19.796807
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:14:21.179346
[batch 120] samples: 1920, Training Loss: 0.0039
   Time since start: 0:14:22.561791
[batch 140] samples: 2240, Training Loss: 0.0036
   Time since start: 0:14:24.332051
[batch 160] samples: 2560, Training Loss: 0.0025
   Time since start: 0:14:26.351210
[batch 180] samples: 2880, Training Loss: 0.0006
   Time since start: 0:14:28.371891
[batch 200] samples: 3200, Training Loss: 0.0013
   Time since start: 0:14:30.392494
[batch 220] samples: 3520, Training Loss: 0.0010
   Time since start: 0:14:32.414074
[batch 240] samples: 3840, Training Loss: 0.0018
   Time since start: 0:14:34.436066
[batch 260] samples: 4160, Training Loss: 0.0031
   Time since start: 0:14:36.459887
[batch 280] samples: 4480, Training Loss: 0.0021
   Time since start: 0:14:38.420645
[batch 300] samples: 4800, Training Loss: 0.0004
   Time since start: 0:14:40.364902
[batch 320] samples: 5120, Training Loss: 0.0056
   Time since start: 0:14:42.305379
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:14:44.236990
[batch 360] samples: 5760, Training Loss: 0.0139
   Time since start: 0:14:46.168585
[batch 380] samples: 6080, Training Loss: 0.0035
   Time since start: 0:14:48.101454
[batch 400] samples: 6400, Training Loss: 0.0015
   Time since start: 0:14:50.023019
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:14:51.944939
[batch 440] samples: 7040, Training Loss: 0.0012
   Time since start: 0:14:53.867863
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:14:55.808780
[batch 480] samples: 7680, Training Loss: 0.0015
   Time since start: 0:14:57.523599
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:14:59.315586
[batch 520] samples: 8320, Training Loss: 0.0006
   Time since start: 0:15:01.106100
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:15:02.881553
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:15:04.228104
[batch 580] samples: 9280, Training Loss: 0.0044
   Time since start: 0:15:05.843498
[batch 600] samples: 9600, Training Loss: 0.0004
   Time since start: 0:15:07.726012
[batch 620] samples: 9920, Training Loss: 0.0007
   Time since start: 0:15:09.587519
[batch 640] samples: 10240, Training Loss: 0.0100
   Time since start: 0:15:11.569397
[batch 660] samples: 10560, Training Loss: 0.0015
   Time since start: 0:15:13.469510
[batch 680] samples: 10880, Training Loss: 0.0004
   Time since start: 0:15:15.400491
[batch 700] samples: 11200, Training Loss: 0.0040
   Time since start: 0:15:16.896908
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:15:18.279369
[batch 740] samples: 11840, Training Loss: 0.0025
   Time since start: 0:15:19.663037
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:15:21.083651
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:15:22.875159
[batch 800] samples: 12800, Training Loss: 0.0004
   Time since start: 0:15:24.658748
[batch 820] samples: 13120, Training Loss: 0.0058
   Time since start: 0:15:26.429566
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:15:28.255370
[batch 860] samples: 13760, Training Loss: 0.0029
   Time since start: 0:15:30.064683
[batch 880] samples: 14080, Training Loss: 0.0005
   Time since start: 0:15:31.996479
[batch 900] samples: 14400, Training Loss: 0.0309
   Time since start: 0:15:33.512901
[batch 920] samples: 14720, Training Loss: 0.0073
   Time since start: 0:15:34.886841
[batch 940] samples: 15040, Training Loss: 0.0008
   Time since start: 0:15:36.257886
[batch 960] samples: 15360, Training Loss: 0.0010
   Time since start: 0:15:37.580949
[batch 980] samples: 15680, Training Loss: 0.0018
   Time since start: 0:15:38.897832
[batch 1000] samples: 16000, Training Loss: 0.0003
   Time since start: 0:15:40.208565
[batch 1020] samples: 16320, Training Loss: 0.0027
   Time since start: 0:15:41.746048
[batch 1040] samples: 16640, Training Loss: 0.0003
   Time since start: 0:15:43.526808
[batch 1060] samples: 16960, Training Loss: 0.0005
   Time since start: 0:15:45.335243
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:15:47.083254
[batch 1100] samples: 17600, Training Loss: 0.0020
   Time since start: 0:15:48.869241
[batch 1120] samples: 17920, Training Loss: 0.0036
   Time since start: 0:15:50.649024
[batch 1140] samples: 18240, Training Loss: 0.0057
   Time since start: 0:15:52.008976
[batch 1160] samples: 18560, Training Loss: 0.0034
   Time since start: 0:15:53.314546
[batch 1180] samples: 18880, Training Loss: 0.0005
   Time since start: 0:15:54.617785
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:15:55.921897
[batch 1220] samples: 19520, Training Loss: 0.0003
   Time since start: 0:15:57.224989
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:15:58.529285
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:15:59.833876
[batch 1280] samples: 20480, Training Loss: 0.0044
   Time since start: 0:16:01.181918
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:16:02.554659
[batch 1320] samples: 21120, Training Loss: 0.0006
   Time since start: 0:16:03.924194
[batch 1340] samples: 21440, Training Loss: 0.0007
   Time since start: 0:16:05.241598
[batch 1360] samples: 21760, Training Loss: 0.0024
   Time since start: 0:16:06.547464
[batch 1380] samples: 22080, Training Loss: 0.0009
   Time since start: 0:16:07.861117
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:16:09.175771
[batch 1420] samples: 22720, Training Loss: 0.0049
   Time since start: 0:16:10.481436
[batch 1440] samples: 23040, Training Loss: 0.0048
   Time since start: 0:16:11.787502
[batch 1460] samples: 23360, Training Loss: 0.0032
   Time since start: 0:16:13.100010
[batch 1480] samples: 23680, Training Loss: 0.0007
   Time since start: 0:16:14.406768
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:16:15.712015
[batch 1520] samples: 24320, Training Loss: 0.0104
   Time since start: 0:16:17.018088
[batch 1540] samples: 24640, Training Loss: 0.0003
   Time since start: 0:16:18.324877
[batch 1560] samples: 24960, Training Loss: 0.0010
   Time since start: 0:16:19.630463
[batch 1580] samples: 25280, Training Loss: 0.0002
   Time since start: 0:16:20.936985
[batch 1600] samples: 25600, Training Loss: 0.0105
   Time since start: 0:16:22.241875
[batch 1620] samples: 25920, Training Loss: 0.0062
   Time since start: 0:16:23.549438
[batch 1640] samples: 26240, Training Loss: 0.0067
   Time since start: 0:16:24.855749
[batch 1660] samples: 26560, Training Loss: 0.0006
   Time since start: 0:16:26.162134
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:16:27.469103
[batch 1700] samples: 27200, Training Loss: 0.0009
   Time since start: 0:16:28.776137
[batch 1720] samples: 27520, Training Loss: 0.0034
   Time since start: 0:16:30.081339
[batch 1740] samples: 27840, Training Loss: 0.0006
   Time since start: 0:16:31.817300
[batch 1760] samples: 28160, Training Loss: 0.0009
   Time since start: 0:16:33.405877
[batch 1780] samples: 28480, Training Loss: 0.0015
   Time since start: 0:16:35.273028
[batch 1800] samples: 28800, Training Loss: 0.0013
   Time since start: 0:16:36.986697
[batch 1820] samples: 29120, Training Loss: 0.0005
   Time since start: 0:16:38.338358
[batch 1840] samples: 29440, Training Loss: 0.0024
   Time since start: 0:16:39.713767
[batch 1860] samples: 29760, Training Loss: 0.0003
   Time since start: 0:16:41.222529
[batch 1880] samples: 30080, Training Loss: 0.0022
   Time since start: 0:16:43.232860
[batch 1900] samples: 30400, Training Loss: 0.0008
   Time since start: 0:16:44.786885
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:16:46.378384
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:16:47.791006
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:16:49.563794
--m-Epoch 6 done.
   Training Loss: 0.0037
   Validation Loss: 0.0016
Epoch: 7 of 30
[batch 20] samples: 320, Training Loss: 0.0015
   Time since start: 0:17:01.350081
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:17:02.701598
[batch 60] samples: 960, Training Loss: 0.0001
   Time since start: 0:17:04.415788
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:17:06.228396
[batch 100] samples: 1600, Training Loss: 0.0003
   Time since start: 0:17:08.051678
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:17:09.889008
[batch 140] samples: 2240, Training Loss: 0.0002
   Time since start: 0:17:11.645064
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:17:13.354711
[batch 180] samples: 2880, Training Loss: 0.0076
   Time since start: 0:17:15.076642
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:17:16.835527
[batch 220] samples: 3520, Training Loss: 0.0005
   Time since start: 0:17:18.557556
[batch 240] samples: 3840, Training Loss: 0.0039
   Time since start: 0:17:20.278837
[batch 260] samples: 4160, Training Loss: 0.0010
   Time since start: 0:17:21.993244
[batch 280] samples: 4480, Training Loss: 0.0030
   Time since start: 0:17:23.699718
[batch 300] samples: 4800, Training Loss: 0.0123
   Time since start: 0:17:25.420104
[batch 320] samples: 5120, Training Loss: 0.0149
   Time since start: 0:17:26.804581
[batch 340] samples: 5440, Training Loss: 0.0009
   Time since start: 0:17:28.070050
[batch 360] samples: 5760, Training Loss: 0.0003
   Time since start: 0:17:29.330590
[batch 380] samples: 6080, Training Loss: 0.0029
   Time since start: 0:17:30.590525
[batch 400] samples: 6400, Training Loss: 0.0094
   Time since start: 0:17:31.895412
[batch 420] samples: 6720, Training Loss: 0.0085
   Time since start: 0:17:33.199584
[batch 440] samples: 7040, Training Loss: 0.0050
   Time since start: 0:17:34.499920
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:17:35.798954
[batch 480] samples: 7680, Training Loss: 0.0061
   Time since start: 0:17:37.097584
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:17:38.396962
[batch 520] samples: 8320, Training Loss: 0.0098
   Time since start: 0:17:39.700133
[batch 540] samples: 8640, Training Loss: 0.0101
   Time since start: 0:17:41.000774
[batch 560] samples: 8960, Training Loss: 0.0068
   Time since start: 0:17:42.301495
[batch 580] samples: 9280, Training Loss: 0.0008
   Time since start: 0:17:43.602747
[batch 600] samples: 9600, Training Loss: 0.0031
   Time since start: 0:17:44.907237
[batch 620] samples: 9920, Training Loss: 0.0048
   Time since start: 0:17:46.208556
[batch 640] samples: 10240, Training Loss: 0.0304
   Time since start: 0:17:47.508425
[batch 660] samples: 10560, Training Loss: 0.0013
   Time since start: 0:17:48.810692
[batch 680] samples: 10880, Training Loss: 0.0102
   Time since start: 0:17:50.111681
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:17:51.413346
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:17:52.721929
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:17:54.024146
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:17:55.324608
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:17:56.626948
[batch 800] samples: 12800, Training Loss: 0.0008
   Time since start: 0:17:57.927434
[batch 820] samples: 13120, Training Loss: 0.0009
   Time since start: 0:17:59.227945
[batch 840] samples: 13440, Training Loss: 0.0007
   Time since start: 0:18:00.528796
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:18:01.829805
[batch 880] samples: 14080, Training Loss: 0.0007
   Time since start: 0:18:03.131894
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:18:04.433193
[batch 920] samples: 14720, Training Loss: 0.0326
   Time since start: 0:18:05.733332
[batch 940] samples: 15040, Training Loss: 0.0060
   Time since start: 0:18:07.061296
[batch 960] samples: 15360, Training Loss: 0.0015
   Time since start: 0:18:08.402607
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:18:09.741112
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:18:11.083144
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:18:12.420834
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:18:13.758389
[batch 1060] samples: 16960, Training Loss: 0.0008
   Time since start: 0:18:15.096798
[batch 1080] samples: 17280, Training Loss: 0.0033
   Time since start: 0:18:16.433564
[batch 1100] samples: 17600, Training Loss: 0.0191
   Time since start: 0:18:17.771888
[batch 1120] samples: 17920, Training Loss: 0.0004
   Time since start: 0:18:19.110227
[batch 1140] samples: 18240, Training Loss: 0.0051
   Time since start: 0:18:21.087961
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:18:22.987375
[batch 1180] samples: 18880, Training Loss: 0.0091
   Time since start: 0:18:24.850786
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:18:26.721932
[batch 1220] samples: 19520, Training Loss: 0.0052
   Time since start: 0:18:28.604274
[batch 1240] samples: 19840, Training Loss: 0.0095
   Time since start: 0:18:30.475773
[batch 1260] samples: 20160, Training Loss: 0.0008
   Time since start: 0:18:32.345337
[batch 1280] samples: 20480, Training Loss: 0.0008
   Time since start: 0:18:34.084530
[batch 1300] samples: 20800, Training Loss: 0.0006
   Time since start: 0:18:35.871184
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:18:37.649937
[batch 1340] samples: 21440, Training Loss: 0.0006
   Time since start: 0:18:39.442417
[batch 1360] samples: 21760, Training Loss: 0.0192
   Time since start: 0:18:41.227531
[batch 1380] samples: 22080, Training Loss: 0.0005
   Time since start: 0:18:42.996353
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:18:44.774946
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:18:46.557562
[batch 1440] samples: 23040, Training Loss: 0.0003
   Time since start: 0:18:48.351195
[batch 1460] samples: 23360, Training Loss: 0.0005
   Time since start: 0:18:50.132119
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:18:51.914979
[batch 1500] samples: 24000, Training Loss: 0.0031
   Time since start: 0:18:53.680904
[batch 1520] samples: 24320, Training Loss: 0.0002
   Time since start: 0:18:55.452799
[batch 1540] samples: 24640, Training Loss: 0.0029
   Time since start: 0:18:57.209914
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:18:58.991675
[batch 1580] samples: 25280, Training Loss: 0.0025
   Time since start: 0:19:00.780556
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:19:02.558862
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:19:04.330338
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:19:06.103614
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:19:07.423229
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:19:08.721023
[batch 1700] samples: 27200, Training Loss: 0.0123
   Time since start: 0:19:10.018015
[batch 1720] samples: 27520, Training Loss: 0.0005
   Time since start: 0:19:11.318971
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:19:12.621737
[batch 1760] samples: 28160, Training Loss: 0.0089
   Time since start: 0:19:13.928878
[batch 1780] samples: 28480, Training Loss: 0.0031
   Time since start: 0:19:15.235881
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:19:16.547758
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:19:17.984257
[batch 1840] samples: 29440, Training Loss: 0.0005
   Time since start: 0:19:19.814852
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:19:21.701465
[batch 1880] samples: 30080, Training Loss: 0.0013
   Time since start: 0:19:23.582775
[batch 1900] samples: 30400, Training Loss: 0.0098
   Time since start: 0:19:25.289003
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:19:26.674742
[batch 1940] samples: 31040, Training Loss: 0.0062
   Time since start: 0:19:28.497741
[batch 1960] samples: 31360, Training Loss: 0.0090
   Time since start: 0:19:30.336690
--m-Epoch 7 done.
   Training Loss: 0.0031
   Validation Loss: 0.0037
Patience decreased: Patience is now  4
Epoch: 8 of 30
[batch 20] samples: 320, Training Loss: 0.0095
   Time since start: 0:19:42.942739
[batch 40] samples: 640, Training Loss: 0.0010
   Time since start: 0:19:44.279352
[batch 60] samples: 960, Training Loss: 0.0005
   Time since start: 0:19:45.837719
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:19:47.558137
[batch 100] samples: 1600, Training Loss: 0.0036
   Time since start: 0:19:48.898077
[batch 120] samples: 1920, Training Loss: 0.0004
   Time since start: 0:19:50.234707
[batch 140] samples: 2240, Training Loss: 0.0002
   Time since start: 0:19:51.573143
[batch 160] samples: 2560, Training Loss: 0.0023
   Time since start: 0:19:52.911991
[batch 180] samples: 2880, Training Loss: 0.0011
   Time since start: 0:19:54.251773
[batch 200] samples: 3200, Training Loss: 0.0019
   Time since start: 0:19:55.589199
[batch 220] samples: 3520, Training Loss: 0.0270
   Time since start: 0:19:56.930436
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:19:58.276997
[batch 260] samples: 4160, Training Loss: 0.0075
   Time since start: 0:19:59.619544
[batch 280] samples: 4480, Training Loss: 0.0027
   Time since start: 0:20:00.960738
[batch 300] samples: 4800, Training Loss: 0.0014
   Time since start: 0:20:02.300095
[batch 320] samples: 5120, Training Loss: 0.0030
   Time since start: 0:20:03.642002
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:20:04.979672
[batch 360] samples: 5760, Training Loss: 0.0026
   Time since start: 0:20:06.317701
[batch 380] samples: 6080, Training Loss: 0.0028
   Time since start: 0:20:07.655877
[batch 400] samples: 6400, Training Loss: 0.0172
   Time since start: 0:20:09.111925
[batch 420] samples: 6720, Training Loss: 0.0004
   Time since start: 0:20:10.677529
[batch 440] samples: 7040, Training Loss: 0.0172
   Time since start: 0:20:12.545460
[batch 460] samples: 7360, Training Loss: 0.0086
   Time since start: 0:20:14.416191
[batch 480] samples: 7680, Training Loss: 0.0007
   Time since start: 0:20:16.277046
[batch 500] samples: 8000, Training Loss: 0.0004
   Time since start: 0:20:18.146606
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:20:20.019513
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:20:21.889128
[batch 560] samples: 8960, Training Loss: 0.0011
   Time since start: 0:20:23.758224
[batch 580] samples: 9280, Training Loss: 0.0052
   Time since start: 0:20:25.633964
[batch 600] samples: 9600, Training Loss: 0.0012
   Time since start: 0:20:27.512778
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:20:28.945837
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:20:30.457946
[batch 660] samples: 10560, Training Loss: 0.0042
   Time since start: 0:20:32.328302
[batch 680] samples: 10880, Training Loss: 0.0011
   Time since start: 0:20:34.184332
[batch 700] samples: 11200, Training Loss: 0.0005
   Time since start: 0:20:35.961370
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:20:37.721852
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:20:39.494395
[batch 760] samples: 12160, Training Loss: 0.0034
   Time since start: 0:20:40.958954
[batch 780] samples: 12480, Training Loss: 0.0004
   Time since start: 0:20:42.217592
[batch 800] samples: 12800, Training Loss: 0.0009
   Time since start: 0:20:43.478632
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:20:44.747535
[batch 840] samples: 13440, Training Loss: 0.0011
   Time since start: 0:20:46.008142
[batch 860] samples: 13760, Training Loss: 0.0031
   Time since start: 0:20:47.272283
[batch 880] samples: 14080, Training Loss: 0.0018
   Time since start: 0:20:48.531745
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:20:49.806899
[batch 920] samples: 14720, Training Loss: 0.0026
   Time since start: 0:20:51.106296
[batch 940] samples: 15040, Training Loss: 0.0011
   Time since start: 0:20:52.406196
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:20:53.729541
[batch 980] samples: 15680, Training Loss: 0.0020
   Time since start: 0:20:55.076567
[batch 1000] samples: 16000, Training Loss: 0.0006
   Time since start: 0:20:56.453063
[batch 1020] samples: 16320, Training Loss: 0.0002
   Time since start: 0:20:57.961778
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:20:59.802347
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:21:01.774092
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:21:03.684120
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:21:05.552321
[batch 1120] samples: 17920, Training Loss: 0.0005
   Time since start: 0:21:06.949959
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:21:08.292773
[batch 1160] samples: 18560, Training Loss: 0.0082
   Time since start: 0:21:09.628923
[batch 1180] samples: 18880, Training Loss: 0.0002
   Time since start: 0:21:10.964747
[batch 1200] samples: 19200, Training Loss: 0.0019
   Time since start: 0:21:12.300299
[batch 1220] samples: 19520, Training Loss: 0.0033
   Time since start: 0:21:13.637546
[batch 1240] samples: 19840, Training Loss: 0.0003
   Time since start: 0:21:15.386276
[batch 1260] samples: 20160, Training Loss: 0.0015
   Time since start: 0:21:17.206157
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:21:19.009451
[batch 1300] samples: 20800, Training Loss: 0.0012
   Time since start: 0:21:20.307975
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:21:21.606974
[batch 1340] samples: 21440, Training Loss: 0.0084
   Time since start: 0:21:22.910214
[batch 1360] samples: 21760, Training Loss: 0.0064
   Time since start: 0:21:24.211360
[batch 1380] samples: 22080, Training Loss: 0.0516
   Time since start: 0:21:25.512674
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:21:26.813962
[batch 1420] samples: 22720, Training Loss: 0.0023
   Time since start: 0:21:28.146856
[batch 1440] samples: 23040, Training Loss: 0.0014
   Time since start: 0:21:29.457800
[batch 1460] samples: 23360, Training Loss: 0.0017
   Time since start: 0:21:30.796747
[batch 1480] samples: 23680, Training Loss: 0.0098
   Time since start: 0:21:32.138649
[batch 1500] samples: 24000, Training Loss: 0.0006
   Time since start: 0:21:33.477916
[batch 1520] samples: 24320, Training Loss: 0.0027
   Time since start: 0:21:34.792496
[batch 1540] samples: 24640, Training Loss: 0.0002
   Time since start: 0:21:36.095930
[batch 1560] samples: 24960, Training Loss: 0.0010
   Time since start: 0:21:37.399056
[batch 1580] samples: 25280, Training Loss: 0.0039
   Time since start: 0:21:38.701953
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:21:40.004883
[batch 1620] samples: 25920, Training Loss: 0.0007
   Time since start: 0:21:41.426044
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:21:42.751737
[batch 1660] samples: 26560, Training Loss: 0.0009
   Time since start: 0:21:44.093596
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:21:45.438365
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:21:46.758166
[batch 1720] samples: 27520, Training Loss: 0.0004
   Time since start: 0:21:48.063270
[batch 1740] samples: 27840, Training Loss: 0.0033
   Time since start: 0:21:49.367836
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:21:50.671566
[batch 1780] samples: 28480, Training Loss: 0.0002
   Time since start: 0:21:51.980346
[batch 1800] samples: 28800, Training Loss: 0.0007
   Time since start: 0:21:53.286071
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:21:54.589654
[batch 1840] samples: 29440, Training Loss: 0.0003
   Time since start: 0:21:55.895288
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:21:57.200363
[batch 1880] samples: 30080, Training Loss: 0.0004
   Time since start: 0:21:58.505254
[batch 1900] samples: 30400, Training Loss: 0.0004
   Time since start: 0:21:59.811707
[batch 1920] samples: 30720, Training Loss: 0.0004
   Time since start: 0:22:01.117770
[batch 1940] samples: 31040, Training Loss: 0.0028
   Time since start: 0:22:02.667445
[batch 1960] samples: 31360, Training Loss: 0.0197
   Time since start: 0:22:03.981662
--m-Epoch 8 done.
   Training Loss: 0.0033
   Validation Loss: 0.0029
Patience decreased: Patience is now  3
Epoch: 9 of 30
[batch 20] samples: 320, Training Loss: 0.0035
   Time since start: 0:22:16.739828
[batch 40] samples: 640, Training Loss: 0.0005
   Time since start: 0:22:18.609509
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:22:20.468493
[batch 80] samples: 1280, Training Loss: 0.0011
   Time since start: 0:22:22.337116
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:22:24.216231
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:22:26.085392
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:22:27.957518
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:22:29.828346
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:22:31.710038
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:22:33.628210
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:22:35.530411
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:22:37.421055
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:22:39.318899
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:22:41.202847
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:22:43.101986
[batch 320] samples: 5120, Training Loss: 0.0045
   Time since start: 0:22:45.010097
[batch 340] samples: 5440, Training Loss: 0.0016
   Time since start: 0:22:46.393496
[batch 360] samples: 5760, Training Loss: 0.0033
   Time since start: 0:22:47.742532
[batch 380] samples: 6080, Training Loss: 0.0014
   Time since start: 0:22:49.088132
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:22:50.432872
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:22:51.780577
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:22:53.127567
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:22:54.472954
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:22:55.819073
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:22:57.163981
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:22:58.493742
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:22:59.800824
[batch 560] samples: 8960, Training Loss: 0.0042
   Time since start: 0:23:01.107529
[batch 580] samples: 9280, Training Loss: 0.0004
   Time since start: 0:23:02.414756
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:23:03.721541
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:23:05.171814
[batch 640] samples: 10240, Training Loss: 0.0010
   Time since start: 0:23:06.770946
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:23:08.653163
[batch 680] samples: 10880, Training Loss: 0.0005
   Time since start: 0:23:10.189882
[batch 700] samples: 11200, Training Loss: 0.0046
   Time since start: 0:23:11.488030
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:23:12.786330
[batch 740] samples: 11840, Training Loss: 0.0008
   Time since start: 0:23:14.084182
[batch 760] samples: 12160, Training Loss: 0.0020
   Time since start: 0:23:15.380927
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:23:16.790922
[batch 800] samples: 12800, Training Loss: 0.0011
   Time since start: 0:23:18.650978
[batch 820] samples: 13120, Training Loss: 0.0005
   Time since start: 0:23:20.520456
[batch 840] samples: 13440, Training Loss: 0.0001
   Time since start: 0:23:22.384506
[batch 860] samples: 13760, Training Loss: 0.0031
   Time since start: 0:23:24.254165
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:23:26.248897
[batch 900] samples: 14400, Training Loss: 0.0027
   Time since start: 0:23:28.111097
[batch 920] samples: 14720, Training Loss: 0.0005
   Time since start: 0:23:29.981078
[batch 940] samples: 15040, Training Loss: 0.0036
   Time since start: 0:23:31.853706
[batch 960] samples: 15360, Training Loss: 0.0006
   Time since start: 0:23:33.654865
[batch 980] samples: 15680, Training Loss: 0.0210
   Time since start: 0:23:35.384821
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:23:36.922790
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:23:38.821124
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:23:40.715968
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:23:42.609072
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:23:44.495355
[batch 1100] samples: 17600, Training Loss: 0.0009
   Time since start: 0:23:46.133089
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:23:47.688024
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:23:49.361904
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:23:51.060240
[batch 1180] samples: 18880, Training Loss: 0.0002
   Time since start: 0:23:52.858566
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:23:54.631754
[batch 1220] samples: 19520, Training Loss: 0.0026
   Time since start: 0:23:56.406479
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:23:58.216818
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:24:00.028013
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:24:01.883266
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:24:03.701281
[batch 1320] samples: 21120, Training Loss: 0.0018
   Time since start: 0:24:05.493805
[batch 1340] samples: 21440, Training Loss: 0.0069
   Time since start: 0:24:07.270571
[batch 1360] samples: 21760, Training Loss: 0.0004
   Time since start: 0:24:09.034521
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:24:10.803503
[batch 1400] samples: 22400, Training Loss: 0.0008
   Time since start: 0:24:12.551864
[batch 1420] samples: 22720, Training Loss: 0.0019
   Time since start: 0:24:13.827804
[batch 1440] samples: 23040, Training Loss: 0.0012
   Time since start: 0:24:15.103263
[batch 1460] samples: 23360, Training Loss: 0.0004
   Time since start: 0:24:16.977193
[batch 1480] samples: 23680, Training Loss: 0.0002
   Time since start: 0:24:18.916456
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:24:20.845674
[batch 1520] samples: 24320, Training Loss: 0.0007
   Time since start: 0:24:22.729582
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:24:24.640071
[batch 1560] samples: 24960, Training Loss: 0.0042
   Time since start: 0:24:26.550389
[batch 1580] samples: 25280, Training Loss: 0.0007
   Time since start: 0:24:28.459681
[batch 1600] samples: 25600, Training Loss: 0.0035
   Time since start: 0:24:30.002969
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:24:31.339596
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:24:32.679413
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:24:34.571022
[batch 1680] samples: 26880, Training Loss: 0.0010
   Time since start: 0:24:36.591389
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:24:38.613435
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:24:40.634491
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:24:42.655629
[batch 1760] samples: 28160, Training Loss: 0.0029
   Time since start: 0:24:44.678032
[batch 1780] samples: 28480, Training Loss: 0.0076
   Time since start: 0:24:46.698538
[batch 1800] samples: 28800, Training Loss: 0.0027
   Time since start: 0:24:48.710108
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:24:50.732346
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:24:52.751893
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:24:54.774199
[batch 1880] samples: 30080, Training Loss: 0.0002
   Time since start: 0:24:56.784562
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:24:58.806569
[batch 1920] samples: 30720, Training Loss: 0.0082
   Time since start: 0:25:00.828340
[batch 1940] samples: 31040, Training Loss: 0.0015
   Time since start: 0:25:02.849766
[batch 1960] samples: 31360, Training Loss: 0.0019
   Time since start: 0:25:04.790103
--m-Epoch 9 done.
   Training Loss: 0.0018
   Validation Loss: 0.0034
Patience decreased: Patience is now  2
Epoch: 10 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:25:18.401657
[batch 40] samples: 640, Training Loss: 0.0003
   Time since start: 0:25:20.201422
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:25:21.989481
[batch 80] samples: 1280, Training Loss: 0.0029
   Time since start: 0:25:23.807239
[batch 100] samples: 1600, Training Loss: 0.0016
   Time since start: 0:25:25.606453
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:25:27.415578
[batch 140] samples: 2240, Training Loss: 0.0071
   Time since start: 0:25:29.335823
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:25:31.131182
[batch 180] samples: 2880, Training Loss: 0.0015
   Time since start: 0:25:32.940978
[batch 200] samples: 3200, Training Loss: 0.0015
   Time since start: 0:25:34.683969
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:25:36.595741
[batch 240] samples: 3840, Training Loss: 0.0001
   Time since start: 0:25:38.494841
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:25:40.395869
[batch 280] samples: 4480, Training Loss: 0.0009
   Time since start: 0:25:42.287537
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:25:44.183146
[batch 320] samples: 5120, Training Loss: 0.0007
   Time since start: 0:25:46.076826
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:25:47.956681
[batch 360] samples: 5760, Training Loss: 0.0062
   Time since start: 0:25:49.785844
[batch 380] samples: 6080, Training Loss: 0.0009
   Time since start: 0:25:51.563154
[batch 400] samples: 6400, Training Loss: 0.0023
   Time since start: 0:25:53.219138
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:25:54.529842
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:25:55.840075
[batch 460] samples: 7360, Training Loss: 0.0035
   Time since start: 0:25:57.148897
[batch 480] samples: 7680, Training Loss: 0.0017
   Time since start: 0:25:58.460482
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:25:59.770784
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:26:01.224283
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:26:03.054775
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:26:04.414730
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:26:05.746444
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:26:07.049537
[batch 620] samples: 9920, Training Loss: 0.0027
   Time since start: 0:26:08.357018
[batch 640] samples: 10240, Training Loss: 0.0008
   Time since start: 0:26:09.662888
[batch 660] samples: 10560, Training Loss: 0.0002
   Time since start: 0:26:10.962505
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:26:12.261662
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:26:13.562174
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:26:14.861343
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:26:16.167099
[batch 760] samples: 12160, Training Loss: 0.0011
   Time since start: 0:26:17.470559
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:26:18.773924
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:26:20.075016
[batch 820] samples: 13120, Training Loss: 0.0003
   Time since start: 0:26:21.376975
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:26:23.275661
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:26:25.202852
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:26:27.132452
[batch 900] samples: 14400, Training Loss: 0.0002
   Time since start: 0:26:29.067684
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:26:30.937524
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:26:32.769857
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:26:34.120614
[batch 980] samples: 15680, Training Loss: 0.0008
   Time since start: 0:26:35.462106
[batch 1000] samples: 16000, Training Loss: 0.0016
   Time since start: 0:26:36.802135
[batch 1020] samples: 16320, Training Loss: 0.0003
   Time since start: 0:26:38.142614
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:26:39.482676
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:26:40.822454
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:26:42.162051
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:26:43.502635
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:26:44.846725
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:26:46.184950
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:26:47.523198
[batch 1180] samples: 18880, Training Loss: 0.0081
   Time since start: 0:26:48.862217
[batch 1200] samples: 19200, Training Loss: 0.0021
   Time since start: 0:26:50.199224
[batch 1220] samples: 19520, Training Loss: 0.0034
   Time since start: 0:26:52.080185
[batch 1240] samples: 19840, Training Loss: 0.0002
   Time since start: 0:26:53.935710
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:26:55.798502
[batch 1280] samples: 20480, Training Loss: 0.0010
   Time since start: 0:26:57.669968
[batch 1300] samples: 20800, Training Loss: 0.0030
   Time since start: 0:26:59.542885
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:27:01.413827
[batch 1340] samples: 21440, Training Loss: 0.0019
   Time since start: 0:27:03.273559
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:27:05.164859
[batch 1380] samples: 22080, Training Loss: 0.0075
   Time since start: 0:27:07.156210
[batch 1400] samples: 22400, Training Loss: 0.0022
   Time since start: 0:27:09.037102
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:27:10.907077
[batch 1440] samples: 23040, Training Loss: 0.0026
   Time since start: 0:27:12.750559
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:27:14.612032
[batch 1480] samples: 23680, Training Loss: 0.0050
   Time since start: 0:27:16.480675
[batch 1500] samples: 24000, Training Loss: 0.0051
   Time since start: 0:27:17.962401
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:27:19.301119
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:27:20.639329
[batch 1560] samples: 24960, Training Loss: 0.0001
   Time since start: 0:27:21.978534
[batch 1580] samples: 25280, Training Loss: 0.0065
   Time since start: 0:27:23.520164
[batch 1600] samples: 25600, Training Loss: 0.0036
   Time since start: 0:27:25.400984
[batch 1620] samples: 25920, Training Loss: 0.0013
   Time since start: 0:27:27.269002
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:27:29.150708
[batch 1660] samples: 26560, Training Loss: 0.0033
   Time since start: 0:27:31.073411
[batch 1680] samples: 26880, Training Loss: 0.0006
   Time since start: 0:27:33.025028
[batch 1700] samples: 27200, Training Loss: 0.0033
   Time since start: 0:27:34.968239
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:27:36.891083
[batch 1740] samples: 27840, Training Loss: 0.0015
   Time since start: 0:27:38.810212
[batch 1760] samples: 28160, Training Loss: 0.0003
   Time since start: 0:27:40.730650
[batch 1780] samples: 28480, Training Loss: 0.0079
   Time since start: 0:27:42.660160
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:27:44.620228
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:27:46.541033
[batch 1840] samples: 29440, Training Loss: 0.0014
   Time since start: 0:27:48.173260
[batch 1860] samples: 29760, Training Loss: 0.0015
   Time since start: 0:27:49.965054
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:27:51.723817
[batch 1900] samples: 30400, Training Loss: 0.0017
   Time since start: 0:27:53.491729
[batch 1920] samples: 30720, Training Loss: 0.0053
   Time since start: 0:27:55.291525
[batch 1940] samples: 31040, Training Loss: 0.0005
   Time since start: 0:27:56.666168
[batch 1960] samples: 31360, Training Loss: 0.0075
   Time since start: 0:27:57.937601
--m-Epoch 10 done.
   Training Loss: 0.0023
   Validation Loss: 0.0021
Patience decreased: Patience is now  1
Epoch: 11 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:28:11.171741
[batch 40] samples: 640, Training Loss: 0.0005
   Time since start: 0:28:13.041684
[batch 60] samples: 960, Training Loss: 0.0007
   Time since start: 0:28:14.910644
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:28:16.710836
[batch 100] samples: 1600, Training Loss: 0.0003
   Time since start: 0:28:18.110268
[batch 120] samples: 1920, Training Loss: 0.0003
   Time since start: 0:28:19.421964
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:28:20.731326
[batch 160] samples: 2560, Training Loss: 0.0054
   Time since start: 0:28:22.045027
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:28:23.438581
[batch 200] samples: 3200, Training Loss: 0.0011
   Time since start: 0:28:25.295233
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:28:27.160992
[batch 240] samples: 3840, Training Loss: 0.0019
   Time since start: 0:28:29.013822
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:28:30.877694
[batch 280] samples: 4480, Training Loss: 0.0062
   Time since start: 0:28:32.747483
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:28:34.618645
[batch 320] samples: 5120, Training Loss: 0.0083
   Time since start: 0:28:36.490722
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:28:38.272284
[batch 360] samples: 5760, Training Loss: 0.0002
   Time since start: 0:28:40.054804
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:28:41.864267
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:28:43.667973
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:28:45.498705
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:28:47.394612
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:28:49.296085
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:28:51.279317
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:28:53.267183
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:28:55.251429
[batch 540] samples: 8640, Training Loss: 0.0045
   Time since start: 0:28:57.248122
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:28:59.229507
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:29:00.943969
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:29:02.437738
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:29:03.935034
[batch 640] samples: 10240, Training Loss: 0.0006
   Time since start: 0:29:05.555836
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:29:07.052082
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:29:08.556213
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:29:10.058347
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:29:11.554938
[batch 740] samples: 11840, Training Loss: 0.0095
   Time since start: 0:29:13.055925
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:29:14.554474
[batch 780] samples: 12480, Training Loss: 0.0027
   Time since start: 0:29:16.053287
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:29:17.553573
[batch 820] samples: 13120, Training Loss: 0.0002
   Time since start: 0:29:19.048467
[batch 840] samples: 13440, Training Loss: 0.0004
   Time since start: 0:29:20.548397
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:29:22.090282
[batch 880] samples: 14080, Training Loss: 0.0132
   Time since start: 0:29:24.064063
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:29:26.045890
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:29:27.979745
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:29:29.903877
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:29:31.823925
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:29:33.770236
[batch 1000] samples: 16000, Training Loss: 0.0013
   Time since start: 0:29:35.775514
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:29:37.802188
[batch 1040] samples: 16640, Training Loss: 0.0022
   Time since start: 0:29:39.825409
[batch 1060] samples: 16960, Training Loss: 0.0033
   Time since start: 0:29:41.851907
[batch 1080] samples: 17280, Training Loss: 0.0046
   Time since start: 0:29:43.873048
[batch 1100] samples: 17600, Training Loss: 0.0005
   Time since start: 0:29:45.895309
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:29:47.921002
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:29:49.956043
[batch 1160] samples: 18560, Training Loss: 0.0003
   Time since start: 0:29:51.976402
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:29:53.998666
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:29:55.773104
[batch 1220] samples: 19520, Training Loss: 0.0019
   Time since start: 0:29:57.191543
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:29:58.616936
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:30:00.041714
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:30:01.469386
[batch 1300] samples: 20800, Training Loss: 0.0004
   Time since start: 0:30:02.893873
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:30:04.318729
[batch 1340] samples: 21440, Training Loss: 0.0007
   Time since start: 0:30:05.742387
[batch 1360] samples: 21760, Training Loss: 0.0012
   Time since start: 0:30:07.161298
[batch 1380] samples: 22080, Training Loss: 0.0074
   Time since start: 0:30:08.584601
[batch 1400] samples: 22400, Training Loss: 0.0024
   Time since start: 0:30:10.008458
[batch 1420] samples: 22720, Training Loss: 0.0006
   Time since start: 0:30:11.432975
[batch 1440] samples: 23040, Training Loss: 0.0002
   Time since start: 0:30:12.862159
[batch 1460] samples: 23360, Training Loss: 0.0027
   Time since start: 0:30:14.286139
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:30:15.703734
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:30:17.127925
[batch 1520] samples: 24320, Training Loss: 0.0003
   Time since start: 0:30:18.553577
[batch 1540] samples: 24640, Training Loss: 0.0143
   Time since start: 0:30:20.358957
[batch 1560] samples: 24960, Training Loss: 0.0004
   Time since start: 0:30:22.009567
[batch 1580] samples: 25280, Training Loss: 0.0044
   Time since start: 0:30:23.467829
[batch 1600] samples: 25600, Training Loss: 0.0011
   Time since start: 0:30:24.927135
[batch 1620] samples: 25920, Training Loss: 0.0073
   Time since start: 0:30:26.390005
[batch 1640] samples: 26240, Training Loss: 0.0007
   Time since start: 0:30:27.862522
[batch 1660] samples: 26560, Training Loss: 0.0051
   Time since start: 0:30:29.335066
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:30:30.798306
[batch 1700] samples: 27200, Training Loss: 0.0004
   Time since start: 0:30:32.270964
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:30:33.753593
[batch 1740] samples: 27840, Training Loss: 0.0004
   Time since start: 0:30:35.442650
[batch 1760] samples: 28160, Training Loss: 0.0007
   Time since start: 0:30:37.426621
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:30:39.430719
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:30:41.415529
[batch 1820] samples: 29120, Training Loss: 0.0017
   Time since start: 0:30:43.388577
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:30:45.240050
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:30:46.667116
[batch 1880] samples: 30080, Training Loss: 0.0002
   Time since start: 0:30:48.551123
[batch 1900] samples: 30400, Training Loss: 0.0002
   Time since start: 0:30:50.412898
[batch 1920] samples: 30720, Training Loss: 0.0033
   Time since start: 0:30:52.151627
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:30:53.586388
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:30:54.992116
--m-Epoch 11 done.
   Training Loss: 0.0018
   Validation Loss: 0.0015
Epoch: 12 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:31:09.247383
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:31:10.760280
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:31:12.614398
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:31:14.400453
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:31:15.832775
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:31:17.264546
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:31:18.726545
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:31:20.189003
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:31:21.647003
[batch 200] samples: 3200, Training Loss: 0.0005
   Time since start: 0:31:23.357594
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:31:25.110077
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:31:26.876229
[batch 260] samples: 4160, Training Loss: 0.0010
   Time since start: 0:31:28.664438
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:31:30.420326
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:31:32.198858
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:31:33.993668
[batch 340] samples: 5440, Training Loss: 0.0020
   Time since start: 0:31:35.853483
[batch 360] samples: 5760, Training Loss: 0.0134
   Time since start: 0:31:37.715729
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:31:39.593081
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:31:41.455919
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:31:43.324625
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:31:45.169763
[batch 460] samples: 7360, Training Loss: 0.0046
   Time since start: 0:31:46.644501
[batch 480] samples: 7680, Training Loss: 0.0002
   Time since start: 0:31:48.114685
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:31:49.547435
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:31:50.969938
[batch 540] samples: 8640, Training Loss: 0.0004
   Time since start: 0:31:52.398505
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:31:54.090772
[batch 580] samples: 9280, Training Loss: 0.0016
   Time since start: 0:31:55.913731
[batch 600] samples: 9600, Training Loss: 0.0011
   Time since start: 0:31:57.768395
[batch 620] samples: 9920, Training Loss: 0.0868
   Time since start: 0:31:59.581015
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:32:01.425514
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:32:03.278004
[batch 680] samples: 10880, Training Loss: 0.0002
   Time since start: 0:32:04.714359
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:32:06.152881
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:32:07.590602
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:32:09.023774
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:32:10.457855
[batch 780] samples: 12480, Training Loss: 0.0020
   Time since start: 0:32:11.891723
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:32:13.657950
[batch 820] samples: 13120, Training Loss: 0.0004
   Time since start: 0:32:15.584399
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:32:17.540972
[batch 860] samples: 13760, Training Loss: 0.0010
   Time since start: 0:32:19.067757
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:32:20.991073
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:32:22.899277
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:32:24.360315
[batch 940] samples: 15040, Training Loss: 0.0002
   Time since start: 0:32:25.824987
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:32:27.279032
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:32:28.743623
[batch 1000] samples: 16000, Training Loss: 0.0023
   Time since start: 0:32:30.215095
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:32:31.683278
[batch 1040] samples: 16640, Training Loss: 0.0003
   Time since start: 0:32:33.146116
[batch 1060] samples: 16960, Training Loss: 0.0023
   Time since start: 0:32:34.862548
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:32:36.816442
[batch 1100] samples: 17600, Training Loss: 0.0003
   Time since start: 0:32:38.730297
[batch 1120] samples: 17920, Training Loss: 0.0138
   Time since start: 0:32:40.633992
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:32:42.687043
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:32:44.629452
[batch 1180] samples: 18880, Training Loss: 0.0008
   Time since start: 0:32:46.582175
[batch 1200] samples: 19200, Training Loss: 0.0002
   Time since start: 0:32:48.475343
[batch 1220] samples: 19520, Training Loss: 0.0008
   Time since start: 0:32:50.360594
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:32:52.071065
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:32:53.496943
[batch 1280] samples: 20480, Training Loss: 0.0017
   Time since start: 0:32:54.929811
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:32:56.361524
[batch 1320] samples: 21120, Training Loss: 0.0006
   Time since start: 0:32:57.793225
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:32:59.223124
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:33:00.650416
[batch 1380] samples: 22080, Training Loss: 0.0019
   Time since start: 0:33:02.082819
[batch 1400] samples: 22400, Training Loss: 0.0023
   Time since start: 0:33:03.522264
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:33:05.313085
[batch 1440] samples: 23040, Training Loss: 0.0005
   Time since start: 0:33:07.197397
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:33:09.094590
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:33:11.030972
[batch 1500] samples: 24000, Training Loss: 0.0065
   Time since start: 0:33:12.949851
[batch 1520] samples: 24320, Training Loss: 0.0002
   Time since start: 0:33:14.860449
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:33:16.748291
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:33:18.393477
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:33:20.369479
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:33:22.339685
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:33:24.313554
[batch 1640] samples: 26240, Training Loss: 0.0025
   Time since start: 0:33:26.279532
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:33:28.246974
[batch 1680] samples: 26880, Training Loss: 0.0041
   Time since start: 0:33:30.220790
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:33:32.085886
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:33:33.589614
[batch 1740] samples: 27840, Training Loss: 0.0035
   Time since start: 0:33:35.091090
[batch 1760] samples: 28160, Training Loss: 0.0050
   Time since start: 0:33:36.588041
[batch 1780] samples: 28480, Training Loss: 0.0004
   Time since start: 0:33:38.090701
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:33:39.585780
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:33:41.079118
[batch 1840] samples: 29440, Training Loss: 0.0018
   Time since start: 0:33:42.926914
[batch 1860] samples: 29760, Training Loss: 0.0002
   Time since start: 0:33:44.715120
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:33:46.474170
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:33:48.123342
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:33:49.557845
[batch 1940] samples: 31040, Training Loss: 0.0043
   Time since start: 0:33:50.972595
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:33:52.276573
--m-Epoch 12 done.
   Training Loss: 0.0016
   Validation Loss: 0.0017
Patience decreased: Patience is now  1
Epoch: 13 of 30
[batch 20] samples: 320, Training Loss: 0.0017
   Time since start: 0:34:04.279673
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:34:06.078020
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:34:07.876875
[batch 80] samples: 1280, Training Loss: 0.0003
   Time since start: 0:34:09.252655
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:34:10.652230
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:34:12.363972
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:34:13.834312
[batch 160] samples: 2560, Training Loss: 0.0002
   Time since start: 0:34:15.230836
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:34:16.623128
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:34:18.006962
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:34:19.698581
[batch 240] samples: 3840, Training Loss: 0.0160
   Time since start: 0:34:21.593908
[batch 260] samples: 4160, Training Loss: 0.0061
   Time since start: 0:34:23.514076
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:34:25.414749
[batch 300] samples: 4800, Training Loss: 0.0135
   Time since start: 0:34:27.284406
[batch 320] samples: 5120, Training Loss: 0.0032
   Time since start: 0:34:29.167035
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:34:30.571633
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:34:31.963620
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:34:33.363652
[batch 400] samples: 6400, Training Loss: 0.0015
   Time since start: 0:34:34.822743
[batch 420] samples: 6720, Training Loss: 0.0002
   Time since start: 0:34:36.156519
[batch 440] samples: 7040, Training Loss: 0.0004
   Time since start: 0:34:37.489094
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:34:38.798931
[batch 480] samples: 7680, Training Loss: 0.0025
   Time since start: 0:34:40.113744
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:34:41.467996
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:34:42.876765
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:34:44.272060
[batch 560] samples: 8960, Training Loss: 0.0015
   Time since start: 0:34:45.667463
[batch 580] samples: 9280, Training Loss: 0.0022
   Time since start: 0:34:47.003302
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:34:48.468855
[batch 620] samples: 9920, Training Loss: 0.0011
   Time since start: 0:34:49.860222
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:34:51.222445
[batch 660] samples: 10560, Training Loss: 0.0015
   Time since start: 0:34:52.549702
[batch 680] samples: 10880, Training Loss: 0.0006
   Time since start: 0:34:53.875642
[batch 700] samples: 11200, Training Loss: 0.0004
   Time since start: 0:34:55.204149
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:34:56.527183
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:34:58.041224
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:34:59.645988
[batch 780] samples: 12480, Training Loss: 0.0003
   Time since start: 0:35:01.413558
[batch 800] samples: 12800, Training Loss: 0.0029
   Time since start: 0:35:02.902560
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:35:04.342616
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:35:05.844752
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:35:07.315459
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:35:08.791450
[batch 900] samples: 14400, Training Loss: 0.0113
   Time since start: 0:35:10.690900
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:35:12.561544
[batch 940] samples: 15040, Training Loss: 0.0005
   Time since start: 0:35:14.195381
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:35:15.570476
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:35:16.954456
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:35:18.354823
[batch 1020] samples: 16320, Training Loss: 0.0003
   Time since start: 0:35:19.748247
[batch 1040] samples: 16640, Training Loss: 0.0041
   Time since start: 0:35:21.061153
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:35:22.379140
[batch 1080] samples: 17280, Training Loss: 0.0079
   Time since start: 0:35:23.665134
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:35:25.341391
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:35:27.222686
[batch 1140] samples: 18240, Training Loss: 0.0001
   Time since start: 0:35:29.096460
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:35:31.044486
[batch 1180] samples: 18880, Training Loss: 0.0002
   Time since start: 0:35:32.976099
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:35:34.911198
[batch 1220] samples: 19520, Training Loss: 0.0070
   Time since start: 0:35:36.278727
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:35:37.643583
[batch 1260] samples: 20160, Training Loss: 0.0030
   Time since start: 0:35:39.460282
[batch 1280] samples: 20480, Training Loss: 0.0011
   Time since start: 0:35:41.409676
[batch 1300] samples: 20800, Training Loss: 0.0127
   Time since start: 0:35:43.384868
[batch 1320] samples: 21120, Training Loss: 0.0023
   Time since start: 0:35:45.345613
[batch 1340] samples: 21440, Training Loss: 0.0014
   Time since start: 0:35:47.199775
[batch 1360] samples: 21760, Training Loss: 0.0382
   Time since start: 0:35:48.992777
[batch 1380] samples: 22080, Training Loss: 0.0007
   Time since start: 0:35:50.697978
[batch 1400] samples: 22400, Training Loss: 0.0070
   Time since start: 0:35:52.063663
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:35:53.424594
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:35:54.791153
[batch 1460] samples: 23360, Training Loss: 0.0020
   Time since start: 0:35:56.276594
[batch 1480] samples: 23680, Training Loss: 0.0003
   Time since start: 0:35:57.937966
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:35:59.758102
[batch 1520] samples: 24320, Training Loss: 0.0003
   Time since start: 0:36:01.635070
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:36:03.084574
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:36:04.491453
[batch 1580] samples: 25280, Training Loss: 0.0008
   Time since start: 0:36:05.903714
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:36:07.578053
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:36:09.218215
[batch 1640] samples: 26240, Training Loss: 0.0115
   Time since start: 0:36:11.210621
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:36:13.082767
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:36:14.945482
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:36:16.858673
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:36:18.760272
[batch 1740] samples: 27840, Training Loss: 0.0087
   Time since start: 0:36:20.580161
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:36:21.990382
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:36:23.381079
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:36:24.807681
[batch 1820] samples: 29120, Training Loss: 0.0023
   Time since start: 0:36:26.617204
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:36:28.420412
[batch 1860] samples: 29760, Training Loss: 0.0018
   Time since start: 0:36:29.819917
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:36:31.208595
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:36:33.035409
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:36:34.641377
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:36:36.172115
[batch 1960] samples: 31360, Training Loss: 0.0014
   Time since start: 0:36:37.817406
--m-Epoch 13 done.
   Training Loss: 0.0016
   Validation Loss: 0.0015
Patience decreased: Patience is now  0
Stopping early
     precision    recall  f1-score  support  epoch  class
0     0.986976  0.999155  0.993028   5916.0      1      0
1     0.997354  0.997354  0.997354    378.0      1      1
2     1.000000  0.936170  0.967033   1128.0      1      2
3     0.985714  0.985714  0.985714    420.0      1      3
4     0.986135  0.987847  0.986990    576.0      1      4
..         ...       ...       ...      ...    ...    ...
606   1.000000  1.000000  1.000000     72.0     13     42
607   0.998587  0.997637  0.998112  32592.0     13      0
608   0.996260  0.993611  0.994890  32592.0     13      1
609   0.998608  0.997637  0.998109  32592.0     13      2
610   0.998593  0.997779  0.998034  32592.0     13      3

[611 rows x 6 columns]
device: cuda
Epoch: 1 of 30
[batch 20] samples: 320, Training Loss: 0.0968
   Time since start: 0:00:02.071253
[batch 40] samples: 640, Training Loss: 0.1021
   Time since start: 0:00:04.015282
[batch 60] samples: 960, Training Loss: 0.0531
   Time since start: 0:00:05.961112
[batch 80] samples: 1280, Training Loss: 0.0888
   Time since start: 0:00:07.914629
[batch 100] samples: 1600, Training Loss: 0.0677
   Time since start: 0:00:09.878916
[batch 120] samples: 1920, Training Loss: 0.0541
   Time since start: 0:00:11.831633
[batch 140] samples: 2240, Training Loss: 0.0538
   Time since start: 0:00:13.782269
[batch 160] samples: 2560, Training Loss: 0.0590
   Time since start: 0:00:15.746966
[batch 180] samples: 2880, Training Loss: 0.0237
   Time since start: 0:00:17.717164
[batch 200] samples: 3200, Training Loss: 0.0176
   Time since start: 0:00:19.737417
[batch 220] samples: 3520, Training Loss: 0.0288
   Time since start: 0:00:21.758332
[batch 240] samples: 3840, Training Loss: 0.0539
   Time since start: 0:00:23.781094
[batch 260] samples: 4160, Training Loss: 0.0391
   Time since start: 0:00:25.802266
[batch 280] samples: 4480, Training Loss: 0.0161
   Time since start: 0:00:27.826162
[batch 300] samples: 4800, Training Loss: 0.0218
   Time since start: 0:00:29.849755
[batch 320] samples: 5120, Training Loss: 0.0187
   Time since start: 0:00:31.873512
[batch 340] samples: 5440, Training Loss: 0.0213
   Time since start: 0:00:33.895912
[batch 360] samples: 5760, Training Loss: 0.0132
   Time since start: 0:00:35.916977
[batch 380] samples: 6080, Training Loss: 0.0094
   Time since start: 0:00:37.938704
[batch 400] samples: 6400, Training Loss: 0.0132
   Time since start: 0:00:39.963194
[batch 420] samples: 6720, Training Loss: 0.0300
   Time since start: 0:00:41.985358
[batch 440] samples: 7040, Training Loss: 0.0287
   Time since start: 0:00:44.007528
[batch 460] samples: 7360, Training Loss: 0.0216
   Time since start: 0:00:46.029801
[batch 480] samples: 7680, Training Loss: 0.0204
   Time since start: 0:00:48.052558
[batch 500] samples: 8000, Training Loss: 0.0044
   Time since start: 0:00:50.075203
[batch 520] samples: 8320, Training Loss: 0.0141
   Time since start: 0:00:52.098233
[batch 540] samples: 8640, Training Loss: 0.0074
   Time since start: 0:00:54.119134
[batch 560] samples: 8960, Training Loss: 0.0072
   Time since start: 0:00:56.140628
[batch 580] samples: 9280, Training Loss: 0.0166
   Time since start: 0:00:58.161967
[batch 600] samples: 9600, Training Loss: 0.0090
   Time since start: 0:01:00.184583
[batch 620] samples: 9920, Training Loss: 0.0048
   Time since start: 0:01:02.207733
[batch 640] samples: 10240, Training Loss: 0.0368
   Time since start: 0:01:04.231225
[batch 660] samples: 10560, Training Loss: 0.0086
   Time since start: 0:01:06.253690
[batch 680] samples: 10880, Training Loss: 0.0040
   Time since start: 0:01:08.276484
[batch 700] samples: 11200, Training Loss: 0.0046
   Time since start: 0:01:10.298080
[batch 720] samples: 11520, Training Loss: 0.0101
   Time since start: 0:01:12.319183
[batch 740] samples: 11840, Training Loss: 0.0116
   Time since start: 0:01:14.340020
[batch 760] samples: 12160, Training Loss: 0.0044
   Time since start: 0:01:16.362859
[batch 780] samples: 12480, Training Loss: 0.0128
   Time since start: 0:01:18.374999
[batch 800] samples: 12800, Training Loss: 0.0136
   Time since start: 0:01:20.398877
[batch 820] samples: 13120, Training Loss: 0.0104
   Time since start: 0:01:22.421703
[batch 840] samples: 13440, Training Loss: 0.0180
   Time since start: 0:01:24.444235
[batch 860] samples: 13760, Training Loss: 0.0056
   Time since start: 0:01:26.435746
[batch 880] samples: 14080, Training Loss: 0.0056
   Time since start: 0:01:28.459293
[batch 900] samples: 14400, Training Loss: 0.0014
   Time since start: 0:01:30.429779
[batch 920] samples: 14720, Training Loss: 0.0064
   Time since start: 0:01:32.371150
[batch 940] samples: 15040, Training Loss: 0.0039
   Time since start: 0:01:34.331474
[batch 960] samples: 15360, Training Loss: 0.0502
   Time since start: 0:01:36.290696
[batch 980] samples: 15680, Training Loss: 0.0250
   Time since start: 0:01:38.262303
[batch 1000] samples: 16000, Training Loss: 0.0189
   Time since start: 0:01:39.656875
[batch 1020] samples: 16320, Training Loss: 0.0047
   Time since start: 0:01:41.039641
[batch 1040] samples: 16640, Training Loss: 0.0012
   Time since start: 0:01:42.417892
[batch 1060] samples: 16960, Training Loss: 0.0140
   Time since start: 0:01:43.797003
[batch 1080] samples: 17280, Training Loss: 0.0014
   Time since start: 0:01:45.176230
[batch 1100] samples: 17600, Training Loss: 0.0260
   Time since start: 0:01:46.879560
[batch 1120] samples: 17920, Training Loss: 0.0020
   Time since start: 0:01:48.841568
[batch 1140] samples: 18240, Training Loss: 0.0164
   Time since start: 0:01:50.784694
[batch 1160] samples: 18560, Training Loss: 0.0116
   Time since start: 0:01:52.745410
[batch 1180] samples: 18880, Training Loss: 0.0090
   Time since start: 0:01:54.747264
[batch 1200] samples: 19200, Training Loss: 0.0029
   Time since start: 0:01:56.769049
[batch 1220] samples: 19520, Training Loss: 0.0029
   Time since start: 0:01:58.790809
[batch 1240] samples: 19840, Training Loss: 0.0008
   Time since start: 0:02:00.811258
[batch 1260] samples: 20160, Training Loss: 0.0069
   Time since start: 0:02:02.831387
[batch 1280] samples: 20480, Training Loss: 0.0059
   Time since start: 0:02:04.852805
[batch 1300] samples: 20800, Training Loss: 0.0007
   Time since start: 0:02:06.874319
[batch 1320] samples: 21120, Training Loss: 0.0056
   Time since start: 0:02:09.056676
[batch 1340] samples: 21440, Training Loss: 0.0055
   Time since start: 0:02:11.077426
[batch 1360] samples: 21760, Training Loss: 0.0017
   Time since start: 0:02:13.099090
[batch 1380] samples: 22080, Training Loss: 0.0013
   Time since start: 0:02:15.121720
[batch 1400] samples: 22400, Training Loss: 0.0018
   Time since start: 0:02:17.143625
[batch 1420] samples: 22720, Training Loss: 0.0020
   Time since start: 0:02:19.166546
[batch 1440] samples: 23040, Training Loss: 0.0011
   Time since start: 0:02:21.191838
[batch 1460] samples: 23360, Training Loss: 0.0009
   Time since start: 0:02:23.214855
[batch 1480] samples: 23680, Training Loss: 0.0012
   Time since start: 0:02:25.236851
[batch 1500] samples: 24000, Training Loss: 0.0003
   Time since start: 0:02:27.251233
[batch 1520] samples: 24320, Training Loss: 0.0009
   Time since start: 0:02:29.274333
[batch 1540] samples: 24640, Training Loss: 0.0116
   Time since start: 0:02:31.297269
[batch 1560] samples: 24960, Training Loss: 0.0100
   Time since start: 0:02:33.320393
[batch 1580] samples: 25280, Training Loss: 0.0036
   Time since start: 0:02:35.342495
[batch 1600] samples: 25600, Training Loss: 0.0010
   Time since start: 0:02:37.364992
[batch 1620] samples: 25920, Training Loss: 0.0242
   Time since start: 0:02:39.388587
[batch 1640] samples: 26240, Training Loss: 0.0204
   Time since start: 0:02:41.410702
[batch 1660] samples: 26560, Training Loss: 0.0006
   Time since start: 0:02:43.433717
[batch 1680] samples: 26880, Training Loss: 0.0052
   Time since start: 0:02:45.453472
[batch 1700] samples: 27200, Training Loss: 0.0008
   Time since start: 0:02:47.465276
[batch 1720] samples: 27520, Training Loss: 0.0022
   Time since start: 0:02:49.486654
[batch 1740] samples: 27840, Training Loss: 0.0140
   Time since start: 0:02:51.506477
[batch 1760] samples: 28160, Training Loss: 0.0010
   Time since start: 0:02:53.526807
[batch 1780] samples: 28480, Training Loss: 0.0018
   Time since start: 0:02:55.547239
[batch 1800] samples: 28800, Training Loss: 0.0028
   Time since start: 0:02:57.579004
[batch 1820] samples: 29120, Training Loss: 0.0004
   Time since start: 0:02:59.599714
[batch 1840] samples: 29440, Training Loss: 0.0044
   Time since start: 0:03:01.621339
[batch 1860] samples: 29760, Training Loss: 0.0145
   Time since start: 0:03:03.296458
[batch 1880] samples: 30080, Training Loss: 0.0014
   Time since start: 0:03:04.577493
[batch 1900] samples: 30400, Training Loss: 0.0011
   Time since start: 0:03:05.857349
[batch 1920] samples: 30720, Training Loss: 0.0008
   Time since start: 0:03:07.147080
[batch 1940] samples: 31040, Training Loss: 0.0020
   Time since start: 0:03:08.949221
[batch 1960] samples: 31360, Training Loss: 0.0006
   Time since start: 0:03:10.761755
--m-Epoch 1 done.
   Training Loss: 0.0173
   Validation Loss: 0.0043
Epoch: 2 of 30
[batch 20] samples: 320, Training Loss: 0.0023
   Time since start: 0:03:23.839746
[batch 40] samples: 640, Training Loss: 0.0007
   Time since start: 0:03:25.192576
[batch 60] samples: 960, Training Loss: 0.0008
   Time since start: 0:03:26.499782
[batch 80] samples: 1280, Training Loss: 0.0010
   Time since start: 0:03:27.805722
[batch 100] samples: 1600, Training Loss: 0.0004
   Time since start: 0:03:29.116058
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:03:30.429136
[batch 140] samples: 2240, Training Loss: 0.0004
   Time since start: 0:03:31.738163
[batch 160] samples: 2560, Training Loss: 0.0006
   Time since start: 0:03:33.359093
[batch 180] samples: 2880, Training Loss: 0.0018
   Time since start: 0:03:35.178423
[batch 200] samples: 3200, Training Loss: 0.0018
   Time since start: 0:03:36.997240
[batch 220] samples: 3520, Training Loss: 0.0002
   Time since start: 0:03:38.839994
[batch 240] samples: 3840, Training Loss: 0.0021
   Time since start: 0:03:40.758987
[batch 260] samples: 4160, Training Loss: 0.0007
   Time since start: 0:03:42.657559
[batch 280] samples: 4480, Training Loss: 0.0005
   Time since start: 0:03:44.549929
[batch 300] samples: 4800, Training Loss: 0.0046
   Time since start: 0:03:46.451454
[batch 320] samples: 5120, Training Loss: 0.0032
   Time since start: 0:03:48.348333
[batch 340] samples: 5440, Training Loss: 0.0058
   Time since start: 0:03:50.251252
[batch 360] samples: 5760, Training Loss: 0.0020
   Time since start: 0:03:52.160380
[batch 380] samples: 6080, Training Loss: 0.0002
   Time since start: 0:03:54.083088
[batch 400] samples: 6400, Training Loss: 0.0022
   Time since start: 0:03:55.982333
[batch 420] samples: 6720, Training Loss: 0.0013
   Time since start: 0:03:57.870801
[batch 440] samples: 7040, Training Loss: 0.0028
   Time since start: 0:03:59.781287
[batch 460] samples: 7360, Training Loss: 0.0168
   Time since start: 0:04:01.679798
[batch 480] samples: 7680, Training Loss: 0.0008
   Time since start: 0:04:03.579816
[batch 500] samples: 8000, Training Loss: 0.0006
   Time since start: 0:04:05.478351
[batch 520] samples: 8320, Training Loss: 0.0078
   Time since start: 0:04:07.394281
[batch 540] samples: 8640, Training Loss: 0.0244
   Time since start: 0:04:09.339072
[batch 560] samples: 8960, Training Loss: 0.0234
   Time since start: 0:04:11.251327
[batch 580] samples: 9280, Training Loss: 0.0116
   Time since start: 0:04:13.131351
[batch 600] samples: 9600, Training Loss: 0.0003
   Time since start: 0:04:15.197348
[batch 620] samples: 9920, Training Loss: 0.0032
   Time since start: 0:04:17.160387
[batch 640] samples: 10240, Training Loss: 0.0004
   Time since start: 0:04:19.123560
[batch 660] samples: 10560, Training Loss: 0.0017
   Time since start: 0:04:21.084564
[batch 680] samples: 10880, Training Loss: 0.0015
   Time since start: 0:04:23.055886
[batch 700] samples: 11200, Training Loss: 0.0005
   Time since start: 0:04:25.018108
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:04:26.992093
[batch 740] samples: 11840, Training Loss: 0.0004
   Time since start: 0:04:29.014841
[batch 760] samples: 12160, Training Loss: 0.0024
   Time since start: 0:04:31.038171
[batch 780] samples: 12480, Training Loss: 0.0002
   Time since start: 0:04:33.061321
[batch 800] samples: 12800, Training Loss: 0.0003
   Time since start: 0:04:35.085397
[batch 820] samples: 13120, Training Loss: 0.0006
   Time since start: 0:04:37.109108
[batch 840] samples: 13440, Training Loss: 0.0008
   Time since start: 0:04:39.120099
[batch 860] samples: 13760, Training Loss: 0.0024
   Time since start: 0:04:41.154505
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:04:43.178291
[batch 900] samples: 14400, Training Loss: 0.0052
   Time since start: 0:04:45.200353
[batch 920] samples: 14720, Training Loss: 0.0011
   Time since start: 0:04:47.214574
[batch 940] samples: 15040, Training Loss: 0.0011
   Time since start: 0:04:49.238131
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:04:51.264474
[batch 980] samples: 15680, Training Loss: 0.0086
   Time since start: 0:04:53.286241
[batch 1000] samples: 16000, Training Loss: 0.0003
   Time since start: 0:04:55.310209
[batch 1020] samples: 16320, Training Loss: 0.0060
   Time since start: 0:04:57.333353
[batch 1040] samples: 16640, Training Loss: 0.0007
   Time since start: 0:04:59.357446
[batch 1060] samples: 16960, Training Loss: 0.0004
   Time since start: 0:05:01.379991
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:05:03.402230
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:05:05.406417
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:05:07.428659
[batch 1140] samples: 18240, Training Loss: 0.0009
   Time since start: 0:05:09.450701
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:05:11.474361
[batch 1180] samples: 18880, Training Loss: 0.0004
   Time since start: 0:05:13.499114
[batch 1200] samples: 19200, Training Loss: 0.0089
   Time since start: 0:05:15.511654
[batch 1220] samples: 19520, Training Loss: 0.0001
   Time since start: 0:05:17.485615
[batch 1240] samples: 19840, Training Loss: 0.0002
   Time since start: 0:05:19.468517
[batch 1260] samples: 20160, Training Loss: 0.0002
   Time since start: 0:05:21.429797
[batch 1280] samples: 20480, Training Loss: 0.0151
   Time since start: 0:05:23.401995
[batch 1300] samples: 20800, Training Loss: 0.0019
   Time since start: 0:05:25.376269
[batch 1320] samples: 21120, Training Loss: 0.0017
   Time since start: 0:05:27.345868
[batch 1340] samples: 21440, Training Loss: 0.0027
   Time since start: 0:05:29.348758
[batch 1360] samples: 21760, Training Loss: 0.0011
   Time since start: 0:05:31.372228
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:05:33.393438
[batch 1400] samples: 22400, Training Loss: 0.0009
   Time since start: 0:05:35.411982
[batch 1420] samples: 22720, Training Loss: 0.0014
   Time since start: 0:05:37.369895
[batch 1440] samples: 23040, Training Loss: 0.0002
   Time since start: 0:05:39.339968
[batch 1460] samples: 23360, Training Loss: 0.0003
   Time since start: 0:05:41.292254
[batch 1480] samples: 23680, Training Loss: 0.0002
   Time since start: 0:05:43.263709
[batch 1500] samples: 24000, Training Loss: 0.0250
   Time since start: 0:05:45.226164
[batch 1520] samples: 24320, Training Loss: 0.0102
   Time since start: 0:05:47.206663
[batch 1540] samples: 24640, Training Loss: 0.0156
   Time since start: 0:05:49.178096
[batch 1560] samples: 24960, Training Loss: 0.0007
   Time since start: 0:05:51.051568
[batch 1580] samples: 25280, Training Loss: 0.0020
   Time since start: 0:05:52.418458
[batch 1600] samples: 25600, Training Loss: 0.0002
   Time since start: 0:05:53.780852
[batch 1620] samples: 25920, Training Loss: 0.0005
   Time since start: 0:05:55.303537
[batch 1640] samples: 26240, Training Loss: 0.0068
   Time since start: 0:05:57.130527
[batch 1660] samples: 26560, Training Loss: 0.0001
   Time since start: 0:05:58.973227
[batch 1680] samples: 26880, Training Loss: 0.0011
   Time since start: 0:06:00.369395
[batch 1700] samples: 27200, Training Loss: 0.0009
   Time since start: 0:06:01.700311
[batch 1720] samples: 27520, Training Loss: 0.0013
   Time since start: 0:06:03.029684
[batch 1740] samples: 27840, Training Loss: 0.0012
   Time since start: 0:06:04.364215
[batch 1760] samples: 28160, Training Loss: 0.0003
   Time since start: 0:06:05.696575
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:06:07.029906
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:06:08.827234
[batch 1820] samples: 29120, Training Loss: 0.0031
   Time since start: 0:06:10.672323
[batch 1840] samples: 29440, Training Loss: 0.0030
   Time since start: 0:06:12.623565
[batch 1860] samples: 29760, Training Loss: 0.0053
   Time since start: 0:06:14.466426
[batch 1880] samples: 30080, Training Loss: 0.0004
   Time since start: 0:06:16.320273
[batch 1900] samples: 30400, Training Loss: 0.0012
   Time since start: 0:06:18.165437
[batch 1920] samples: 30720, Training Loss: 0.0033
   Time since start: 0:06:20.007672
[batch 1940] samples: 31040, Training Loss: 0.0191
   Time since start: 0:06:21.828555
[batch 1960] samples: 31360, Training Loss: 0.0019
   Time since start: 0:06:23.668714
--m-Epoch 2 done.
   Training Loss: 0.0034
   Validation Loss: 0.0026
Epoch: 3 of 30
[batch 20] samples: 320, Training Loss: 0.0012
   Time since start: 0:06:37.722289
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:06:39.723874
[batch 60] samples: 960, Training Loss: 0.0001
   Time since start: 0:06:41.221961
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:06:42.784875
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:06:44.245871
[batch 120] samples: 1920, Training Loss: 0.0002
   Time since start: 0:06:46.102476
[batch 140] samples: 2240, Training Loss: 0.0004
   Time since start: 0:06:48.012554
[batch 160] samples: 2560, Training Loss: 0.0064
   Time since start: 0:06:49.912961
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:06:51.822803
[batch 200] samples: 3200, Training Loss: 0.0002
   Time since start: 0:06:53.733343
[batch 220] samples: 3520, Training Loss: 0.0039
   Time since start: 0:06:55.653824
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:06:57.495060
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:06:58.830760
[batch 280] samples: 4480, Training Loss: 0.0014
   Time since start: 0:07:00.256850
[batch 300] samples: 4800, Training Loss: 0.0032
   Time since start: 0:07:01.913545
[batch 320] samples: 5120, Training Loss: 0.0011
   Time since start: 0:07:03.673918
[batch 340] samples: 5440, Training Loss: 0.0010
   Time since start: 0:07:05.489510
[batch 360] samples: 5760, Training Loss: 0.0031
   Time since start: 0:07:07.308639
[batch 380] samples: 6080, Training Loss: 0.0109
   Time since start: 0:07:09.139771
[batch 400] samples: 6400, Training Loss: 0.0005
   Time since start: 0:07:10.953485
[batch 420] samples: 6720, Training Loss: 0.0007
   Time since start: 0:07:12.769210
[batch 440] samples: 7040, Training Loss: 0.0050
   Time since start: 0:07:14.600833
[batch 460] samples: 7360, Training Loss: 0.0057
   Time since start: 0:07:16.443186
[batch 480] samples: 7680, Training Loss: 0.0008
   Time since start: 0:07:18.256971
[batch 500] samples: 8000, Training Loss: 0.0035
   Time since start: 0:07:20.078833
[batch 520] samples: 8320, Training Loss: 0.0004
   Time since start: 0:07:21.897897
[batch 540] samples: 8640, Training Loss: 0.0034
   Time since start: 0:07:23.716162
[batch 560] samples: 8960, Training Loss: 0.0073
   Time since start: 0:07:25.536962
[batch 580] samples: 9280, Training Loss: 0.0214
   Time since start: 0:07:27.357047
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:07:29.178415
[batch 620] samples: 9920, Training Loss: 0.0010
   Time since start: 0:07:30.996800
[batch 640] samples: 10240, Training Loss: 0.0006
   Time since start: 0:07:32.815609
[batch 660] samples: 10560, Training Loss: 0.0039
   Time since start: 0:07:34.617184
[batch 680] samples: 10880, Training Loss: 0.0002
   Time since start: 0:07:36.509823
[batch 700] samples: 11200, Training Loss: 0.0003
   Time since start: 0:07:38.380274
[batch 720] samples: 11520, Training Loss: 0.0004
   Time since start: 0:07:40.300691
[batch 740] samples: 11840, Training Loss: 0.0019
   Time since start: 0:07:42.201103
[batch 760] samples: 12160, Training Loss: 0.0030
   Time since start: 0:07:44.071106
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:07:45.940763
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:07:47.416339
[batch 820] samples: 13120, Training Loss: 0.0024
   Time since start: 0:07:48.758545
[batch 840] samples: 13440, Training Loss: 0.0022
   Time since start: 0:07:50.136338
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:07:51.532783
[batch 880] samples: 14080, Training Loss: 0.0006
   Time since start: 0:07:52.894141
[batch 900] samples: 14400, Training Loss: 0.0020
   Time since start: 0:07:54.252207
[batch 920] samples: 14720, Training Loss: 0.0026
   Time since start: 0:07:55.614712
[batch 940] samples: 15040, Training Loss: 0.0057
   Time since start: 0:07:56.976258
[batch 960] samples: 15360, Training Loss: 0.0002
   Time since start: 0:07:58.337965
[batch 980] samples: 15680, Training Loss: 0.0002
   Time since start: 0:08:00.104445
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:08:01.984769
[batch 1020] samples: 16320, Training Loss: 0.0104
   Time since start: 0:08:03.890399
[batch 1040] samples: 16640, Training Loss: 0.0015
   Time since start: 0:08:05.784201
[batch 1060] samples: 16960, Training Loss: 0.0007
   Time since start: 0:08:07.680482
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:08:09.579724
[batch 1100] samples: 17600, Training Loss: 0.0006
   Time since start: 0:08:11.569450
[batch 1120] samples: 17920, Training Loss: 0.0024
   Time since start: 0:08:13.450214
[batch 1140] samples: 18240, Training Loss: 0.0015
   Time since start: 0:08:15.313488
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:08:17.199793
[batch 1180] samples: 18880, Training Loss: 0.0033
   Time since start: 0:08:19.080626
[batch 1200] samples: 19200, Training Loss: 0.0060
   Time since start: 0:08:20.958266
[batch 1220] samples: 19520, Training Loss: 0.0107
   Time since start: 0:08:22.735150
[batch 1240] samples: 19840, Training Loss: 0.0100
   Time since start: 0:08:24.093231
[batch 1260] samples: 20160, Training Loss: 0.0017
   Time since start: 0:08:25.455482
[batch 1280] samples: 20480, Training Loss: 0.0045
   Time since start: 0:08:27.201819
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:08:29.106690
[batch 1320] samples: 21120, Training Loss: 0.0010
   Time since start: 0:08:30.939450
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:08:32.330657
[batch 1360] samples: 21760, Training Loss: 0.0004
   Time since start: 0:08:34.175705
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:08:35.996193
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:08:37.812383
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:08:39.628682
[batch 1440] samples: 23040, Training Loss: 0.0006
   Time since start: 0:08:41.105594
[batch 1460] samples: 23360, Training Loss: 0.0008
   Time since start: 0:08:42.487125
[batch 1480] samples: 23680, Training Loss: 0.0034
   Time since start: 0:08:43.855955
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:08:45.227007
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:08:46.597996
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:08:48.340846
[batch 1560] samples: 24960, Training Loss: 0.0036
   Time since start: 0:08:50.184875
[batch 1580] samples: 25280, Training Loss: 0.0025
   Time since start: 0:08:52.005194
[batch 1600] samples: 25600, Training Loss: 0.0191
   Time since start: 0:08:53.844821
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:08:55.638290
[batch 1640] samples: 26240, Training Loss: 0.0054
   Time since start: 0:08:57.468010
[batch 1660] samples: 26560, Training Loss: 0.0002
   Time since start: 0:08:59.292357
[batch 1680] samples: 26880, Training Loss: 0.0062
   Time since start: 0:09:00.893243
[batch 1700] samples: 27200, Training Loss: 0.0006
   Time since start: 0:09:02.211044
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:09:03.527806
[batch 1740] samples: 27840, Training Loss: 0.0007
   Time since start: 0:09:04.847104
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:09:06.165363
[batch 1780] samples: 28480, Training Loss: 0.0006
   Time since start: 0:09:07.501541
[batch 1800] samples: 28800, Training Loss: 0.0004
   Time since start: 0:09:08.825314
[batch 1820] samples: 29120, Training Loss: 0.0100
   Time since start: 0:09:10.152298
[batch 1840] samples: 29440, Training Loss: 0.0008
   Time since start: 0:09:11.468035
[batch 1860] samples: 29760, Training Loss: 0.0006
   Time since start: 0:09:12.785011
[batch 1880] samples: 30080, Training Loss: 0.0005
   Time since start: 0:09:14.112442
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:09:15.429349
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:09:16.745666
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:09:18.061134
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:09:19.346918
--m-Epoch 3 done.
   Training Loss: 0.0026
   Validation Loss: 0.0004
Epoch: 4 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:09:33.554548
[batch 40] samples: 640, Training Loss: 0.0017
   Time since start: 0:09:35.576787
[batch 60] samples: 960, Training Loss: 0.0002
   Time since start: 0:09:37.599007
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:09:39.622949
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:09:41.644578
[batch 120] samples: 1920, Training Loss: 0.0016
   Time since start: 0:09:43.656934
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:09:45.677866
[batch 160] samples: 2560, Training Loss: 0.0345
   Time since start: 0:09:47.689667
[batch 180] samples: 2880, Training Loss: 0.0008
   Time since start: 0:09:49.713101
[batch 200] samples: 3200, Training Loss: 0.0051
   Time since start: 0:09:51.736193
[batch 220] samples: 3520, Training Loss: 0.0002
   Time since start: 0:09:53.749025
[batch 240] samples: 3840, Training Loss: 0.0019
   Time since start: 0:09:55.769783
[batch 260] samples: 4160, Training Loss: 0.0003
   Time since start: 0:09:57.793673
[batch 280] samples: 4480, Training Loss: 0.0010
   Time since start: 0:09:59.463347
[batch 300] samples: 4800, Training Loss: 0.0005
   Time since start: 0:10:01.477152
[batch 320] samples: 5120, Training Loss: 0.0037
   Time since start: 0:10:03.490236
[batch 340] samples: 5440, Training Loss: 0.0059
   Time since start: 0:10:05.502089
[batch 360] samples: 5760, Training Loss: 0.0023
   Time since start: 0:10:07.513939
[batch 380] samples: 6080, Training Loss: 0.0105
   Time since start: 0:10:09.657492
[batch 400] samples: 6400, Training Loss: 0.0002
   Time since start: 0:10:11.670604
[batch 420] samples: 6720, Training Loss: 0.0002
   Time since start: 0:10:13.675749
[batch 440] samples: 7040, Training Loss: 0.0113
   Time since start: 0:10:15.636895
[batch 460] samples: 7360, Training Loss: 0.0002
   Time since start: 0:10:17.577899
[batch 480] samples: 7680, Training Loss: 0.0009
   Time since start: 0:10:19.557594
[batch 500] samples: 8000, Training Loss: 0.0057
   Time since start: 0:10:21.526076
[batch 520] samples: 8320, Training Loss: 0.0050
   Time since start: 0:10:23.505945
[batch 540] samples: 8640, Training Loss: 0.0090
   Time since start: 0:10:25.478335
[batch 560] samples: 8960, Training Loss: 0.0047
   Time since start: 0:10:27.443496
[batch 580] samples: 9280, Training Loss: 0.0046
   Time since start: 0:10:29.411883
[batch 600] samples: 9600, Training Loss: 0.0004
   Time since start: 0:10:31.382031
[batch 620] samples: 9920, Training Loss: 0.0029
   Time since start: 0:10:33.348190
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:10:35.310372
[batch 660] samples: 10560, Training Loss: 0.0017
   Time since start: 0:10:37.277223
[batch 680] samples: 10880, Training Loss: 0.0048
   Time since start: 0:10:39.236433
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:10:41.207815
[batch 720] samples: 11520, Training Loss: 0.0068
   Time since start: 0:10:43.034459
[batch 740] samples: 11840, Training Loss: 0.0052
   Time since start: 0:10:44.399393
[batch 760] samples: 12160, Training Loss: 0.0006
   Time since start: 0:10:45.762823
[batch 780] samples: 12480, Training Loss: 0.0004
   Time since start: 0:10:47.128939
[batch 800] samples: 12800, Training Loss: 0.0029
   Time since start: 0:10:48.493490
[batch 820] samples: 13120, Training Loss: 0.0025
   Time since start: 0:10:49.869138
[batch 840] samples: 13440, Training Loss: 0.0008
   Time since start: 0:10:51.705298
[batch 860] samples: 13760, Training Loss: 0.0004
   Time since start: 0:10:53.583697
[batch 880] samples: 14080, Training Loss: 0.0008
   Time since start: 0:10:55.497547
[batch 900] samples: 14400, Training Loss: 0.0004
   Time since start: 0:10:57.469122
[batch 920] samples: 14720, Training Loss: 0.0003
   Time since start: 0:10:59.440334
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:11:01.419456
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:11:03.392225
[batch 980] samples: 15680, Training Loss: 0.0015
   Time since start: 0:11:05.360953
[batch 1000] samples: 16000, Training Loss: 0.0002
   Time since start: 0:11:07.312551
[batch 1020] samples: 16320, Training Loss: 0.0117
   Time since start: 0:11:09.274899
[batch 1040] samples: 16640, Training Loss: 0.0007
   Time since start: 0:11:11.246881
[batch 1060] samples: 16960, Training Loss: 0.0003
   Time since start: 0:11:13.216139
[batch 1080] samples: 17280, Training Loss: 0.0034
   Time since start: 0:11:15.176867
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:11:17.147680
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:11:19.106440
[batch 1140] samples: 18240, Training Loss: 0.0003
   Time since start: 0:11:21.068007
[batch 1160] samples: 18560, Training Loss: 0.0031
   Time since start: 0:11:23.037134
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:11:25.004555
[batch 1200] samples: 19200, Training Loss: 0.0002
   Time since start: 0:11:26.988469
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:11:28.960689
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:11:30.960544
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:11:32.925272
[batch 1280] samples: 20480, Training Loss: 0.0017
   Time since start: 0:11:34.882811
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:11:36.853138
[batch 1320] samples: 21120, Training Loss: 0.0006
   Time since start: 0:11:38.824310
[batch 1340] samples: 21440, Training Loss: 0.0277
   Time since start: 0:11:40.784862
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:11:42.745558
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:11:44.728666
[batch 1400] samples: 22400, Training Loss: 0.0005
   Time since start: 0:11:46.636480
[batch 1420] samples: 22720, Training Loss: 0.0009
   Time since start: 0:11:48.186329
[batch 1440] samples: 23040, Training Loss: 0.0049
   Time since start: 0:11:50.031679
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:11:51.944432
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:11:53.832638
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:11:55.725198
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:11:57.640508
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:11:59.544528
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:12:01.441132
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:12:03.346374
[batch 1600] samples: 25600, Training Loss: 0.0002
   Time since start: 0:12:05.234196
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:12:07.086466
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:12:08.825746
[batch 1660] samples: 26560, Training Loss: 0.0003
   Time since start: 0:12:10.577427
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:12:12.325540
[batch 1700] samples: 27200, Training Loss: 0.0039
   Time since start: 0:12:14.053115
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:12:15.783320
[batch 1740] samples: 27840, Training Loss: 0.0016
   Time since start: 0:12:17.510607
[batch 1760] samples: 28160, Training Loss: 0.0006
   Time since start: 0:12:19.266253
[batch 1780] samples: 28480, Training Loss: 0.0008
   Time since start: 0:12:21.007899
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:12:22.721211
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:12:24.496132
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:12:26.271426
[batch 1860] samples: 29760, Training Loss: 0.0003
   Time since start: 0:12:28.010170
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:12:29.794833
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:12:31.543440
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:12:33.291414
[batch 1940] samples: 31040, Training Loss: 0.0017
   Time since start: 0:12:35.079662
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:12:36.911328
--m-Epoch 4 done.
   Training Loss: 0.0022
   Validation Loss: 0.0094
Patience decreased: Patience is now  4
Epoch: 5 of 30
[batch 20] samples: 320, Training Loss: 0.0003
   Time since start: 0:12:50.217372
[batch 40] samples: 640, Training Loss: 0.0004
   Time since start: 0:12:52.105119
[batch 60] samples: 960, Training Loss: 0.0001
   Time since start: 0:12:54.031407
[batch 80] samples: 1280, Training Loss: 0.0086
   Time since start: 0:12:55.901257
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:12:57.780385
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:12:59.642476
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:13:01.513152
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:13:03.404338
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:13:05.272884
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:13:07.135029
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:13:08.975611
[batch 240] samples: 3840, Training Loss: 0.0042
   Time since start: 0:13:10.792334
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:13:12.605245
[batch 280] samples: 4480, Training Loss: 0.0002
   Time since start: 0:13:14.416219
[batch 300] samples: 4800, Training Loss: 0.0002
   Time since start: 0:13:15.911108
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:13:17.232541
[batch 340] samples: 5440, Training Loss: 0.0007
   Time since start: 0:13:18.545358
[batch 360] samples: 5760, Training Loss: 0.0130
   Time since start: 0:13:19.859898
[batch 380] samples: 6080, Training Loss: 0.0005
   Time since start: 0:13:21.187954
[batch 400] samples: 6400, Training Loss: 0.0018
   Time since start: 0:13:22.548981
[batch 420] samples: 6720, Training Loss: 0.0058
   Time since start: 0:13:24.190887
[batch 440] samples: 7040, Training Loss: 0.0002
   Time since start: 0:13:25.937123
[batch 460] samples: 7360, Training Loss: 0.0058
   Time since start: 0:13:27.355968
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:13:29.328973
[batch 500] samples: 8000, Training Loss: 0.0058
   Time since start: 0:13:31.299081
[batch 520] samples: 8320, Training Loss: 0.0012
   Time since start: 0:13:33.273199
[batch 540] samples: 8640, Training Loss: 0.0009
   Time since start: 0:13:35.243423
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:13:37.214734
[batch 580] samples: 9280, Training Loss: 0.0003
   Time since start: 0:13:39.207642
[batch 600] samples: 9600, Training Loss: 0.0009
   Time since start: 0:13:41.052297
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:13:42.910838
[batch 640] samples: 10240, Training Loss: 0.0093
   Time since start: 0:13:44.756646
[batch 660] samples: 10560, Training Loss: 0.0085
   Time since start: 0:13:46.581895
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:13:48.445887
[batch 700] samples: 11200, Training Loss: 0.0004
   Time since start: 0:13:50.336849
[batch 720] samples: 11520, Training Loss: 0.0046
   Time since start: 0:13:52.129970
[batch 740] samples: 11840, Training Loss: 0.0022
   Time since start: 0:13:53.483292
[batch 760] samples: 12160, Training Loss: 0.0001
   Time since start: 0:13:54.840695
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:13:56.200085
[batch 800] samples: 12800, Training Loss: 0.0004
   Time since start: 0:13:57.561670
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:13:58.920695
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:14:00.279935
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:14:01.643208
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:14:03.130618
[batch 900] samples: 14400, Training Loss: 0.0002
   Time since start: 0:14:04.492514
[batch 920] samples: 14720, Training Loss: 0.0009
   Time since start: 0:14:05.856336
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:14:07.508714
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:14:09.482105
[batch 980] samples: 15680, Training Loss: 0.0074
   Time since start: 0:14:11.402450
[batch 1000] samples: 16000, Training Loss: 0.0055
   Time since start: 0:14:12.765273
[batch 1020] samples: 16320, Training Loss: 0.0005
   Time since start: 0:14:14.122811
[batch 1040] samples: 16640, Training Loss: 0.0006
   Time since start: 0:14:15.518654
[batch 1060] samples: 16960, Training Loss: 0.0197
   Time since start: 0:14:16.912288
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:14:18.305630
[batch 1100] samples: 17600, Training Loss: 0.0003
   Time since start: 0:14:19.704230
[batch 1120] samples: 17920, Training Loss: 0.0054
   Time since start: 0:14:21.099398
[batch 1140] samples: 18240, Training Loss: 0.0017
   Time since start: 0:14:22.492980
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:14:23.861761
[batch 1180] samples: 18880, Training Loss: 0.0018
   Time since start: 0:14:25.219895
[batch 1200] samples: 19200, Training Loss: 0.0024
   Time since start: 0:14:26.576974
[batch 1220] samples: 19520, Training Loss: 0.0008
   Time since start: 0:14:27.934684
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:14:29.510897
[batch 1260] samples: 20160, Training Loss: 0.0002
   Time since start: 0:14:31.462894
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:14:33.424760
[batch 1300] samples: 20800, Training Loss: 0.0042
   Time since start: 0:14:35.394443
[batch 1320] samples: 21120, Training Loss: 0.0029
   Time since start: 0:14:37.375629
[batch 1340] samples: 21440, Training Loss: 0.0022
   Time since start: 0:14:39.347218
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:14:41.320089
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:14:43.289237
[batch 1400] samples: 22400, Training Loss: 0.0381
   Time since start: 0:14:45.261187
[batch 1420] samples: 22720, Training Loss: 0.0004
   Time since start: 0:14:47.234912
[batch 1440] samples: 23040, Training Loss: 0.0006
   Time since start: 0:14:49.206115
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:14:51.178584
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:14:53.151994
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:14:55.103752
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:14:57.065740
[batch 1540] samples: 24640, Training Loss: 0.0065
   Time since start: 0:14:59.026209
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:15:01.008260
[batch 1580] samples: 25280, Training Loss: 0.0004
   Time since start: 0:15:02.978935
[batch 1600] samples: 25600, Training Loss: 0.0019
   Time since start: 0:15:04.951802
[batch 1620] samples: 25920, Training Loss: 0.0006
   Time since start: 0:15:06.914818
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:15:08.764789
[batch 1660] samples: 26560, Training Loss: 0.0014
   Time since start: 0:15:10.162706
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:15:11.556347
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:15:12.956830
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:15:14.352847
[batch 1740] samples: 27840, Training Loss: 0.0026
   Time since start: 0:15:15.747594
[batch 1760] samples: 28160, Training Loss: 0.0007
   Time since start: 0:15:17.141980
[batch 1780] samples: 28480, Training Loss: 0.0054
   Time since start: 0:15:18.526979
[batch 1800] samples: 28800, Training Loss: 0.0071
   Time since start: 0:15:19.885242
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:15:21.244459
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:15:22.614205
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:15:23.977026
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:15:25.336059
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:15:26.706774
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:15:28.104155
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:15:29.498799
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:15:30.829739
--m-Epoch 5 done.
   Training Loss: 0.0017
   Validation Loss: 0.0011
Patience decreased: Patience is now  3
Epoch: 6 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:15:44.259097
[batch 40] samples: 640, Training Loss: 0.0005
   Time since start: 0:15:46.075858
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:15:47.763300
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:15:49.064281
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:15:50.365024
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:15:51.666846
[batch 140] samples: 2240, Training Loss: 0.0051
   Time since start: 0:15:53.386109
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:15:55.108556
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:15:56.821084
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:15:58.537724
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:16:00.269073
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:16:02.006927
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:16:03.830995
[batch 280] samples: 4480, Training Loss: 0.0307
   Time since start: 0:16:05.638237
[batch 300] samples: 4800, Training Loss: 0.0097
   Time since start: 0:16:07.374525
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:16:08.697826
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:16:10.024082
[batch 360] samples: 5760, Training Loss: 0.0010
   Time since start: 0:16:11.458959
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:16:13.328415
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:16:15.208764
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:16:17.110399
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:16:19.031581
[batch 460] samples: 7360, Training Loss: 0.0002
   Time since start: 0:16:20.932448
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:16:22.820117
[batch 500] samples: 8000, Training Loss: 0.0036
   Time since start: 0:16:24.713757
[batch 520] samples: 8320, Training Loss: 0.0005
   Time since start: 0:16:26.603721
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:16:28.523906
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:16:30.229752
[batch 580] samples: 9280, Training Loss: 0.0004
   Time since start: 0:16:31.588777
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:16:32.946601
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:16:34.305322
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:16:35.659430
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:16:37.015619
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:16:38.376914
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:16:39.746730
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:16:41.107767
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:16:42.460295
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:16:43.814281
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:16:45.167851
[batch 800] samples: 12800, Training Loss: 0.0317
   Time since start: 0:16:46.519141
[batch 820] samples: 13120, Training Loss: 0.0024
   Time since start: 0:16:47.873931
[batch 840] samples: 13440, Training Loss: 0.0006
   Time since start: 0:16:49.231566
[batch 860] samples: 13760, Training Loss: 0.0014
   Time since start: 0:16:50.583781
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:16:51.938754
[batch 900] samples: 14400, Training Loss: 0.0005
   Time since start: 0:16:53.337876
[batch 920] samples: 14720, Training Loss: 0.0050
   Time since start: 0:16:55.237383
[batch 940] samples: 15040, Training Loss: 0.0003
   Time since start: 0:16:57.147280
[batch 960] samples: 15360, Training Loss: 0.0009
   Time since start: 0:16:59.118710
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:17:01.080867
[batch 1000] samples: 16000, Training Loss: 0.0014
   Time since start: 0:17:03.050642
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:17:05.012074
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:17:06.973108
[batch 1060] samples: 16960, Training Loss: 0.0027
   Time since start: 0:17:08.944842
[batch 1080] samples: 17280, Training Loss: 0.0002
   Time since start: 0:17:10.904667
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:17:12.296722
[batch 1120] samples: 17920, Training Loss: 0.0003
   Time since start: 0:17:13.685359
[batch 1140] samples: 18240, Training Loss: 0.0002
   Time since start: 0:17:15.184460
[batch 1160] samples: 18560, Training Loss: 0.0007
   Time since start: 0:17:17.001419
[batch 1180] samples: 18880, Training Loss: 0.0088
   Time since start: 0:17:18.819896
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:17:20.621691
[batch 1220] samples: 19520, Training Loss: 0.0001
   Time since start: 0:17:22.410601
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:17:24.201338
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:17:25.998420
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:17:27.798327
[batch 1300] samples: 20800, Training Loss: 0.0083
   Time since start: 0:17:29.610035
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:17:31.421946
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:17:33.229569
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:17:35.029193
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:17:36.955530
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:17:38.752236
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:17:40.560292
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:17:42.381352
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:17:44.205093
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:17:46.004552
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:17:47.614109
[batch 1520] samples: 24320, Training Loss: 0.0014
   Time since start: 0:17:48.944814
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:17:50.272949
[batch 1560] samples: 24960, Training Loss: 0.0050
   Time since start: 0:17:51.853183
[batch 1580] samples: 25280, Training Loss: 0.0021
   Time since start: 0:17:53.825102
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:17:55.799716
[batch 1620] samples: 25920, Training Loss: 0.0003
   Time since start: 0:17:57.759013
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:17:59.719830
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:18:01.711048
[batch 1680] samples: 26880, Training Loss: 0.0006
   Time since start: 0:18:03.654117
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:18:05.563967
[batch 1720] samples: 27520, Training Loss: 0.0003
   Time since start: 0:18:07.475832
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:18:09.374906
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:18:11.298430
[batch 1780] samples: 28480, Training Loss: 0.0002
   Time since start: 0:18:13.268857
[batch 1800] samples: 28800, Training Loss: 0.0029
   Time since start: 0:18:15.229726
[batch 1820] samples: 29120, Training Loss: 0.0010
   Time since start: 0:18:17.191009
[batch 1840] samples: 29440, Training Loss: 0.0116
   Time since start: 0:18:19.163485
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:18:21.122945
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:18:23.083997
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:18:25.055350
[batch 1920] samples: 30720, Training Loss: 0.0028
   Time since start: 0:18:27.015346
[batch 1940] samples: 31040, Training Loss: 0.0004
   Time since start: 0:18:28.989556
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:18:30.919628
--m-Epoch 6 done.
   Training Loss: 0.0015
   Validation Loss: 0.0010
Patience decreased: Patience is now  2
Epoch: 7 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:18:44.273199
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:18:45.607748
[batch 60] samples: 960, Training Loss: 0.0082
   Time since start: 0:18:46.936171
[batch 80] samples: 1280, Training Loss: 0.0002
   Time since start: 0:18:48.264369
[batch 100] samples: 1600, Training Loss: 0.0004
   Time since start: 0:18:49.588382
[batch 120] samples: 1920, Training Loss: 0.0035
   Time since start: 0:18:50.912260
[batch 140] samples: 2240, Training Loss: 0.0006
   Time since start: 0:18:52.247094
[batch 160] samples: 2560, Training Loss: 0.0013
   Time since start: 0:18:53.572414
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:18:54.895972
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:18:56.219516
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:18:57.541677
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:18:58.870507
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:19:00.195276
[batch 280] samples: 4480, Training Loss: 0.0002
   Time since start: 0:19:01.518449
[batch 300] samples: 4800, Training Loss: 0.0015
   Time since start: 0:19:02.841706
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:19:04.167168
[batch 340] samples: 5440, Training Loss: 0.0057
   Time since start: 0:19:05.857743
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:19:07.713778
[batch 380] samples: 6080, Training Loss: 0.0003
   Time since start: 0:19:09.574939
[batch 400] samples: 6400, Training Loss: 0.0056
   Time since start: 0:19:11.455616
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:19:13.272102
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:19:15.028482
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:19:16.823129
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:19:18.629283
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:19:20.443352
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:19:22.261406
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:19:24.080354
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:19:25.407205
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:19:26.728791
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:19:28.049810
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:19:29.374611
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:19:30.825767
[batch 660] samples: 10560, Training Loss: 0.0003
   Time since start: 0:19:32.149350
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:19:33.470014
[batch 700] samples: 11200, Training Loss: 0.0018
   Time since start: 0:19:34.795563
[batch 720] samples: 11520, Training Loss: 0.0010
   Time since start: 0:19:36.119621
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:19:37.441555
[batch 760] samples: 12160, Training Loss: 0.0008
   Time since start: 0:19:38.764520
[batch 780] samples: 12480, Training Loss: 0.0037
   Time since start: 0:19:40.091866
[batch 800] samples: 12800, Training Loss: 0.0008
   Time since start: 0:19:41.385264
[batch 820] samples: 13120, Training Loss: 0.0108
   Time since start: 0:19:42.735093
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:19:44.092845
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:19:45.417757
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:19:46.736226
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:19:48.058615
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:19:49.398912
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:19:50.753840
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:19:52.111012
[batch 980] samples: 15680, Training Loss: 0.0004
   Time since start: 0:19:53.468824
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:19:55.227773
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:19:56.948827
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:19:58.342525
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:19:59.787406
[batch 1080] samples: 17280, Training Loss: 0.0004
   Time since start: 0:20:01.134893
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:20:02.485896
[batch 1120] samples: 17920, Training Loss: 0.0003
   Time since start: 0:20:03.834715
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:20:05.184052
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:20:06.531916
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:20:07.881884
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:20:09.429549
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:20:11.248407
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:20:13.056076
[batch 1260] samples: 20160, Training Loss: 0.0036
   Time since start: 0:20:14.613825
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:20:16.208579
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:20:18.008150
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:20:19.818829
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:20:21.618319
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:20:23.439752
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:20:24.963882
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:20:26.283103
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:20:27.602794
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:20:28.923706
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:20:30.246036
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:20:31.566199
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:20:32.886793
[batch 1520] samples: 24320, Training Loss: 0.0002
   Time since start: 0:20:34.209924
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:20:35.532396
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:20:36.851843
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:20:38.172721
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:20:39.505317
[batch 1620] samples: 25920, Training Loss: 0.0011
   Time since start: 0:20:40.845818
[batch 1640] samples: 26240, Training Loss: 0.0057
   Time since start: 0:20:42.170402
[batch 1660] samples: 26560, Training Loss: 0.0005
   Time since start: 0:20:43.495615
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:20:44.826065
[batch 1700] samples: 27200, Training Loss: 0.0064
   Time since start: 0:20:46.151861
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:20:47.482088
[batch 1740] samples: 27840, Training Loss: 0.0007
   Time since start: 0:20:48.810364
[batch 1760] samples: 28160, Training Loss: 0.0020
   Time since start: 0:20:50.152681
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:20:51.521959
[batch 1800] samples: 28800, Training Loss: 0.0006
   Time since start: 0:20:52.851872
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:20:54.178989
[batch 1840] samples: 29440, Training Loss: 0.0013
   Time since start: 0:20:55.510015
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:20:57.059212
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:20:58.555074
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:21:00.060292
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:21:01.872017
[batch 1940] samples: 31040, Training Loss: 0.0003
   Time since start: 0:21:03.624316
[batch 1960] samples: 31360, Training Loss: 0.0005
   Time since start: 0:21:05.418719
--m-Epoch 7 done.
   Training Loss: 0.0013
   Validation Loss: 0.0014
Patience decreased: Patience is now  1
Epoch: 8 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:21:19.253629
[batch 40] samples: 640, Training Loss: 0.0003
   Time since start: 0:21:20.610648
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:21:21.965170
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:21:23.793592
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:21:25.660510
[batch 120] samples: 1920, Training Loss: 0.0002
   Time since start: 0:21:27.346660
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:21:29.198383
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:21:31.099998
[batch 180] samples: 2880, Training Loss: 0.0034
   Time since start: 0:21:32.670787
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:21:34.344874
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:21:36.189525
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:21:37.571375
[batch 260] samples: 4160, Training Loss: 0.0010
   Time since start: 0:21:38.955072
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:21:40.350371
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:21:42.155321
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:21:43.977030
[batch 340] samples: 5440, Training Loss: 0.0004
   Time since start: 0:21:45.790317
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:21:47.600048
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:21:49.392705
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:21:51.197095
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:21:53.006864
[batch 440] samples: 7040, Training Loss: 0.0010
   Time since start: 0:21:54.805846
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:21:56.627538
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:21:58.437311
[batch 500] samples: 8000, Training Loss: 0.0036
   Time since start: 0:22:00.246690
[batch 520] samples: 8320, Training Loss: 0.0002
   Time since start: 0:22:02.066008
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:22:03.825215
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:22:05.573707
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:22:07.300101
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:22:09.048762
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:22:10.870197
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:22:12.800323
[batch 660] samples: 10560, Training Loss: 0.0002
   Time since start: 0:22:14.741118
[batch 680] samples: 10880, Training Loss: 0.0006
   Time since start: 0:22:16.682436
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:22:18.623616
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:22:20.574486
[batch 740] samples: 11840, Training Loss: 0.0074
   Time since start: 0:22:22.535276
[batch 760] samples: 12160, Training Loss: 0.0004
   Time since start: 0:22:24.496666
[batch 780] samples: 12480, Training Loss: 0.0010
   Time since start: 0:22:26.437566
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:22:28.377823
[batch 820] samples: 13120, Training Loss: 0.0004
   Time since start: 0:22:30.288238
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:22:31.769016
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:22:33.662262
[batch 880] samples: 14080, Training Loss: 0.0003
   Time since start: 0:22:35.544740
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:22:37.431767
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:22:39.334348
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:22:40.736356
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:22:42.015945
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:22:43.353269
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:22:44.634368
[batch 1020] samples: 16320, Training Loss: 0.0011
   Time since start: 0:22:46.348242
[batch 1040] samples: 16640, Training Loss: 0.0155
   Time since start: 0:22:48.122001
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:22:49.473369
[batch 1080] samples: 17280, Training Loss: 0.0697
   Time since start: 0:22:50.751906
[batch 1100] samples: 17600, Training Loss: 0.0002
   Time since start: 0:22:52.035400
[batch 1120] samples: 17920, Training Loss: 0.0007
   Time since start: 0:22:53.951303
[batch 1140] samples: 18240, Training Loss: 0.0017
   Time since start: 0:22:55.901476
[batch 1160] samples: 18560, Training Loss: 0.0004
   Time since start: 0:22:57.983108
[batch 1180] samples: 18880, Training Loss: 0.0033
   Time since start: 0:22:59.945653
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:23:01.816525
[batch 1220] samples: 19520, Training Loss: 0.0003
   Time since start: 0:23:03.675656
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:23:05.547441
[batch 1260] samples: 20160, Training Loss: 0.0010
   Time since start: 0:23:07.406702
[batch 1280] samples: 20480, Training Loss: 0.0006
   Time since start: 0:23:09.278303
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:23:11.149143
[batch 1320] samples: 21120, Training Loss: 0.0006
   Time since start: 0:23:13.009291
[batch 1340] samples: 21440, Training Loss: 0.0003
   Time since start: 0:23:14.880306
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:23:16.832099
[batch 1380] samples: 22080, Training Loss: 0.0157
   Time since start: 0:23:18.792990
[batch 1400] samples: 22400, Training Loss: 0.0003
   Time since start: 0:23:20.742576
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:23:22.685471
[batch 1440] samples: 23040, Training Loss: 0.0003
   Time since start: 0:23:24.635499
[batch 1460] samples: 23360, Training Loss: 0.0001
   Time since start: 0:23:26.558053
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:23:28.508626
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:23:30.460004
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:23:32.433671
[batch 1540] samples: 24640, Training Loss: 0.0012
   Time since start: 0:23:34.403081
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:23:36.365002
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:23:38.306725
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:23:40.290963
[batch 1620] samples: 25920, Training Loss: 0.0004
   Time since start: 0:23:42.242076
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:23:44.184934
[batch 1660] samples: 26560, Training Loss: 0.0035
   Time since start: 0:23:46.094542
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:23:48.055393
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:23:50.019291
[batch 1720] samples: 27520, Training Loss: 0.0379
   Time since start: 0:23:51.919575
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:23:53.460721
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:23:54.736647
[batch 1780] samples: 28480, Training Loss: 0.0005
   Time since start: 0:23:56.031559
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:23:57.343946
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:23:58.653903
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:23:59.984403
[batch 1860] samples: 29760, Training Loss: 0.0006
   Time since start: 0:24:01.339867
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:24:02.692192
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:24:04.059365
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:24:05.597862
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:24:07.644256
[batch 1960] samples: 31360, Training Loss: 0.0008
   Time since start: 0:24:09.646972
--m-Epoch 8 done.
   Training Loss: 0.0010
   Validation Loss: 0.0007
Patience decreased: Patience is now  0
Stopping early
     precision    recall  f1-score  support  epoch  class
0     1.000000  1.000000  1.000000   5916.0      1      0
1     0.997361  1.000000  0.998679    378.0      1      1
2     1.000000  1.000000  1.000000   1128.0      1      2
3     1.000000  1.000000  1.000000    420.0      1      3
4     1.000000  1.000000  1.000000    576.0      1      4
..         ...       ...       ...      ...    ...    ...
371   1.000000  1.000000  1.000000     72.0      8     42
372   0.998712  0.999264  0.998988  32592.0      8      0
373   0.995615  0.998283  0.996923  32592.0      8      1
374   0.998729  0.999264  0.998992  32592.0      8      2
375   0.999046  0.999320  0.999109  32592.0      8      3

[376 rows x 6 columns]
device: cuda
Epoch: 1 of 30
[batch 20] samples: 320, Training Loss: 0.4846
   Time since start: 0:00:02.011564
[batch 40] samples: 640, Training Loss: 0.2800
   Time since start: 0:00:03.875900
[batch 60] samples: 960, Training Loss: 0.1842
   Time since start: 0:00:05.751684
[batch 80] samples: 1280, Training Loss: 0.1360
   Time since start: 0:00:07.622416
[batch 100] samples: 1600, Training Loss: 0.1273
   Time since start: 0:00:09.502552
[batch 120] samples: 1920, Training Loss: 0.0975
   Time since start: 0:00:11.372239
[batch 140] samples: 2240, Training Loss: 0.0820
   Time since start: 0:00:13.250334
[batch 160] samples: 2560, Training Loss: 0.0727
   Time since start: 0:00:14.737557
[batch 180] samples: 2880, Training Loss: 0.0919
   Time since start: 0:00:16.078052
[batch 200] samples: 3200, Training Loss: 0.0955
   Time since start: 0:00:17.423727
[batch 220] samples: 3520, Training Loss: 0.0698
   Time since start: 0:00:18.765251
[batch 240] samples: 3840, Training Loss: 0.0637
   Time since start: 0:00:20.103932
[batch 260] samples: 4160, Training Loss: 0.0753
   Time since start: 0:00:21.441499
[batch 280] samples: 4480, Training Loss: 0.0525
   Time since start: 0:00:22.937177
[batch 300] samples: 4800, Training Loss: 0.0416
   Time since start: 0:00:24.714485
[batch 320] samples: 5120, Training Loss: 0.0444
   Time since start: 0:00:26.501069
[batch 340] samples: 5440, Training Loss: 0.0372
   Time since start: 0:00:28.296429
[batch 360] samples: 5760, Training Loss: 0.0421
   Time since start: 0:00:30.163169
[batch 380] samples: 6080, Training Loss: 0.0381
   Time since start: 0:00:32.048048
[batch 400] samples: 6400, Training Loss: 0.0341
   Time since start: 0:00:33.933137
[batch 420] samples: 6720, Training Loss: 0.0248
   Time since start: 0:00:35.828307
[batch 440] samples: 7040, Training Loss: 0.0202
   Time since start: 0:00:37.708513
[batch 460] samples: 7360, Training Loss: 0.0220
   Time since start: 0:00:39.595773
[batch 480] samples: 7680, Training Loss: 0.0170
   Time since start: 0:00:41.353921
[batch 500] samples: 8000, Training Loss: 0.0220
   Time since start: 0:00:42.738006
[batch 520] samples: 8320, Training Loss: 0.0259
   Time since start: 0:00:44.079314
[batch 540] samples: 8640, Training Loss: 0.0191
   Time since start: 0:00:45.383112
[batch 560] samples: 8960, Training Loss: 0.0222
   Time since start: 0:00:46.964859
[batch 580] samples: 9280, Training Loss: 0.0202
   Time since start: 0:00:48.305273
[batch 600] samples: 9600, Training Loss: 0.0132
   Time since start: 0:00:49.656886
[batch 620] samples: 9920, Training Loss: 0.0483
   Time since start: 0:00:50.996631
[batch 640] samples: 10240, Training Loss: 0.0191
   Time since start: 0:00:52.336901
[batch 660] samples: 10560, Training Loss: 0.0171
   Time since start: 0:00:53.839524
[batch 680] samples: 10880, Training Loss: 0.0133
   Time since start: 0:00:55.588705
[batch 700] samples: 11200, Training Loss: 0.0066
   Time since start: 0:00:57.448925
[batch 720] samples: 11520, Training Loss: 0.0256
   Time since start: 0:00:59.318736
[batch 740] samples: 11840, Training Loss: 0.0088
   Time since start: 0:01:01.209406
[batch 760] samples: 12160, Training Loss: 0.0079
   Time since start: 0:01:03.079339
[batch 780] samples: 12480, Training Loss: 0.0086
   Time since start: 0:01:04.638808
[batch 800] samples: 12800, Training Loss: 0.0094
   Time since start: 0:01:05.985267
[batch 820] samples: 13120, Training Loss: 0.0051
   Time since start: 0:01:07.331508
[batch 840] samples: 13440, Training Loss: 0.0077
   Time since start: 0:01:08.677481
[batch 860] samples: 13760, Training Loss: 0.0075
   Time since start: 0:01:10.023356
[batch 880] samples: 14080, Training Loss: 0.0044
   Time since start: 0:01:11.370075
[batch 900] samples: 14400, Training Loss: 0.0057
   Time since start: 0:01:12.716017
[batch 920] samples: 14720, Training Loss: 0.0077
   Time since start: 0:01:14.019799
[batch 940] samples: 15040, Training Loss: 0.0069
   Time since start: 0:01:15.323226
[batch 960] samples: 15360, Training Loss: 0.0143
   Time since start: 0:01:16.645389
[batch 980] samples: 15680, Training Loss: 0.0068
   Time since start: 0:01:18.013447
[batch 1000] samples: 16000, Training Loss: 0.0047
   Time since start: 0:01:19.395127
[batch 1020] samples: 16320, Training Loss: 0.0045
   Time since start: 0:01:20.783042
[batch 1040] samples: 16640, Training Loss: 0.0088
   Time since start: 0:01:22.171460
[batch 1060] samples: 16960, Training Loss: 0.0081
   Time since start: 0:01:23.552398
[batch 1080] samples: 17280, Training Loss: 0.0063
   Time since start: 0:01:24.935847
[batch 1100] samples: 17600, Training Loss: 0.0040
   Time since start: 0:01:26.320576
[batch 1120] samples: 17920, Training Loss: 0.0039
   Time since start: 0:01:27.704472
[batch 1140] samples: 18240, Training Loss: 0.0033
   Time since start: 0:01:29.089549
[batch 1160] samples: 18560, Training Loss: 0.0051
   Time since start: 0:01:30.472448
[batch 1180] samples: 18880, Training Loss: 0.0052
   Time since start: 0:01:31.858166
[batch 1200] samples: 19200, Training Loss: 0.0069
   Time since start: 0:01:33.242533
[batch 1220] samples: 19520, Training Loss: 0.0046
   Time since start: 0:01:35.068787
[batch 1240] samples: 19840, Training Loss: 0.0034
   Time since start: 0:01:37.030852
[batch 1260] samples: 20160, Training Loss: 0.0066
   Time since start: 0:01:39.001598
[batch 1280] samples: 20480, Training Loss: 0.0107
   Time since start: 0:01:40.950442
[batch 1300] samples: 20800, Training Loss: 0.0090
   Time since start: 0:01:42.901006
[batch 1320] samples: 21120, Training Loss: 0.0026
   Time since start: 0:01:44.841882
[batch 1340] samples: 21440, Training Loss: 0.0054
   Time since start: 0:01:46.792585
[batch 1360] samples: 21760, Training Loss: 0.0038
   Time since start: 0:01:48.752988
[batch 1380] samples: 22080, Training Loss: 0.0029
   Time since start: 0:01:50.692680
[batch 1400] samples: 22400, Training Loss: 0.0033
   Time since start: 0:01:52.663097
[batch 1420] samples: 22720, Training Loss: 0.0040
   Time since start: 0:01:54.622095
[batch 1440] samples: 23040, Training Loss: 0.0080
   Time since start: 0:01:56.594700
[batch 1460] samples: 23360, Training Loss: 0.0023
   Time since start: 0:01:58.555663
[batch 1480] samples: 23680, Training Loss: 0.0032
   Time since start: 0:02:00.515097
[batch 1500] samples: 24000, Training Loss: 0.0029
   Time since start: 0:02:02.457807
[batch 1520] samples: 24320, Training Loss: 0.0047
   Time since start: 0:02:04.239494
[batch 1540] samples: 24640, Training Loss: 0.0023
   Time since start: 0:02:05.626678
[batch 1560] samples: 24960, Training Loss: 0.0023
   Time since start: 0:02:07.014748
[batch 1580] samples: 25280, Training Loss: 0.0020
   Time since start: 0:02:08.396696
[batch 1600] samples: 25600, Training Loss: 0.0029
   Time since start: 0:02:09.778827
[batch 1620] samples: 25920, Training Loss: 0.0036
   Time since start: 0:02:11.131388
[batch 1640] samples: 26240, Training Loss: 0.0085
   Time since start: 0:02:12.478193
[batch 1660] samples: 26560, Training Loss: 0.0027
   Time since start: 0:02:13.822418
[batch 1680] samples: 26880, Training Loss: 0.0015
   Time since start: 0:02:15.169547
[batch 1700] samples: 27200, Training Loss: 0.0028
   Time since start: 0:02:16.518209
[batch 1720] samples: 27520, Training Loss: 0.0028
   Time since start: 0:02:17.861350
[batch 1740] samples: 27840, Training Loss: 0.0021
   Time since start: 0:02:19.229305
[batch 1760] samples: 28160, Training Loss: 0.0018
   Time since start: 0:02:20.612837
[batch 1780] samples: 28480, Training Loss: 0.0147
   Time since start: 0:02:21.998259
[batch 1800] samples: 28800, Training Loss: 0.0022
   Time since start: 0:02:23.380883
[batch 1820] samples: 29120, Training Loss: 0.0075
   Time since start: 0:02:24.763443
[batch 1840] samples: 29440, Training Loss: 0.0064
   Time since start: 0:02:26.145737
[batch 1860] samples: 29760, Training Loss: 0.0012
   Time since start: 0:02:27.530138
[batch 1880] samples: 30080, Training Loss: 0.0022
   Time since start: 0:02:28.915920
[batch 1900] samples: 30400, Training Loss: 0.0089
   Time since start: 0:02:30.262081
[batch 1920] samples: 30720, Training Loss: 0.0023
   Time since start: 0:02:31.606735
[batch 1940] samples: 31040, Training Loss: 0.0012
   Time since start: 0:02:32.954242
[batch 1960] samples: 31360, Training Loss: 0.0021
   Time since start: 0:02:34.256920
--m-Epoch 1 done.
   Training Loss: 0.0322
   Validation Loss: 0.0017
Epoch: 2 of 30
[batch 20] samples: 320, Training Loss: 0.0160
   Time since start: 0:02:45.867015
[batch 40] samples: 640, Training Loss: 0.0016
   Time since start: 0:02:47.319485
[batch 60] samples: 960, Training Loss: 0.0184
   Time since start: 0:02:48.630019
[batch 80] samples: 1280, Training Loss: 0.0021
   Time since start: 0:02:49.958248
[batch 100] samples: 1600, Training Loss: 0.0038
   Time since start: 0:02:51.509306
[batch 120] samples: 1920, Training Loss: 0.0012
   Time since start: 0:02:53.381270
[batch 140] samples: 2240, Training Loss: 0.0011
   Time since start: 0:02:55.249252
[batch 160] samples: 2560, Training Loss: 0.0010
   Time since start: 0:02:56.795206
[batch 180] samples: 2880, Training Loss: 0.0011
   Time since start: 0:02:58.150631
[batch 200] samples: 3200, Training Loss: 0.0009
   Time since start: 0:02:59.569197
[batch 220] samples: 3520, Training Loss: 0.0027
   Time since start: 0:03:01.292380
[batch 240] samples: 3840, Training Loss: 0.0009
   Time since start: 0:03:02.602286
[batch 260] samples: 4160, Training Loss: 0.0012
   Time since start: 0:03:03.910348
[batch 280] samples: 4480, Training Loss: 0.0013
   Time since start: 0:03:05.566505
[batch 300] samples: 4800, Training Loss: 0.0013
   Time since start: 0:03:07.031658
[batch 320] samples: 5120, Training Loss: 0.0010
   Time since start: 0:03:08.340879
[batch 340] samples: 5440, Training Loss: 0.0017
   Time since start: 0:03:09.650304
[batch 360] samples: 5760, Training Loss: 0.0008
   Time since start: 0:03:10.959575
[batch 380] samples: 6080, Training Loss: 0.0019
   Time since start: 0:03:12.287427
[batch 400] samples: 6400, Training Loss: 0.0019
   Time since start: 0:03:13.633988
[batch 420] samples: 6720, Training Loss: 0.0015
   Time since start: 0:03:14.984556
[batch 440] samples: 7040, Training Loss: 0.0015
   Time since start: 0:03:16.335823
[batch 460] samples: 7360, Training Loss: 0.0009
   Time since start: 0:03:17.716208
[batch 480] samples: 7680, Training Loss: 0.0006
   Time since start: 0:03:19.067336
[batch 500] samples: 8000, Training Loss: 0.0008
   Time since start: 0:03:20.826619
[batch 520] samples: 8320, Training Loss: 0.0011
   Time since start: 0:03:22.687400
[batch 540] samples: 8640, Training Loss: 0.0010
   Time since start: 0:03:24.556950
[batch 560] samples: 8960, Training Loss: 0.0007
   Time since start: 0:03:26.170866
[batch 580] samples: 9280, Training Loss: 0.0006
   Time since start: 0:03:27.520049
[batch 600] samples: 9600, Training Loss: 0.0006
   Time since start: 0:03:28.870508
[batch 620] samples: 9920, Training Loss: 0.0009
   Time since start: 0:03:30.218457
[batch 640] samples: 10240, Training Loss: 0.0010
   Time since start: 0:03:31.565742
[batch 660] samples: 10560, Training Loss: 0.0005
   Time since start: 0:03:32.912552
[batch 680] samples: 10880, Training Loss: 0.0009
   Time since start: 0:03:34.682231
[batch 700] samples: 11200, Training Loss: 0.0007
   Time since start: 0:03:36.538297
[batch 720] samples: 11520, Training Loss: 0.0006
   Time since start: 0:03:38.410629
[batch 740] samples: 11840, Training Loss: 0.0018
   Time since start: 0:03:40.174881
[batch 760] samples: 12160, Training Loss: 0.0012
   Time since start: 0:03:41.975100
[batch 780] samples: 12480, Training Loss: 0.0006
   Time since start: 0:03:43.370852
[batch 800] samples: 12800, Training Loss: 0.0094
   Time since start: 0:03:45.308132
[batch 820] samples: 13120, Training Loss: 0.0010
   Time since start: 0:03:47.105714
[batch 840] samples: 13440, Training Loss: 0.0025
   Time since start: 0:03:48.404488
[batch 860] samples: 13760, Training Loss: 0.0011
   Time since start: 0:03:49.735491
[batch 880] samples: 14080, Training Loss: 0.0005
   Time since start: 0:03:51.436844
[batch 900] samples: 14400, Training Loss: 0.0004
   Time since start: 0:03:53.241170
[batch 920] samples: 14720, Training Loss: 0.0006
   Time since start: 0:03:55.054254
[batch 940] samples: 15040, Training Loss: 0.0017
   Time since start: 0:03:56.875941
[batch 960] samples: 15360, Training Loss: 0.0070
   Time since start: 0:03:58.694493
[batch 980] samples: 15680, Training Loss: 0.0013
   Time since start: 0:04:00.517241
[batch 1000] samples: 16000, Training Loss: 0.0006
   Time since start: 0:04:02.360185
[batch 1020] samples: 16320, Training Loss: 0.0006
   Time since start: 0:04:04.196829
[batch 1040] samples: 16640, Training Loss: 0.0009
   Time since start: 0:04:06.058184
[batch 1060] samples: 16960, Training Loss: 0.0004
   Time since start: 0:04:07.820523
[batch 1080] samples: 17280, Training Loss: 0.0010
   Time since start: 0:04:09.214391
[batch 1100] samples: 17600, Training Loss: 0.0009
   Time since start: 0:04:10.612077
[batch 1120] samples: 17920, Training Loss: 0.0006
   Time since start: 0:04:12.009255
[batch 1140] samples: 18240, Training Loss: 0.0005
   Time since start: 0:04:13.404971
[batch 1160] samples: 18560, Training Loss: 0.0008
   Time since start: 0:04:14.799037
[batch 1180] samples: 18880, Training Loss: 0.0197
   Time since start: 0:04:16.197338
[batch 1200] samples: 19200, Training Loss: 0.0006
   Time since start: 0:04:17.607301
[batch 1220] samples: 19520, Training Loss: 0.0013
   Time since start: 0:04:19.021935
[batch 1240] samples: 19840, Training Loss: 0.0039
   Time since start: 0:04:20.384956
[batch 1260] samples: 20160, Training Loss: 0.0010
   Time since start: 0:04:21.749283
[batch 1280] samples: 20480, Training Loss: 0.0008
   Time since start: 0:04:23.230018
[batch 1300] samples: 20800, Training Loss: 0.0028
   Time since start: 0:04:24.596577
[batch 1320] samples: 21120, Training Loss: 0.0006
   Time since start: 0:04:25.959908
[batch 1340] samples: 21440, Training Loss: 0.0005
   Time since start: 0:04:27.613703
[batch 1360] samples: 21760, Training Loss: 0.0011
   Time since start: 0:04:29.413088
[batch 1380] samples: 22080, Training Loss: 0.0013
   Time since start: 0:04:31.221273
[batch 1400] samples: 22400, Training Loss: 0.0005
   Time since start: 0:04:33.030811
[batch 1420] samples: 22720, Training Loss: 0.0009
   Time since start: 0:04:34.810935
[batch 1440] samples: 23040, Training Loss: 0.0055
   Time since start: 0:04:36.625401
[batch 1460] samples: 23360, Training Loss: 0.0010
   Time since start: 0:04:38.427681
[batch 1480] samples: 23680, Training Loss: 0.0004
   Time since start: 0:04:40.211464
[batch 1500] samples: 24000, Training Loss: 0.0003
   Time since start: 0:04:42.012847
[batch 1520] samples: 24320, Training Loss: 0.0003
   Time since start: 0:04:43.839566
[batch 1540] samples: 24640, Training Loss: 0.0015
   Time since start: 0:04:45.229294
[batch 1560] samples: 24960, Training Loss: 0.0022
   Time since start: 0:04:46.585836
[batch 1580] samples: 25280, Training Loss: 0.0010
   Time since start: 0:04:48.288468
[batch 1600] samples: 25600, Training Loss: 0.0007
   Time since start: 0:04:50.079916
[batch 1620] samples: 25920, Training Loss: 0.0004
   Time since start: 0:04:51.497822
[batch 1640] samples: 26240, Training Loss: 0.0006
   Time since start: 0:04:52.966318
[batch 1660] samples: 26560, Training Loss: 0.0003
   Time since start: 0:04:54.780451
[batch 1680] samples: 26880, Training Loss: 0.0003
   Time since start: 0:04:56.358184
[batch 1700] samples: 27200, Training Loss: 0.0003
   Time since start: 0:04:57.665813
[batch 1720] samples: 27520, Training Loss: 0.0004
   Time since start: 0:04:58.971888
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:05:00.278045
[batch 1760] samples: 28160, Training Loss: 0.0004
   Time since start: 0:05:01.585493
[batch 1780] samples: 28480, Training Loss: 0.0022
   Time since start: 0:05:02.891882
[batch 1800] samples: 28800, Training Loss: 0.0004
   Time since start: 0:05:04.206458
[batch 1820] samples: 29120, Training Loss: 0.0008
   Time since start: 0:05:05.513392
[batch 1840] samples: 29440, Training Loss: 0.0005
   Time since start: 0:05:06.821190
[batch 1860] samples: 29760, Training Loss: 0.0008
   Time since start: 0:05:08.128280
[batch 1880] samples: 30080, Training Loss: 0.0004
   Time since start: 0:05:09.435027
[batch 1900] samples: 30400, Training Loss: 0.0009
   Time since start: 0:05:10.743135
[batch 1920] samples: 30720, Training Loss: 0.0002
   Time since start: 0:05:12.049821
[batch 1940] samples: 31040, Training Loss: 0.0084
   Time since start: 0:05:13.357592
[batch 1960] samples: 31360, Training Loss: 0.0004
   Time since start: 0:05:14.633860
--m-Epoch 2 done.
   Training Loss: 0.0019
   Validation Loss: 0.0003
Epoch: 3 of 30
[batch 20] samples: 320, Training Loss: 0.0012
   Time since start: 0:05:27.143150
[batch 40] samples: 640, Training Loss: 0.0009
   Time since start: 0:05:28.460188
[batch 60] samples: 960, Training Loss: 0.0004
   Time since start: 0:05:29.779717
[batch 80] samples: 1280, Training Loss: 0.0006
   Time since start: 0:05:31.102759
[batch 100] samples: 1600, Training Loss: 0.0012
   Time since start: 0:05:32.426492
[batch 120] samples: 1920, Training Loss: 0.0006
   Time since start: 0:05:33.904419
[batch 140] samples: 2240, Training Loss: 0.0003
   Time since start: 0:05:35.876230
[batch 160] samples: 2560, Training Loss: 0.0002
   Time since start: 0:05:37.846650
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:05:39.817667
[batch 200] samples: 3200, Training Loss: 0.0003
   Time since start: 0:05:41.780921
[batch 220] samples: 3520, Training Loss: 0.0002
   Time since start: 0:05:43.747873
[batch 240] samples: 3840, Training Loss: 0.0004
   Time since start: 0:05:45.696489
[batch 260] samples: 4160, Training Loss: 0.0004
   Time since start: 0:05:47.648506
[batch 280] samples: 4480, Training Loss: 0.0005
   Time since start: 0:05:49.623331
[batch 300] samples: 4800, Training Loss: 0.0002
   Time since start: 0:05:51.570946
[batch 320] samples: 5120, Training Loss: 0.0005
   Time since start: 0:05:53.537670
[batch 340] samples: 5440, Training Loss: 0.0002
   Time since start: 0:05:55.503231
[batch 360] samples: 5760, Training Loss: 0.0003
   Time since start: 0:05:57.459266
[batch 380] samples: 6080, Training Loss: 0.0002
   Time since start: 0:05:59.398357
[batch 400] samples: 6400, Training Loss: 0.0002
   Time since start: 0:06:01.367983
[batch 420] samples: 6720, Training Loss: 0.0005
   Time since start: 0:06:03.290432
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:06:05.162173
[batch 460] samples: 7360, Training Loss: 0.0002
   Time since start: 0:06:06.980576
[batch 480] samples: 7680, Training Loss: 0.0002
   Time since start: 0:06:08.770392
[batch 500] samples: 8000, Training Loss: 0.0003
   Time since start: 0:06:10.430750
[batch 520] samples: 8320, Training Loss: 0.0002
   Time since start: 0:06:11.779671
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:06:13.326434
[batch 560] samples: 8960, Training Loss: 0.0002
   Time since start: 0:06:14.675562
[batch 580] samples: 9280, Training Loss: 0.0002
   Time since start: 0:06:16.062604
[batch 600] samples: 9600, Training Loss: 0.0003
   Time since start: 0:06:17.482671
[batch 620] samples: 9920, Training Loss: 0.0005
   Time since start: 0:06:18.880063
[batch 640] samples: 10240, Training Loss: 0.0003
   Time since start: 0:06:20.514547
[batch 660] samples: 10560, Training Loss: 0.0038
   Time since start: 0:06:22.338611
[batch 680] samples: 10880, Training Loss: 0.0004
   Time since start: 0:06:24.154071
[batch 700] samples: 11200, Training Loss: 0.0003
   Time since start: 0:06:25.969742
[batch 720] samples: 11520, Training Loss: 0.0003
   Time since start: 0:06:27.775656
[batch 740] samples: 11840, Training Loss: 0.0003
   Time since start: 0:06:29.580037
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:06:31.385657
[batch 780] samples: 12480, Training Loss: 0.0008
   Time since start: 0:06:33.184654
[batch 800] samples: 12800, Training Loss: 0.0005
   Time since start: 0:06:34.993547
[batch 820] samples: 13120, Training Loss: 0.0003
   Time since start: 0:06:36.797340
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:06:38.604897
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:06:40.414017
[batch 880] samples: 14080, Training Loss: 0.0007
   Time since start: 0:06:42.203352
[batch 900] samples: 14400, Training Loss: 0.0004
   Time since start: 0:06:44.010734
[batch 920] samples: 14720, Training Loss: 0.0004
   Time since start: 0:06:45.829735
[batch 940] samples: 15040, Training Loss: 0.0002
   Time since start: 0:06:47.617840
[batch 960] samples: 15360, Training Loss: 0.0003
   Time since start: 0:06:49.429664
[batch 980] samples: 15680, Training Loss: 0.0005
   Time since start: 0:06:51.234551
[batch 1000] samples: 16000, Training Loss: 0.0003
   Time since start: 0:06:53.032043
[batch 1020] samples: 16320, Training Loss: 0.0002
   Time since start: 0:06:54.816919
[batch 1040] samples: 16640, Training Loss: 0.0002
   Time since start: 0:06:56.597255
[batch 1060] samples: 16960, Training Loss: 0.0007
   Time since start: 0:06:58.407080
[batch 1080] samples: 17280, Training Loss: 0.0078
   Time since start: 0:07:00.219121
[batch 1100] samples: 17600, Training Loss: 0.0009
   Time since start: 0:07:02.028671
[batch 1120] samples: 17920, Training Loss: 0.0007
   Time since start: 0:07:03.827408
[batch 1140] samples: 18240, Training Loss: 0.0004
   Time since start: 0:07:05.592516
[batch 1160] samples: 18560, Training Loss: 0.0088
   Time since start: 0:07:06.918194
[batch 1180] samples: 18880, Training Loss: 0.0005
   Time since start: 0:07:08.236456
[batch 1200] samples: 19200, Training Loss: 0.0008
   Time since start: 0:07:10.002739
[batch 1220] samples: 19520, Training Loss: 0.0003
   Time since start: 0:07:11.804912
[batch 1240] samples: 19840, Training Loss: 0.0080
   Time since start: 0:07:13.623477
[batch 1260] samples: 20160, Training Loss: 0.0003
   Time since start: 0:07:15.415246
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:07:17.237408
[batch 1300] samples: 20800, Training Loss: 0.0002
   Time since start: 0:07:19.053831
[batch 1320] samples: 21120, Training Loss: 0.0008
   Time since start: 0:07:20.865719
[batch 1340] samples: 21440, Training Loss: 0.0005
   Time since start: 0:07:22.662477
[batch 1360] samples: 21760, Training Loss: 0.0029
   Time since start: 0:07:24.455426
[batch 1380] samples: 22080, Training Loss: 0.0003
   Time since start: 0:07:26.271688
[batch 1400] samples: 22400, Training Loss: 0.0003
   Time since start: 0:07:28.096077
[batch 1420] samples: 22720, Training Loss: 0.0005
   Time since start: 0:07:29.576803
[batch 1440] samples: 23040, Training Loss: 0.0006
   Time since start: 0:07:31.258731
[batch 1460] samples: 23360, Training Loss: 0.0004
   Time since start: 0:07:33.156119
[batch 1480] samples: 23680, Training Loss: 0.0004
   Time since start: 0:07:35.077282
[batch 1500] samples: 24000, Training Loss: 0.0006
   Time since start: 0:07:36.527535
[batch 1520] samples: 24320, Training Loss: 0.0007
   Time since start: 0:07:37.888627
[batch 1540] samples: 24640, Training Loss: 0.0002
   Time since start: 0:07:39.247781
[batch 1560] samples: 24960, Training Loss: 0.0003
   Time since start: 0:07:40.833391
[batch 1580] samples: 25280, Training Loss: 0.0004
   Time since start: 0:07:42.621781
[batch 1600] samples: 25600, Training Loss: 0.0002
   Time since start: 0:07:44.399981
[batch 1620] samples: 25920, Training Loss: 0.0008
   Time since start: 0:07:46.162442
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:07:47.899498
[batch 1660] samples: 26560, Training Loss: 0.0005
   Time since start: 0:07:49.657586
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:07:51.435409
[batch 1700] samples: 27200, Training Loss: 0.0021
   Time since start: 0:07:53.194712
[batch 1720] samples: 27520, Training Loss: 0.0005
   Time since start: 0:07:54.952988
[batch 1740] samples: 27840, Training Loss: 0.0048
   Time since start: 0:07:56.720822
[batch 1760] samples: 28160, Training Loss: 0.0002
   Time since start: 0:07:58.489573
[batch 1780] samples: 28480, Training Loss: 0.0002
   Time since start: 0:08:00.341764
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:08:01.691234
[batch 1820] samples: 29120, Training Loss: 0.0009
   Time since start: 0:08:02.983020
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:08:04.721634
[batch 1860] samples: 29760, Training Loss: 0.0023
   Time since start: 0:08:06.683344
[batch 1880] samples: 30080, Training Loss: 0.0003
   Time since start: 0:08:08.643429
[batch 1900] samples: 30400, Training Loss: 0.0018
   Time since start: 0:08:10.603683
[batch 1920] samples: 30720, Training Loss: 0.0003
   Time since start: 0:08:12.576026
[batch 1940] samples: 31040, Training Loss: 0.0017
   Time since start: 0:08:14.539251
[batch 1960] samples: 31360, Training Loss: 0.0005
   Time since start: 0:08:16.459148
--m-Epoch 3 done.
   Training Loss: 0.0012
   Validation Loss: 0.0008
Patience decreased: Patience is now  4
Epoch: 4 of 30
[batch 20] samples: 320, Training Loss: 0.0028
   Time since start: 0:08:29.245935
[batch 40] samples: 640, Training Loss: 0.0003
   Time since start: 0:08:30.924879
[batch 60] samples: 960, Training Loss: 0.0002
   Time since start: 0:08:32.675877
[batch 80] samples: 1280, Training Loss: 0.0002
   Time since start: 0:08:34.403939
[batch 100] samples: 1600, Training Loss: 0.0003
   Time since start: 0:08:36.117425
[batch 120] samples: 1920, Training Loss: 0.0002
   Time since start: 0:08:37.863621
[batch 140] samples: 2240, Training Loss: 0.0022
   Time since start: 0:08:39.602115
[batch 160] samples: 2560, Training Loss: 0.0004
   Time since start: 0:08:41.314108
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:08:43.054650
[batch 200] samples: 3200, Training Loss: 0.0002
   Time since start: 0:08:44.802178
[batch 220] samples: 3520, Training Loss: 0.0002
   Time since start: 0:08:46.559541
[batch 240] samples: 3840, Training Loss: 0.0017
   Time since start: 0:08:48.316076
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:08:49.874470
[batch 280] samples: 4480, Training Loss: 0.0005
   Time since start: 0:08:51.168908
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:08:52.939147
[batch 320] samples: 5120, Training Loss: 0.0008
   Time since start: 0:08:54.713819
[batch 340] samples: 5440, Training Loss: 0.0002
   Time since start: 0:08:56.071398
[batch 360] samples: 5760, Training Loss: 0.0002
   Time since start: 0:08:57.430700
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:08:58.789995
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:09:00.147570
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:09:01.504841
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:09:02.826265
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:09:04.149163
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:09:05.468639
[batch 500] samples: 8000, Training Loss: 0.0002
   Time since start: 0:09:06.788197
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:09:08.107039
[batch 540] samples: 8640, Training Loss: 0.0007
   Time since start: 0:09:09.425736
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:09:10.748975
[batch 580] samples: 9280, Training Loss: 0.0002
   Time since start: 0:09:12.069309
[batch 600] samples: 9600, Training Loss: 0.0003
   Time since start: 0:09:13.387971
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:09:14.706860
[batch 640] samples: 10240, Training Loss: 0.0005
   Time since start: 0:09:16.027256
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:09:17.523701
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:09:19.440743
[batch 700] samples: 11200, Training Loss: 0.0022
   Time since start: 0:09:21.349046
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:09:23.260258
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:09:25.171542
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:09:27.082023
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:09:28.998229
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:09:30.914265
[batch 820] samples: 13120, Training Loss: 0.0006
   Time since start: 0:09:32.816035
[batch 840] samples: 13440, Training Loss: 0.0001
   Time since start: 0:09:34.726303
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:09:36.631453
[batch 880] samples: 14080, Training Loss: 0.0004
   Time since start: 0:09:38.360887
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:09:39.725758
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:09:41.093097
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:09:42.460621
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:09:43.967379
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:09:46.018323
[batch 1000] samples: 16000, Training Loss: 0.0002
   Time since start: 0:09:47.778549
[batch 1020] samples: 16320, Training Loss: 0.0002
   Time since start: 0:09:49.179614
[batch 1040] samples: 16640, Training Loss: 0.0009
   Time since start: 0:09:50.703793
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:09:52.109569
[batch 1080] samples: 17280, Training Loss: 0.0004
   Time since start: 0:09:53.510153
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:09:54.912839
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:09:56.317255
[batch 1140] samples: 18240, Training Loss: 0.0002
   Time since start: 0:09:57.720170
[batch 1160] samples: 18560, Training Loss: 0.0002
   Time since start: 0:09:59.119642
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:10:00.524057
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:10:01.925331
[batch 1220] samples: 19520, Training Loss: 0.0002
   Time since start: 0:10:03.320603
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:10:04.723156
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:10:06.124141
[batch 1280] samples: 20480, Training Loss: 0.0020
   Time since start: 0:10:07.524704
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:10:08.929958
[batch 1320] samples: 21120, Training Loss: 0.0002
   Time since start: 0:10:10.521282
[batch 1340] samples: 21440, Training Loss: 0.0005
   Time since start: 0:10:12.490040
[batch 1360] samples: 21760, Training Loss: 0.0004
   Time since start: 0:10:14.462299
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:10:16.423756
[batch 1400] samples: 22400, Training Loss: 0.0003
   Time since start: 0:10:18.394925
[batch 1420] samples: 22720, Training Loss: 0.0002
   Time since start: 0:10:20.350989
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:10:21.851814
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:10:23.692257
[batch 1480] samples: 23680, Training Loss: 0.0004
   Time since start: 0:10:25.090953
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:10:26.493519
[batch 1520] samples: 24320, Training Loss: 0.0002
   Time since start: 0:10:27.891535
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:10:29.292423
[batch 1560] samples: 24960, Training Loss: 0.0001
   Time since start: 0:10:30.736491
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:10:32.692018
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:10:34.090765
[batch 1620] samples: 25920, Training Loss: 0.0004
   Time since start: 0:10:35.791581
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:10:37.486146
[batch 1660] samples: 26560, Training Loss: 0.0004
   Time since start: 0:10:38.883272
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:10:40.283631
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:10:41.682363
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:10:43.080413
[batch 1740] samples: 27840, Training Loss: 0.0043
   Time since start: 0:10:44.475940
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:10:45.874860
[batch 1780] samples: 28480, Training Loss: 0.0006
   Time since start: 0:10:47.773975
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:10:49.582210
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:10:50.984007
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:10:52.382742
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:10:53.795262
[batch 1880] samples: 30080, Training Loss: 0.0002
   Time since start: 0:10:55.221030
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:10:56.618807
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:10:58.016652
[batch 1940] samples: 31040, Training Loss: 0.0003
   Time since start: 0:10:59.424693
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:11:00.772685
--m-Epoch 4 done.
   Training Loss: 0.0007
   Validation Loss: 0.0002
Epoch: 5 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:11:12.731005
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:11:14.087120
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:11:15.488016
[batch 80] samples: 1280, Training Loss: 0.0003
   Time since start: 0:11:17.462774
[batch 100] samples: 1600, Training Loss: 0.0005
   Time since start: 0:11:19.423739
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:11:21.384155
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:11:23.329713
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:11:24.912482
[batch 180] samples: 2880, Training Loss: 0.0050
   Time since start: 0:11:26.308044
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:11:27.707237
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:11:29.104285
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:11:30.500110
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:11:31.898937
[batch 280] samples: 4480, Training Loss: 0.0001
   Time since start: 0:11:33.294281
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:11:34.689646
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:11:36.216422
[batch 340] samples: 5440, Training Loss: 0.0002
   Time since start: 0:11:37.612343
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:11:39.014083
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:11:40.412883
[batch 400] samples: 6400, Training Loss: 0.0011
   Time since start: 0:11:42.345151
[batch 420] samples: 6720, Training Loss: 0.0002
   Time since start: 0:11:44.304623
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:11:46.109299
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:11:47.511667
[batch 480] samples: 7680, Training Loss: 0.0031
   Time since start: 0:11:48.902470
[batch 500] samples: 8000, Training Loss: 0.0002
   Time since start: 0:11:50.300954
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:11:51.704610
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:11:53.183758
[batch 560] samples: 8960, Training Loss: 0.0011
   Time since start: 0:11:55.108450
[batch 580] samples: 9280, Training Loss: 0.0119
   Time since start: 0:11:56.992220
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:11:58.861574
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:12:00.739943
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:12:02.621613
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:12:04.493658
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:12:06.387084
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:12:08.262302
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:12:10.184485
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:12:12.164513
[batch 760] samples: 12160, Training Loss: 0.0009
   Time since start: 0:12:14.134661
[batch 780] samples: 12480, Training Loss: 0.0003
   Time since start: 0:12:16.113409
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:12:18.047775
[batch 820] samples: 13120, Training Loss: 0.0018
   Time since start: 0:12:19.946525
[batch 840] samples: 13440, Training Loss: 0.0011
   Time since start: 0:12:21.853195
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:12:23.757936
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:12:25.461075
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:12:26.826770
[batch 920] samples: 14720, Training Loss: 0.0020
   Time since start: 0:12:28.194052
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:12:29.539876
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:12:30.866069
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:12:32.192726
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:12:33.521309
[batch 1020] samples: 16320, Training Loss: 0.0004
   Time since start: 0:12:34.849993
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:12:36.173508
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:12:37.498975
[batch 1080] samples: 17280, Training Loss: 0.0006
   Time since start: 0:12:38.837276
[batch 1100] samples: 17600, Training Loss: 0.0005
   Time since start: 0:12:40.203975
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:12:41.568138
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:12:43.200012
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:12:45.099299
[batch 1180] samples: 18880, Training Loss: 0.0011
   Time since start: 0:12:46.989871
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:12:48.880172
[batch 1220] samples: 19520, Training Loss: 0.0002
   Time since start: 0:12:50.769909
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:12:52.374563
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:12:53.735911
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:12:55.097959
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:12:56.465388
[batch 1320] samples: 21120, Training Loss: 0.0004
   Time since start: 0:12:57.827949
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:12:59.188905
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:13:00.551646
[batch 1380] samples: 22080, Training Loss: 0.0004
   Time since start: 0:13:01.918634
[batch 1400] samples: 22400, Training Loss: 0.0003
   Time since start: 0:13:03.281379
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:13:04.645563
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:13:06.009836
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:13:07.383702
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:13:08.784583
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:13:10.184526
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:13:11.586028
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:13:12.985507
[batch 1560] samples: 24960, Training Loss: 0.0008
   Time since start: 0:13:14.502379
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:13:15.903109
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:13:17.404090
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:13:19.031176
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:13:20.423714
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:13:21.795645
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:13:23.150984
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:13:24.503496
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:13:25.861740
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:13:27.223993
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:13:28.580585
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:13:29.936787
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:13:31.293992
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:13:32.651248
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:13:34.005777
[batch 1860] samples: 29760, Training Loss: 0.0015
   Time since start: 0:13:35.359493
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:13:36.717489
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:13:38.076385
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:13:39.432134
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:13:40.788334
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:13:42.099725
--m-Epoch 5 done.
   Training Loss: 0.0005
   Validation Loss: 0.0002
Patience decreased: Patience is now  4
Epoch: 6 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:13:55.258759
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:13:57.159585
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:13:59.080193
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:14:00.969794
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:14:02.859666
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:14:04.763800
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:14:06.617235
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:14:08.518574
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:14:10.428422
[batch 200] samples: 3200, Training Loss: 0.0010
   Time since start: 0:14:12.397778
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:14:14.359219
[batch 240] samples: 3840, Training Loss: 0.0001
   Time since start: 0:14:16.273391
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:14:18.052438
[batch 280] samples: 4480, Training Loss: 0.0002
   Time since start: 0:14:20.033643
[batch 300] samples: 4800, Training Loss: 0.0002
   Time since start: 0:14:21.955891
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:14:23.639956
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:14:25.009615
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:14:26.463225
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:14:28.371602
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:14:30.292781
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:14:32.196002
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:14:33.619567
[batch 460] samples: 7360, Training Loss: 0.0088
   Time since start: 0:14:35.192181
[batch 480] samples: 7680, Training Loss: 0.0010
   Time since start: 0:14:36.641930
[batch 500] samples: 8000, Training Loss: 0.0002
   Time since start: 0:14:37.995608
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:14:39.345288
[batch 540] samples: 8640, Training Loss: 0.0003
   Time since start: 0:14:40.699320
[batch 560] samples: 8960, Training Loss: 0.0004
   Time since start: 0:14:42.056457
[batch 580] samples: 9280, Training Loss: 0.0001
   Time since start: 0:14:43.407950
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:14:44.825005
[batch 620] samples: 9920, Training Loss: 0.0002
   Time since start: 0:14:46.238782
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:14:47.632920
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:14:49.060704
[batch 680] samples: 10880, Training Loss: 0.0003
   Time since start: 0:14:50.891984
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:14:52.249669
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:14:53.600214
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:14:54.947732
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:14:56.298649
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:14:57.942969
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:14:59.814872
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:15:01.695886
[batch 840] samples: 13440, Training Loss: 0.0014
   Time since start: 0:15:03.279979
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:15:04.632905
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:15:05.983930
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:15:07.337288
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:15:09.166166
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:15:11.056375
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:15:12.936639
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:15:14.853746
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:15:16.389605
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:15:17.888247
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:15:19.747727
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:15:21.617921
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:15:23.487248
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:15:25.356923
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:15:27.218184
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:15:29.097132
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:15:30.966425
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:15:32.864216
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:15:34.753018
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:15:36.627767
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:15:38.521235
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:15:40.410797
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:15:42.302308
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:15:43.969017
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:15:45.323700
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:15:47.168195
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:15:49.069999
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:15:50.975380
[batch 1400] samples: 22400, Training Loss: 0.0082
   Time since start: 0:15:52.852879
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:15:54.763005
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:15:56.653150
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:15:58.341638
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:15:59.704463
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:16:01.537698
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:16:03.447550
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:16:05.313978
[batch 1560] samples: 24960, Training Loss: 0.0001
   Time since start: 0:16:07.169479
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:16:08.521085
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:16:10.155249
[batch 1620] samples: 25920, Training Loss: 0.0002
   Time since start: 0:16:11.815239
[batch 1640] samples: 26240, Training Loss: 0.0010
   Time since start: 0:16:13.164710
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:16:14.831703
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:16:16.186853
[batch 1700] samples: 27200, Training Loss: 0.0005
   Time since start: 0:16:17.553921
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:16:18.906945
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:16:20.253659
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:16:21.606880
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:16:22.964932
[batch 1800] samples: 28800, Training Loss: 0.0023
   Time since start: 0:16:24.318422
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:16:25.673785
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:16:27.026049
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:16:28.526319
[batch 1880] samples: 30080, Training Loss: 0.0018
   Time since start: 0:16:30.431598
[batch 1900] samples: 30400, Training Loss: 0.0004
   Time since start: 0:16:32.320490
[batch 1920] samples: 30720, Training Loss: 0.0005
   Time since start: 0:16:34.219917
[batch 1940] samples: 31040, Training Loss: 0.0002
   Time since start: 0:16:36.102512
[batch 1960] samples: 31360, Training Loss: 0.0003
   Time since start: 0:16:37.972984
--m-Epoch 6 done.
   Training Loss: 0.0006
   Validation Loss: 0.0009
Patience decreased: Patience is now  3
Epoch: 7 of 30
[batch 20] samples: 320, Training Loss: 0.0008
   Time since start: 0:16:50.411224
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:16:52.218203
[batch 60] samples: 960, Training Loss: 0.0004
   Time since start: 0:16:54.047088
[batch 80] samples: 1280, Training Loss: 0.0007
   Time since start: 0:16:55.833879
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:16:57.646108
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:16:59.577761
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:17:01.391717
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:17:03.207611
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:17:05.055545
[batch 200] samples: 3200, Training Loss: 0.0014
   Time since start: 0:17:06.961115
[batch 220] samples: 3520, Training Loss: 0.0005
   Time since start: 0:17:08.843633
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:17:10.719522
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:17:12.583723
[batch 280] samples: 4480, Training Loss: 0.0001
   Time since start: 0:17:14.457654
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:17:16.337902
[batch 320] samples: 5120, Training Loss: 0.0003
   Time since start: 0:17:18.208584
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:17:20.085267
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:17:21.958539
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:17:23.824312
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:17:25.690695
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:17:27.548644
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:17:29.441838
[batch 460] samples: 7360, Training Loss: 0.0006
   Time since start: 0:17:31.321255
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:17:33.203894
[batch 500] samples: 8000, Training Loss: 0.0007
   Time since start: 0:17:35.081816
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:17:36.940860
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:17:38.738640
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:17:40.287416
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:17:41.574802
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:17:42.857824
[batch 620] samples: 9920, Training Loss: 0.0059
   Time since start: 0:17:44.142089
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:17:45.779241
[batch 660] samples: 10560, Training Loss: 0.0002
   Time since start: 0:17:47.488642
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:17:49.226030
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:17:50.942758
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:17:52.668052
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:17:54.371605
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:17:56.094033
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:17:57.847363
[batch 800] samples: 12800, Training Loss: 0.0005
   Time since start: 0:17:59.645130
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:18:01.444639
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:18:03.030098
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:18:04.355064
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:18:05.681374
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:18:07.005656
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:18:08.332002
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:18:09.658031
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:18:11.185651
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:18:12.577674
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:18:13.850173
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:18:15.123014
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:18:16.402685
[batch 1060] samples: 16960, Training Loss: 0.0003
   Time since start: 0:18:17.694409
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:18:18.972286
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:18:20.248356
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:18:21.529494
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:18:22.808220
[batch 1160] samples: 18560, Training Loss: 0.0002
   Time since start: 0:18:24.084981
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:18:25.364312
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:18:26.652491
[batch 1220] samples: 19520, Training Loss: 0.0002
   Time since start: 0:18:27.929574
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:18:29.463620
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:18:30.878540
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:18:32.157179
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:18:33.433934
[batch 1320] samples: 21120, Training Loss: 0.0156
   Time since start: 0:18:34.710410
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:18:36.105067
[batch 1360] samples: 21760, Training Loss: 0.0029
   Time since start: 0:18:37.701812
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:18:39.429237
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:18:41.136016
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:18:42.444636
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:18:43.896311
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:18:45.713294
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:18:47.470508
[batch 1500] samples: 24000, Training Loss: 0.0002
   Time since start: 0:18:48.754618
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:18:50.043771
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:18:51.341002
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:18:52.629041
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:18:53.907872
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:18:55.186515
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:18:56.463304
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:18:57.743369
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:18:59.022375
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:19:00.299741
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:19:01.585733
[batch 1720] samples: 27520, Training Loss: 0.0021
   Time since start: 0:19:02.865107
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:19:04.144093
[batch 1760] samples: 28160, Training Loss: 0.0002
   Time since start: 0:19:05.423787
[batch 1780] samples: 28480, Training Loss: 0.0008
   Time since start: 0:19:06.702445
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:19:07.981260
[batch 1820] samples: 29120, Training Loss: 0.0047
   Time since start: 0:19:09.272191
[batch 1840] samples: 29440, Training Loss: 0.0002
   Time since start: 0:19:10.560624
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:19:11.842672
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:19:13.121125
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:19:14.399933
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:19:15.681298
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:19:16.987617
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:19:18.315153
--m-Epoch 7 done.
   Training Loss: 0.0006
   Validation Loss: 0.0002
Patience decreased: Patience is now  2
Epoch: 8 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:19:32.182947
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:19:34.072974
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:19:35.984938
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:19:37.895580
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:19:39.807138
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:19:41.707669
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:19:43.640383
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:19:45.580795
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:19:47.312981
[batch 200] samples: 3200, Training Loss: 0.0012
   Time since start: 0:19:49.028268
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:19:50.917481
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:19:52.817567
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:19:54.738921
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:19:56.713199
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:19:58.684449
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:20:00.645748
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:20:02.608345
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:20:04.582504
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:20:06.551895
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:20:08.149653
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:20:09.541321
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:20:10.861454
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:20:12.178017
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:20:13.501938
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:20:14.820971
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:20:16.143284
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:20:17.478665
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:20:18.804265
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:20:20.133352
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:20:21.463722
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:20:22.924200
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:20:24.251646
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:20:25.580001
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:20:26.904489
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:20:28.230013
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:20:29.558827
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:20:30.886981
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:20:32.254287
[batch 780] samples: 12480, Training Loss: 0.0041
   Time since start: 0:20:33.580393
[batch 800] samples: 12800, Training Loss: 0.0002
   Time since start: 0:20:34.902632
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:20:36.226215
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:20:37.550863
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:20:38.872140
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:20:40.192926
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:20:41.513727
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:20:42.836064
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:20:44.160596
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:20:45.883333
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:20:47.637857
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:20:48.965049
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:20:50.293005
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:20:51.614609
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:20:52.937922
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:20:54.260524
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:20:55.583203
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:20:56.904276
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:20:58.225707
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:20:59.553252
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:21:00.881102
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:21:02.207770
[batch 1220] samples: 19520, Training Loss: 0.0098
   Time since start: 0:21:03.530305
[batch 1240] samples: 19840, Training Loss: 0.0133
   Time since start: 0:21:04.850800
[batch 1260] samples: 20160, Training Loss: 0.0016
   Time since start: 0:21:06.171612
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:21:07.497409
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:21:09.087461
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:21:10.407305
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:21:11.729910
[batch 1360] samples: 21760, Training Loss: 0.0005
   Time since start: 0:21:13.050442
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:21:14.370535
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:21:15.695401
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:21:17.025885
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:21:18.353057
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:21:19.672751
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:21:20.992624
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:21:22.314320
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:21:23.633030
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:21:24.950830
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:21:26.267538
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:21:27.583125
[batch 1600] samples: 25600, Training Loss: 0.0014
   Time since start: 0:21:28.901148
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:21:30.217034
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:21:31.537692
[batch 1660] samples: 26560, Training Loss: 0.0005
   Time since start: 0:21:32.852176
[batch 1680] samples: 26880, Training Loss: 0.0008
   Time since start: 0:21:34.167792
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:21:35.483303
[batch 1720] samples: 27520, Training Loss: 0.0006
   Time since start: 0:21:36.798552
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:21:38.115828
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:21:39.431795
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:21:40.746528
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:21:42.061725
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:21:43.375018
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:21:44.809460
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:21:46.126874
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:21:47.443796
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:21:48.761302
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:21:50.078723
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:21:51.812972
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:21:53.247129
--m-Epoch 8 done.
   Training Loss: 0.0002
   Validation Loss: 0.0003
Patience decreased: Patience is now  1
Epoch: 9 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:22:05.492384
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:22:07.414785
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:22:09.316268
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:22:11.206907
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:22:13.076379
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:22:14.945405
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:22:16.820517
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:22:18.681991
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:22:20.408467
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:22:21.775376
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:22:23.142558
[batch 240] samples: 3840, Training Loss: 0.0003
   Time since start: 0:22:24.506186
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:22:25.871098
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:22:27.235976
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:22:28.607116
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:22:29.970111
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:22:31.359396
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:22:33.228336
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:22:35.106762
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:22:36.830890
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:22:38.203093
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:22:39.599978
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:22:40.997552
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:22:42.393945
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:22:43.797755
[batch 520] samples: 8320, Training Loss: 0.1345
   Time since start: 0:22:45.176332
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:22:46.539380
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:22:47.898172
[batch 580] samples: 9280, Training Loss: 0.0004
   Time since start: 0:22:49.256971
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:22:50.614583
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:22:51.974395
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:22:53.334799
[batch 660] samples: 10560, Training Loss: 0.0074
   Time since start: 0:22:54.691850
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:22:56.050782
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:22:57.775146
[batch 720] samples: 11520, Training Loss: 0.0129
   Time since start: 0:22:59.617792
[batch 740] samples: 11840, Training Loss: 0.0003
   Time since start: 0:23:01.201900
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:23:02.552661
[batch 780] samples: 12480, Training Loss: 0.0005
   Time since start: 0:23:03.904502
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:23:05.750884
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:23:07.632448
[batch 840] samples: 13440, Training Loss: 0.0001
   Time since start: 0:23:09.512993
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:23:11.384379
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:23:13.262151
[batch 900] samples: 14400, Training Loss: 0.0002
   Time since start: 0:23:15.129492
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:23:17.015996
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:23:18.902769
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:23:20.775125
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:23:22.639697
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:23:24.527594
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:23:26.404366
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:23:27.847475
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:23:29.206018
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:23:30.562170
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:23:31.917420
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:23:33.398872
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:23:34.755333
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:23:36.134853
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:23:37.529653
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:23:38.925969
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:23:40.322200
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:23:41.903055
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:23:43.621084
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:23:45.364904
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:23:47.089038
[batch 1320] samples: 21120, Training Loss: 0.0007
   Time since start: 0:23:48.828994
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:23:50.557827
[batch 1360] samples: 21760, Training Loss: 0.0133
   Time since start: 0:23:52.306002
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:23:54.032031
[batch 1400] samples: 22400, Training Loss: 0.0007
   Time since start: 0:23:55.760511
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:23:57.491410
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:23:59.210175
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:24:00.937243
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:24:02.641876
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:24:04.381289
[batch 1520] samples: 24320, Training Loss: 0.0002
   Time since start: 0:24:06.014829
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:24:07.763018
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:24:09.734444
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:24:11.704305
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:24:13.574974
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:24:15.357609
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:24:17.175551
[batch 1660] samples: 26560, Training Loss: 0.0001
   Time since start: 0:24:19.196800
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:24:21.217942
[batch 1700] samples: 27200, Training Loss: 0.0002
   Time since start: 0:24:23.241674
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:24:25.263482
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:24:27.286450
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:24:29.307878
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:24:31.329018
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:24:33.352186
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:24:35.374939
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:24:37.397344
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:24:39.408276
[batch 1880] samples: 30080, Training Loss: 0.0004
   Time since start: 0:24:41.431483
[batch 1900] samples: 30400, Training Loss: 0.0224
   Time since start: 0:24:43.453424
[batch 1920] samples: 30720, Training Loss: 0.0021
   Time since start: 0:24:45.411369
[batch 1940] samples: 31040, Training Loss: 0.0007
   Time since start: 0:24:46.924228
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:24:48.226358
--m-Epoch 9 done.
   Training Loss: 0.0008
   Validation Loss: 0.0005
Patience decreased: Patience is now  0
Stopping early
     precision    recall  f1-score  support  epoch  class
0     1.000000  1.000000  1.000000   5916.0      1      0
1     1.000000  1.000000  1.000000    378.0      1      1
2     0.999114  1.000000  0.999557   1128.0      1      2
3     1.000000  1.000000  1.000000    420.0      1      3
4     1.000000  1.000000  1.000000    576.0      1      4
..         ...       ...       ...      ...    ...    ...
418   0.986301  1.000000  0.993103     72.0      9     42
419   0.999601  0.998865  0.999233  32592.0      9      0
420   0.998299  0.992526  0.994837  32592.0      9      1
421   0.999605  0.998865  0.999166  32592.0      9      2
422   0.999620  0.998869  0.999175  32592.0      9      3

[423 rows x 6 columns]
device: cuda
Epoch: 1 of 30
[batch 20] samples: 320, Training Loss: 0.6887
   Time since start: 0:00:02.032413
[batch 40] samples: 640, Training Loss: 0.6734
   Time since start: 0:00:03.912927
[batch 60] samples: 960, Training Loss: 0.6507
   Time since start: 0:00:05.783513
[batch 80] samples: 1280, Training Loss: 0.6324
   Time since start: 0:00:07.652945
[batch 100] samples: 1600, Training Loss: 0.6104
   Time since start: 0:00:09.533101
[batch 120] samples: 1920, Training Loss: 0.5771
   Time since start: 0:00:11.413430
[batch 140] samples: 2240, Training Loss: 0.5506
   Time since start: 0:00:13.293063
[batch 160] samples: 2560, Training Loss: 0.4977
   Time since start: 0:00:15.191905
[batch 180] samples: 2880, Training Loss: 0.4797
   Time since start: 0:00:17.053362
[batch 200] samples: 3200, Training Loss: 0.4396
   Time since start: 0:00:18.491152
[batch 220] samples: 3520, Training Loss: 0.4213
   Time since start: 0:00:19.790596
[batch 240] samples: 3840, Training Loss: 0.3810
   Time since start: 0:00:21.093495
[batch 260] samples: 4160, Training Loss: 0.3569
   Time since start: 0:00:22.514115
[batch 280] samples: 4480, Training Loss: 0.3134
   Time since start: 0:00:24.377238
[batch 300] samples: 4800, Training Loss: 0.3204
   Time since start: 0:00:26.250387
[batch 320] samples: 5120, Training Loss: 0.2934
   Time since start: 0:00:27.767291
[batch 340] samples: 5440, Training Loss: 0.2852
   Time since start: 0:00:29.118344
[batch 360] samples: 5760, Training Loss: 0.2605
   Time since start: 0:00:30.468386
[batch 380] samples: 6080, Training Loss: 0.2790
   Time since start: 0:00:32.163456
[batch 400] samples: 6400, Training Loss: 0.2429
   Time since start: 0:00:34.064191
[batch 420] samples: 6720, Training Loss: 0.2272
   Time since start: 0:00:35.974952
[batch 440] samples: 7040, Training Loss: 0.2242
   Time since start: 0:00:37.885112
[batch 460] samples: 7360, Training Loss: 0.2087
   Time since start: 0:00:39.763754
[batch 480] samples: 7680, Training Loss: 0.1980
   Time since start: 0:00:41.622546
[batch 500] samples: 8000, Training Loss: 0.2130
   Time since start: 0:00:43.492279
[batch 520] samples: 8320, Training Loss: 0.1918
   Time since start: 0:00:45.352300
[batch 540] samples: 8640, Training Loss: 0.1878
   Time since start: 0:00:47.233793
[batch 560] samples: 8960, Training Loss: 0.1941
   Time since start: 0:00:49.091472
[batch 580] samples: 9280, Training Loss: 0.1936
   Time since start: 0:00:50.930281
[batch 600] samples: 9600, Training Loss: 0.1932
   Time since start: 0:00:52.709290
[batch 620] samples: 9920, Training Loss: 0.1658
   Time since start: 0:00:54.256578
[batch 640] samples: 10240, Training Loss: 0.1632
   Time since start: 0:00:55.553590
[batch 660] samples: 10560, Training Loss: 0.1720
   Time since start: 0:00:56.850234
[batch 680] samples: 10880, Training Loss: 0.1644
   Time since start: 0:00:58.684585
[batch 700] samples: 11200, Training Loss: 0.1482
   Time since start: 0:01:00.019884
[batch 720] samples: 11520, Training Loss: 0.1396
   Time since start: 0:01:01.323545
[batch 740] samples: 11840, Training Loss: 0.1528
   Time since start: 0:01:02.625074
[batch 760] samples: 12160, Training Loss: 0.1369
   Time since start: 0:01:03.928941
[batch 780] samples: 12480, Training Loss: 0.1439
   Time since start: 0:01:05.232986
[batch 800] samples: 12800, Training Loss: 0.1650
   Time since start: 0:01:06.535944
[batch 820] samples: 13120, Training Loss: 0.1402
   Time since start: 0:01:07.839443
[batch 840] samples: 13440, Training Loss: 0.1316
   Time since start: 0:01:09.142250
[batch 860] samples: 13760, Training Loss: 0.1396
   Time since start: 0:01:10.445603
[batch 880] samples: 14080, Training Loss: 0.1368
   Time since start: 0:01:11.751364
[batch 900] samples: 14400, Training Loss: 0.1314
   Time since start: 0:01:13.056384
[batch 920] samples: 14720, Training Loss: 0.1121
   Time since start: 0:01:14.377148
[batch 940] samples: 15040, Training Loss: 0.1218
   Time since start: 0:01:15.689088
[batch 960] samples: 15360, Training Loss: 0.1363
   Time since start: 0:01:16.997035
[batch 980] samples: 15680, Training Loss: 0.1297
   Time since start: 0:01:18.304844
[batch 1000] samples: 16000, Training Loss: 0.1194
   Time since start: 0:01:19.614376
[batch 1020] samples: 16320, Training Loss: 0.1127
   Time since start: 0:01:20.928162
[batch 1040] samples: 16640, Training Loss: 0.1153
   Time since start: 0:01:22.272088
[batch 1060] samples: 16960, Training Loss: 0.1095
   Time since start: 0:01:24.084249
[batch 1080] samples: 17280, Training Loss: 0.1153
   Time since start: 0:01:25.985337
[batch 1100] samples: 17600, Training Loss: 0.1269
   Time since start: 0:01:27.866085
[batch 1120] samples: 17920, Training Loss: 0.1025
   Time since start: 0:01:29.766328
[batch 1140] samples: 18240, Training Loss: 0.1197
   Time since start: 0:01:31.676284
[batch 1160] samples: 18560, Training Loss: 0.1162
   Time since start: 0:01:33.566568
[batch 1180] samples: 18880, Training Loss: 0.0993
   Time since start: 0:01:35.457456
[batch 1200] samples: 19200, Training Loss: 0.0969
   Time since start: 0:01:37.377346
[batch 1220] samples: 19520, Training Loss: 0.1010
   Time since start: 0:01:39.298325
[batch 1240] samples: 19840, Training Loss: 0.0888
   Time since start: 0:01:41.177498
[batch 1260] samples: 20160, Training Loss: 0.1065
   Time since start: 0:01:43.089984
[batch 1280] samples: 20480, Training Loss: 0.1054
   Time since start: 0:01:45.002340
[batch 1300] samples: 20800, Training Loss: 0.0940
   Time since start: 0:01:46.892900
[batch 1320] samples: 21120, Training Loss: 0.0875
   Time since start: 0:01:48.772797
[batch 1340] samples: 21440, Training Loss: 0.0821
   Time since start: 0:01:50.567933
[batch 1360] samples: 21760, Training Loss: 0.1046
   Time since start: 0:01:51.910096
[batch 1380] samples: 22080, Training Loss: 0.1093
   Time since start: 0:01:53.258126
[batch 1400] samples: 22400, Training Loss: 0.0838
   Time since start: 0:01:54.600467
[batch 1420] samples: 22720, Training Loss: 0.1058
   Time since start: 0:01:56.181874
[batch 1440] samples: 23040, Training Loss: 0.0933
   Time since start: 0:01:58.057362
[batch 1460] samples: 23360, Training Loss: 0.0911
   Time since start: 0:01:59.929376
[batch 1480] samples: 23680, Training Loss: 0.0845
   Time since start: 0:02:01.800380
[batch 1500] samples: 24000, Training Loss: 0.0863
   Time since start: 0:02:03.684899
[batch 1520] samples: 24320, Training Loss: 0.0876
   Time since start: 0:02:05.565227
[batch 1540] samples: 24640, Training Loss: 0.0880
   Time since start: 0:02:07.429882
[batch 1560] samples: 24960, Training Loss: 0.0788
   Time since start: 0:02:09.306396
[batch 1580] samples: 25280, Training Loss: 0.0875
   Time since start: 0:02:11.194176
[batch 1600] samples: 25600, Training Loss: 0.0809
   Time since start: 0:02:13.062114
[batch 1620] samples: 25920, Training Loss: 0.0930
   Time since start: 0:02:14.948419
[batch 1640] samples: 26240, Training Loss: 0.0698
   Time since start: 0:02:16.813919
[batch 1660] samples: 26560, Training Loss: 0.0816
   Time since start: 0:02:18.664505
[batch 1680] samples: 26880, Training Loss: 0.0815
   Time since start: 0:02:20.534627
[batch 1700] samples: 27200, Training Loss: 0.1031
   Time since start: 0:02:22.392084
[batch 1720] samples: 27520, Training Loss: 0.0687
   Time since start: 0:02:24.266611
[batch 1740] samples: 27840, Training Loss: 0.0804
   Time since start: 0:02:26.135442
[batch 1760] samples: 28160, Training Loss: 0.0781
   Time since start: 0:02:27.994764
[batch 1780] samples: 28480, Training Loss: 0.0771
   Time since start: 0:02:29.853195
[batch 1800] samples: 28800, Training Loss: 0.0782
   Time since start: 0:02:31.723935
[batch 1820] samples: 29120, Training Loss: 0.0777
   Time since start: 0:02:33.592463
[batch 1840] samples: 29440, Training Loss: 0.0965
   Time since start: 0:02:35.455944
[batch 1860] samples: 29760, Training Loss: 0.0738
   Time since start: 0:02:37.324214
[batch 1880] samples: 30080, Training Loss: 0.0767
   Time since start: 0:02:39.206222
[batch 1900] samples: 30400, Training Loss: 0.0735
   Time since start: 0:02:41.086421
[batch 1920] samples: 30720, Training Loss: 0.0754
   Time since start: 0:02:42.937983
[batch 1940] samples: 31040, Training Loss: 0.0654
   Time since start: 0:02:44.807055
[batch 1960] samples: 31360, Training Loss: 0.0784
   Time since start: 0:02:46.655599
--m-Epoch 1 done.
   Training Loss: 0.1876
   Validation Loss: 0.0606
Epoch: 2 of 30
[batch 20] samples: 320, Training Loss: 0.0608
   Time since start: 0:02:58.127641
[batch 40] samples: 640, Training Loss: 0.0748
   Time since start: 0:02:59.578254
[batch 60] samples: 960, Training Loss: 0.0703
   Time since start: 0:03:00.891953
[batch 80] samples: 1280, Training Loss: 0.0620
   Time since start: 0:03:02.635597
[batch 100] samples: 1600, Training Loss: 0.0512
   Time since start: 0:03:04.606394
[batch 120] samples: 1920, Training Loss: 0.0572
   Time since start: 0:03:06.568087
[batch 140] samples: 2240, Training Loss: 0.0657
   Time since start: 0:03:08.539479
[batch 160] samples: 2560, Training Loss: 0.0644
   Time since start: 0:03:10.562047
[batch 180] samples: 2880, Training Loss: 0.0659
   Time since start: 0:03:12.585272
[batch 200] samples: 3200, Training Loss: 0.0691
   Time since start: 0:03:14.607690
[batch 220] samples: 3520, Training Loss: 0.0657
   Time since start: 0:03:16.629432
[batch 240] samples: 3840, Training Loss: 0.0650
   Time since start: 0:03:18.631181
[batch 260] samples: 4160, Training Loss: 0.0636
   Time since start: 0:03:20.653419
[batch 280] samples: 4480, Training Loss: 0.0557
   Time since start: 0:03:22.676909
[batch 300] samples: 4800, Training Loss: 0.0606
   Time since start: 0:03:24.708208
[batch 320] samples: 5120, Training Loss: 0.0662
   Time since start: 0:03:26.729203
[batch 340] samples: 5440, Training Loss: 0.0523
   Time since start: 0:03:28.752416
[batch 360] samples: 5760, Training Loss: 0.0611
   Time since start: 0:03:30.774765
[batch 380] samples: 6080, Training Loss: 0.0511
   Time since start: 0:03:32.796133
[batch 400] samples: 6400, Training Loss: 0.0855
   Time since start: 0:03:34.819456
[batch 420] samples: 6720, Training Loss: 0.0537
   Time since start: 0:03:36.842067
[batch 440] samples: 7040, Training Loss: 0.0452
   Time since start: 0:03:38.862824
[batch 460] samples: 7360, Training Loss: 0.0519
   Time since start: 0:03:40.883809
[batch 480] samples: 7680, Training Loss: 0.0437
   Time since start: 0:03:42.904628
[batch 500] samples: 8000, Training Loss: 0.0576
   Time since start: 0:03:44.926121
[batch 520] samples: 8320, Training Loss: 0.0504
   Time since start: 0:03:46.949525
[batch 540] samples: 8640, Training Loss: 0.0535
   Time since start: 0:03:48.970034
[batch 560] samples: 8960, Training Loss: 0.0617
   Time since start: 0:03:50.990347
[batch 580] samples: 9280, Training Loss: 0.0563
   Time since start: 0:03:53.011713
[batch 600] samples: 9600, Training Loss: 0.0370
   Time since start: 0:03:55.033293
[batch 620] samples: 9920, Training Loss: 0.0492
   Time since start: 0:03:57.055073
[batch 640] samples: 10240, Training Loss: 0.0546
   Time since start: 0:03:59.076461
[batch 660] samples: 10560, Training Loss: 0.0477
   Time since start: 0:04:01.098634
[batch 680] samples: 10880, Training Loss: 0.0392
   Time since start: 0:04:03.119634
[batch 700] samples: 11200, Training Loss: 0.0516
   Time since start: 0:04:05.150227
[batch 720] samples: 11520, Training Loss: 0.0472
   Time since start: 0:04:07.170958
[batch 740] samples: 11840, Training Loss: 0.0318
   Time since start: 0:04:09.192452
[batch 760] samples: 12160, Training Loss: 0.0436
   Time since start: 0:04:11.213448
[batch 780] samples: 12480, Training Loss: 0.0463
   Time since start: 0:04:13.204582
[batch 800] samples: 12800, Training Loss: 0.0620
   Time since start: 0:04:15.196278
[batch 820] samples: 13120, Training Loss: 0.0423
   Time since start: 0:04:17.106154
[batch 840] samples: 13440, Training Loss: 0.0491
   Time since start: 0:04:19.066787
[batch 860] samples: 13760, Training Loss: 0.0439
   Time since start: 0:04:21.039747
[batch 880] samples: 14080, Training Loss: 0.0332
   Time since start: 0:04:23.011128
[batch 900] samples: 14400, Training Loss: 0.0428
   Time since start: 0:04:24.981236
[batch 920] samples: 14720, Training Loss: 0.0476
   Time since start: 0:04:26.953837
[batch 940] samples: 15040, Training Loss: 0.0452
   Time since start: 0:04:28.927561
[batch 960] samples: 15360, Training Loss: 0.0356
   Time since start: 0:04:30.567330
[batch 980] samples: 15680, Training Loss: 0.0373
   Time since start: 0:04:32.398033
[batch 1000] samples: 16000, Training Loss: 0.0443
   Time since start: 0:04:34.218642
[batch 1020] samples: 16320, Training Loss: 0.0341
   Time since start: 0:04:35.844120
[batch 1040] samples: 16640, Training Loss: 0.0488
   Time since start: 0:04:37.158430
[batch 1060] samples: 16960, Training Loss: 0.0540
   Time since start: 0:04:38.473017
[batch 1080] samples: 17280, Training Loss: 0.0342
   Time since start: 0:04:39.786616
[batch 1100] samples: 17600, Training Loss: 0.0461
   Time since start: 0:04:41.117244
[batch 1120] samples: 17920, Training Loss: 0.0434
   Time since start: 0:04:43.084152
[batch 1140] samples: 18240, Training Loss: 0.0347
   Time since start: 0:04:44.734437
[batch 1160] samples: 18560, Training Loss: 0.0335
   Time since start: 0:04:46.172356
[batch 1180] samples: 18880, Training Loss: 0.0293
   Time since start: 0:04:47.799486
[batch 1200] samples: 19200, Training Loss: 0.0326
   Time since start: 0:04:49.710839
[batch 1220] samples: 19520, Training Loss: 0.0383
   Time since start: 0:04:51.608362
[batch 1240] samples: 19840, Training Loss: 0.0270
   Time since start: 0:04:53.502040
[batch 1260] samples: 20160, Training Loss: 0.0327
   Time since start: 0:04:55.399103
[batch 1280] samples: 20480, Training Loss: 0.0338
   Time since start: 0:04:57.409031
[batch 1300] samples: 20800, Training Loss: 0.0306
   Time since start: 0:04:59.312498
[batch 1320] samples: 21120, Training Loss: 0.0252
   Time since start: 0:05:01.209637
[batch 1340] samples: 21440, Training Loss: 0.0597
   Time since start: 0:05:03.108318
[batch 1360] samples: 21760, Training Loss: 0.0393
   Time since start: 0:05:05.001734
[batch 1380] samples: 22080, Training Loss: 0.0367
   Time since start: 0:05:06.896487
[batch 1400] samples: 22400, Training Loss: 0.0251
   Time since start: 0:05:08.400793
[batch 1420] samples: 22720, Training Loss: 0.0337
   Time since start: 0:05:10.077102
[batch 1440] samples: 23040, Training Loss: 0.0313
   Time since start: 0:05:11.982300
[batch 1460] samples: 23360, Training Loss: 0.0244
   Time since start: 0:05:13.882107
[batch 1480] samples: 23680, Training Loss: 0.0282
   Time since start: 0:05:15.822137
[batch 1500] samples: 24000, Training Loss: 0.0307
   Time since start: 0:05:17.783048
[batch 1520] samples: 24320, Training Loss: 0.0240
   Time since start: 0:05:19.734525
[batch 1540] samples: 24640, Training Loss: 0.0331
   Time since start: 0:05:21.677115
[batch 1560] samples: 24960, Training Loss: 0.0251
   Time since start: 0:05:23.627547
[batch 1580] samples: 25280, Training Loss: 0.0410
   Time since start: 0:05:25.457431
[batch 1600] samples: 25600, Training Loss: 0.0423
   Time since start: 0:05:26.733941
[batch 1620] samples: 25920, Training Loss: 0.0292
   Time since start: 0:05:28.009452
[batch 1640] samples: 26240, Training Loss: 0.0392
   Time since start: 0:05:29.281775
[batch 1660] samples: 26560, Training Loss: 0.0235
   Time since start: 0:05:30.550798
[batch 1680] samples: 26880, Training Loss: 0.0242
   Time since start: 0:05:31.820651
[batch 1700] samples: 27200, Training Loss: 0.0347
   Time since start: 0:05:33.092654
[batch 1720] samples: 27520, Training Loss: 0.0304
   Time since start: 0:05:34.363232
[batch 1740] samples: 27840, Training Loss: 0.0267
   Time since start: 0:05:35.634679
[batch 1760] samples: 28160, Training Loss: 0.0262
   Time since start: 0:05:36.904397
[batch 1780] samples: 28480, Training Loss: 0.0281
   Time since start: 0:05:38.174773
[batch 1800] samples: 28800, Training Loss: 0.0263
   Time since start: 0:05:39.445397
[batch 1820] samples: 29120, Training Loss: 0.0369
   Time since start: 0:05:40.716121
[batch 1840] samples: 29440, Training Loss: 0.0256
   Time since start: 0:05:41.986752
[batch 1860] samples: 29760, Training Loss: 0.0197
   Time since start: 0:05:43.255614
[batch 1880] samples: 30080, Training Loss: 0.0210
   Time since start: 0:05:44.526463
[batch 1900] samples: 30400, Training Loss: 0.0215
   Time since start: 0:05:45.796454
[batch 1920] samples: 30720, Training Loss: 0.0256
   Time since start: 0:05:47.080854
[batch 1940] samples: 31040, Training Loss: 0.0237
   Time since start: 0:05:48.351710
[batch 1960] samples: 31360, Training Loss: 0.0179
   Time since start: 0:05:49.606556
--m-Epoch 2 done.
   Training Loss: 0.0429
   Validation Loss: 0.0174
Epoch: 3 of 30
[batch 20] samples: 320, Training Loss: 0.0306
   Time since start: 0:06:03.053417
[batch 40] samples: 640, Training Loss: 0.0214
   Time since start: 0:06:04.665373
[batch 60] samples: 960, Training Loss: 0.0301
   Time since start: 0:06:06.049442
[batch 80] samples: 1280, Training Loss: 0.0236
   Time since start: 0:06:07.663671
[batch 100] samples: 1600, Training Loss: 0.0324
   Time since start: 0:06:09.615278
[batch 120] samples: 1920, Training Loss: 0.0276
   Time since start: 0:06:11.584177
[batch 140] samples: 2240, Training Loss: 0.0221
   Time since start: 0:06:13.541136
[batch 160] samples: 2560, Training Loss: 0.0188
   Time since start: 0:06:15.484383
[batch 180] samples: 2880, Training Loss: 0.0234
   Time since start: 0:06:17.385159
[batch 200] samples: 3200, Training Loss: 0.0172
   Time since start: 0:06:19.346263
[batch 220] samples: 3520, Training Loss: 0.0155
   Time since start: 0:06:21.310115
[batch 240] samples: 3840, Training Loss: 0.0190
   Time since start: 0:06:23.278691
[batch 260] samples: 4160, Training Loss: 0.0202
   Time since start: 0:06:25.237405
[batch 280] samples: 4480, Training Loss: 0.0254
   Time since start: 0:06:27.167735
[batch 300] samples: 4800, Training Loss: 0.0171
   Time since start: 0:06:29.139961
[batch 320] samples: 5120, Training Loss: 0.0214
   Time since start: 0:06:31.088461
[batch 340] samples: 5440, Training Loss: 0.0159
   Time since start: 0:06:33.044416
[batch 360] samples: 5760, Training Loss: 0.0165
   Time since start: 0:06:35.005138
[batch 380] samples: 6080, Training Loss: 0.0183
   Time since start: 0:06:36.998511
[batch 400] samples: 6400, Training Loss: 0.0177
   Time since start: 0:06:39.009549
[batch 420] samples: 6720, Training Loss: 0.0158
   Time since start: 0:06:41.031719
[batch 440] samples: 7040, Training Loss: 0.0183
   Time since start: 0:06:43.053717
[batch 460] samples: 7360, Training Loss: 0.0206
   Time since start: 0:06:45.059208
[batch 480] samples: 7680, Training Loss: 0.0153
   Time since start: 0:06:47.065633
[batch 500] samples: 8000, Training Loss: 0.0117
   Time since start: 0:06:49.088952
[batch 520] samples: 8320, Training Loss: 0.0117
   Time since start: 0:06:51.111297
[batch 540] samples: 8640, Training Loss: 0.0129
   Time since start: 0:06:53.134191
[batch 560] samples: 8960, Training Loss: 0.0134
   Time since start: 0:06:55.279125
[batch 580] samples: 9280, Training Loss: 0.0150
   Time since start: 0:06:56.670978
[batch 600] samples: 9600, Training Loss: 0.0171
   Time since start: 0:06:58.514726
[batch 620] samples: 9920, Training Loss: 0.0160
   Time since start: 0:07:00.537959
[batch 640] samples: 10240, Training Loss: 0.0186
   Time since start: 0:07:02.560401
[batch 660] samples: 10560, Training Loss: 0.0180
   Time since start: 0:07:04.583637
[batch 680] samples: 10880, Training Loss: 0.0188
   Time since start: 0:07:06.336919
[batch 700] samples: 11200, Training Loss: 0.0274
   Time since start: 0:07:07.722778
[batch 720] samples: 11520, Training Loss: 0.0151
   Time since start: 0:07:09.108392
[batch 740] samples: 11840, Training Loss: 0.0134
   Time since start: 0:07:10.493999
[batch 760] samples: 12160, Training Loss: 0.0110
   Time since start: 0:07:11.880600
[batch 780] samples: 12480, Training Loss: 0.0148
   Time since start: 0:07:13.269056
[batch 800] samples: 12800, Training Loss: 0.0139
   Time since start: 0:07:14.666664
[batch 820] samples: 13120, Training Loss: 0.0154
   Time since start: 0:07:16.056979
[batch 840] samples: 13440, Training Loss: 0.0100
   Time since start: 0:07:17.450729
[batch 860] samples: 13760, Training Loss: 0.0177
   Time since start: 0:07:18.982375
[batch 880] samples: 14080, Training Loss: 0.0156
   Time since start: 0:07:20.714471
[batch 900] samples: 14400, Training Loss: 0.0120
   Time since start: 0:07:22.440272
[batch 920] samples: 14720, Training Loss: 0.0111
   Time since start: 0:07:24.185813
[batch 940] samples: 15040, Training Loss: 0.0127
   Time since start: 0:07:25.916634
[batch 960] samples: 15360, Training Loss: 0.0102
   Time since start: 0:07:27.661018
[batch 980] samples: 15680, Training Loss: 0.0201
   Time since start: 0:07:29.371164
[batch 1000] samples: 16000, Training Loss: 0.0179
   Time since start: 0:07:31.077399
[batch 1020] samples: 16320, Training Loss: 0.0223
   Time since start: 0:07:32.795775
[batch 1040] samples: 16640, Training Loss: 0.0122
   Time since start: 0:07:34.505287
[batch 1060] samples: 16960, Training Loss: 0.0089
   Time since start: 0:07:36.234496
[batch 1080] samples: 17280, Training Loss: 0.0093
   Time since start: 0:07:37.982324
[batch 1100] samples: 17600, Training Loss: 0.0162
   Time since start: 0:07:39.717423
[batch 1120] samples: 17920, Training Loss: 0.0113
   Time since start: 0:07:41.439223
[batch 1140] samples: 18240, Training Loss: 0.0114
   Time since start: 0:07:43.158496
[batch 1160] samples: 18560, Training Loss: 0.0082
   Time since start: 0:07:44.845933
[batch 1180] samples: 18880, Training Loss: 0.0111
   Time since start: 0:07:46.607188
[batch 1200] samples: 19200, Training Loss: 0.0243
   Time since start: 0:07:48.395726
[batch 1220] samples: 19520, Training Loss: 0.0183
   Time since start: 0:07:50.204178
[batch 1240] samples: 19840, Training Loss: 0.0095
   Time since start: 0:07:52.007056
[batch 1260] samples: 20160, Training Loss: 0.0138
   Time since start: 0:07:53.818579
[batch 1280] samples: 20480, Training Loss: 0.0082
   Time since start: 0:07:55.597661
[batch 1300] samples: 20800, Training Loss: 0.0104
   Time since start: 0:07:57.400586
[batch 1320] samples: 21120, Training Loss: 0.0142
   Time since start: 0:07:59.212794
[batch 1340] samples: 21440, Training Loss: 0.0121
   Time since start: 0:08:01.077622
[batch 1360] samples: 21760, Training Loss: 0.0105
   Time since start: 0:08:02.912588
[batch 1380] samples: 22080, Training Loss: 0.0191
   Time since start: 0:08:04.705553
[batch 1400] samples: 22400, Training Loss: 0.0130
   Time since start: 0:08:06.511473
[batch 1420] samples: 22720, Training Loss: 0.0141
   Time since start: 0:08:08.331182
[batch 1440] samples: 23040, Training Loss: 0.0133
   Time since start: 0:08:10.139934
[batch 1460] samples: 23360, Training Loss: 0.0089
   Time since start: 0:08:11.949954
[batch 1480] samples: 23680, Training Loss: 0.0204
   Time since start: 0:08:13.750261
[batch 1500] samples: 24000, Training Loss: 0.0089
   Time since start: 0:08:15.683869
[batch 1520] samples: 24320, Training Loss: 0.0130
   Time since start: 0:08:17.704011
[batch 1540] samples: 24640, Training Loss: 0.0109
   Time since start: 0:08:19.706915
[batch 1560] samples: 24960, Training Loss: 0.0067
   Time since start: 0:08:21.718841
[batch 1580] samples: 25280, Training Loss: 0.0143
   Time since start: 0:08:23.691121
[batch 1600] samples: 25600, Training Loss: 0.0152
   Time since start: 0:08:25.641685
[batch 1620] samples: 25920, Training Loss: 0.0094
   Time since start: 0:08:27.593114
[batch 1640] samples: 26240, Training Loss: 0.0120
   Time since start: 0:08:29.552654
[batch 1660] samples: 26560, Training Loss: 0.0148
   Time since start: 0:08:31.524369
[batch 1680] samples: 26880, Training Loss: 0.0111
   Time since start: 0:08:33.474537
[batch 1700] samples: 27200, Training Loss: 0.0101
   Time since start: 0:08:35.436863
[batch 1720] samples: 27520, Training Loss: 0.0082
   Time since start: 0:08:37.389968
[batch 1740] samples: 27840, Training Loss: 0.0068
   Time since start: 0:08:39.343232
[batch 1760] samples: 28160, Training Loss: 0.0071
   Time since start: 0:08:41.303480
[batch 1780] samples: 28480, Training Loss: 0.0223
   Time since start: 0:08:43.265763
[batch 1800] samples: 28800, Training Loss: 0.0149
   Time since start: 0:08:45.338464
[batch 1820] samples: 29120, Training Loss: 0.0066
   Time since start: 0:08:47.281474
[batch 1840] samples: 29440, Training Loss: 0.0096
   Time since start: 0:08:49.243854
[batch 1860] samples: 29760, Training Loss: 0.0059
   Time since start: 0:08:51.215069
[batch 1880] samples: 30080, Training Loss: 0.0055
   Time since start: 0:08:53.165252
[batch 1900] samples: 30400, Training Loss: 0.0075
   Time since start: 0:08:55.126432
[batch 1920] samples: 30720, Training Loss: 0.0115
   Time since start: 0:08:57.089531
[batch 1940] samples: 31040, Training Loss: 0.0065
   Time since start: 0:08:59.040963
[batch 1960] samples: 31360, Training Loss: 0.0082
   Time since start: 0:09:00.962079
--m-Epoch 3 done.
   Training Loss: 0.0152
   Validation Loss: 0.0055
Epoch: 4 of 30
[batch 20] samples: 320, Training Loss: 0.0079
   Time since start: 0:09:13.757598
[batch 40] samples: 640, Training Loss: 0.0105
   Time since start: 0:09:15.784757
[batch 60] samples: 960, Training Loss: 0.0171
   Time since start: 0:09:17.698655
[batch 80] samples: 1280, Training Loss: 0.0073
   Time since start: 0:09:19.596199
[batch 100] samples: 1600, Training Loss: 0.0066
   Time since start: 0:09:21.467698
[batch 120] samples: 1920, Training Loss: 0.0079
   Time since start: 0:09:23.340143
[batch 140] samples: 2240, Training Loss: 0.0055
   Time since start: 0:09:25.241870
[batch 160] samples: 2560, Training Loss: 0.0047
   Time since start: 0:09:26.962002
[batch 180] samples: 2880, Training Loss: 0.0070
   Time since start: 0:09:28.325155
[batch 200] samples: 3200, Training Loss: 0.0115
   Time since start: 0:09:29.686652
[batch 220] samples: 3520, Training Loss: 0.0076
   Time since start: 0:09:31.048592
[batch 240] samples: 3840, Training Loss: 0.0083
   Time since start: 0:09:32.409490
[batch 260] samples: 4160, Training Loss: 0.0147
   Time since start: 0:09:33.829199
[batch 280] samples: 4480, Training Loss: 0.0051
   Time since start: 0:09:35.804335
[batch 300] samples: 4800, Training Loss: 0.0086
   Time since start: 0:09:37.769429
[batch 320] samples: 5120, Training Loss: 0.0142
   Time since start: 0:09:39.745305
[batch 340] samples: 5440, Training Loss: 0.0042
   Time since start: 0:09:41.665909
[batch 360] samples: 5760, Training Loss: 0.0055
   Time since start: 0:09:43.203878
[batch 380] samples: 6080, Training Loss: 0.0087
   Time since start: 0:09:44.618463
[batch 400] samples: 6400, Training Loss: 0.0057
   Time since start: 0:09:45.989491
[batch 420] samples: 6720, Training Loss: 0.0099
   Time since start: 0:09:47.370697
[batch 440] samples: 7040, Training Loss: 0.0111
   Time since start: 0:09:48.759399
[batch 460] samples: 7360, Training Loss: 0.0058
   Time since start: 0:09:50.157459
[batch 480] samples: 7680, Training Loss: 0.0065
   Time since start: 0:09:51.544345
[batch 500] samples: 8000, Training Loss: 0.0053
   Time since start: 0:09:53.156447
[batch 520] samples: 8320, Training Loss: 0.0086
   Time since start: 0:09:54.950577
[batch 540] samples: 8640, Training Loss: 0.0053
   Time since start: 0:09:56.809170
[batch 560] samples: 8960, Training Loss: 0.0070
   Time since start: 0:09:58.690897
[batch 580] samples: 9280, Training Loss: 0.0091
   Time since start: 0:10:00.559301
[batch 600] samples: 9600, Training Loss: 0.0068
   Time since start: 0:10:02.428517
[batch 620] samples: 9920, Training Loss: 0.0049
   Time since start: 0:10:04.298078
[batch 640] samples: 10240, Training Loss: 0.0057
   Time since start: 0:10:06.178992
[batch 660] samples: 10560, Training Loss: 0.0077
   Time since start: 0:10:08.077075
[batch 680] samples: 10880, Training Loss: 0.0059
   Time since start: 0:10:10.045593
[batch 700] samples: 11200, Training Loss: 0.0044
   Time since start: 0:10:12.000160
[batch 720] samples: 11520, Training Loss: 0.0044
   Time since start: 0:10:13.961454
[batch 740] samples: 11840, Training Loss: 0.0067
   Time since start: 0:10:15.600257
[batch 760] samples: 12160, Training Loss: 0.0072
   Time since start: 0:10:17.571439
[batch 780] samples: 12480, Training Loss: 0.0039
   Time since start: 0:10:19.531803
[batch 800] samples: 12800, Training Loss: 0.0055
   Time since start: 0:10:21.504041
[batch 820] samples: 13120, Training Loss: 0.0071
   Time since start: 0:10:23.454093
[batch 840] samples: 13440, Training Loss: 0.0051
   Time since start: 0:10:25.422060
[batch 860] samples: 13760, Training Loss: 0.0037
   Time since start: 0:10:26.876492
[batch 880] samples: 14080, Training Loss: 0.0061
   Time since start: 0:10:28.263212
[batch 900] samples: 14400, Training Loss: 0.0042
   Time since start: 0:10:29.651507
[batch 920] samples: 14720, Training Loss: 0.0037
   Time since start: 0:10:31.038499
[batch 940] samples: 15040, Training Loss: 0.0036
   Time since start: 0:10:32.425209
[batch 960] samples: 15360, Training Loss: 0.0040
   Time since start: 0:10:33.809939
[batch 980] samples: 15680, Training Loss: 0.0049
   Time since start: 0:10:35.194716
[batch 1000] samples: 16000, Training Loss: 0.0035
   Time since start: 0:10:36.745790
[batch 1020] samples: 16320, Training Loss: 0.0044
   Time since start: 0:10:38.541530
[batch 1040] samples: 16640, Training Loss: 0.0030
   Time since start: 0:10:40.361538
[batch 1060] samples: 16960, Training Loss: 0.0045
   Time since start: 0:10:42.428781
[batch 1080] samples: 17280, Training Loss: 0.0047
   Time since start: 0:10:44.379241
[batch 1100] samples: 17600, Training Loss: 0.0045
   Time since start: 0:10:46.340388
[batch 1120] samples: 17920, Training Loss: 0.0037
   Time since start: 0:10:48.306973
[batch 1140] samples: 18240, Training Loss: 0.0037
   Time since start: 0:10:49.784062
[batch 1160] samples: 18560, Training Loss: 0.0056
   Time since start: 0:10:51.633100
[batch 1180] samples: 18880, Training Loss: 0.0038
   Time since start: 0:10:53.614594
[batch 1200] samples: 19200, Training Loss: 0.0029
   Time since start: 0:10:55.574539
[batch 1220] samples: 19520, Training Loss: 0.0026
   Time since start: 0:10:57.536987
[batch 1240] samples: 19840, Training Loss: 0.0033
   Time since start: 0:10:59.494914
[batch 1260] samples: 20160, Training Loss: 0.0065
   Time since start: 0:11:01.457748
[batch 1280] samples: 20480, Training Loss: 0.0044
   Time since start: 0:11:03.428732
[batch 1300] samples: 20800, Training Loss: 0.0078
   Time since start: 0:11:05.397641
[batch 1320] samples: 21120, Training Loss: 0.0049
   Time since start: 0:11:07.359809
[batch 1340] samples: 21440, Training Loss: 0.0059
   Time since start: 0:11:09.311440
[batch 1360] samples: 21760, Training Loss: 0.0061
   Time since start: 0:11:11.259360
[batch 1380] samples: 22080, Training Loss: 0.0086
   Time since start: 0:11:13.234777
[batch 1400] samples: 22400, Training Loss: 0.0059
   Time since start: 0:11:15.210986
[batch 1420] samples: 22720, Training Loss: 0.0149
   Time since start: 0:11:17.168385
[batch 1440] samples: 23040, Training Loss: 0.0023
   Time since start: 0:11:19.116850
[batch 1460] samples: 23360, Training Loss: 0.0039
   Time since start: 0:11:21.068978
[batch 1480] samples: 23680, Training Loss: 0.0029
   Time since start: 0:11:23.022279
[batch 1500] samples: 24000, Training Loss: 0.0071
   Time since start: 0:11:24.989510
[batch 1520] samples: 24320, Training Loss: 0.0037
   Time since start: 0:11:26.961420
[batch 1540] samples: 24640, Training Loss: 0.0024
   Time since start: 0:11:28.924952
[batch 1560] samples: 24960, Training Loss: 0.0032
   Time since start: 0:11:30.893862
[batch 1580] samples: 25280, Training Loss: 0.0042
   Time since start: 0:11:32.899842
[batch 1600] samples: 25600, Training Loss: 0.0050
   Time since start: 0:11:34.919227
[batch 1620] samples: 25920, Training Loss: 0.0058
   Time since start: 0:11:36.940859
[batch 1640] samples: 26240, Training Loss: 0.0055
   Time since start: 0:11:38.964161
[batch 1660] samples: 26560, Training Loss: 0.0039
   Time since start: 0:11:40.985301
[batch 1680] samples: 26880, Training Loss: 0.0027
   Time since start: 0:11:43.006073
[batch 1700] samples: 27200, Training Loss: 0.0026
   Time since start: 0:11:45.026869
[batch 1720] samples: 27520, Training Loss: 0.0045
   Time since start: 0:11:47.048989
[batch 1740] samples: 27840, Training Loss: 0.0179
   Time since start: 0:11:49.070148
[batch 1760] samples: 28160, Training Loss: 0.0024
   Time since start: 0:11:51.091016
[batch 1780] samples: 28480, Training Loss: 0.0064
   Time since start: 0:11:53.113526
[batch 1800] samples: 28800, Training Loss: 0.0023
   Time since start: 0:11:55.135134
[batch 1820] samples: 29120, Training Loss: 0.0036
   Time since start: 0:11:57.156230
[batch 1840] samples: 29440, Training Loss: 0.0032
   Time since start: 0:11:59.179376
[batch 1860] samples: 29760, Training Loss: 0.0077
   Time since start: 0:12:01.201185
[batch 1880] samples: 30080, Training Loss: 0.0045
   Time since start: 0:12:03.223838
[batch 1900] samples: 30400, Training Loss: 0.0044
   Time since start: 0:12:05.243719
[batch 1920] samples: 30720, Training Loss: 0.0038
   Time since start: 0:12:07.263955
[batch 1940] samples: 31040, Training Loss: 0.0035
   Time since start: 0:12:09.287308
[batch 1960] samples: 31360, Training Loss: 0.0039
   Time since start: 0:12:11.237423
--m-Epoch 4 done.
   Training Loss: 0.0058
   Validation Loss: 0.0019
Epoch: 5 of 30
[batch 20] samples: 320, Training Loss: 0.0053
   Time since start: 0:12:24.493102
[batch 40] samples: 640, Training Loss: 0.0022
   Time since start: 0:12:26.316702
[batch 60] samples: 960, Training Loss: 0.0020
   Time since start: 0:12:28.152483
[batch 80] samples: 1280, Training Loss: 0.0020
   Time since start: 0:12:29.971324
[batch 100] samples: 1600, Training Loss: 0.0045
   Time since start: 0:12:31.375289
[batch 120] samples: 1920, Training Loss: 0.0042
   Time since start: 0:12:32.692428
[batch 140] samples: 2240, Training Loss: 0.0033
   Time since start: 0:12:34.022721
[batch 160] samples: 2560, Training Loss: 0.0025
   Time since start: 0:12:35.841503
[batch 180] samples: 2880, Training Loss: 0.0019
   Time since start: 0:12:37.640833
[batch 200] samples: 3200, Training Loss: 0.0019
   Time since start: 0:12:39.188697
[batch 220] samples: 3520, Training Loss: 0.0027
   Time since start: 0:12:40.495791
[batch 240] samples: 3840, Training Loss: 0.0033
   Time since start: 0:12:41.804518
[batch 260] samples: 4160, Training Loss: 0.0028
   Time since start: 0:12:43.111354
[batch 280] samples: 4480, Training Loss: 0.0066
   Time since start: 0:12:44.422545
[batch 300] samples: 4800, Training Loss: 0.0033
   Time since start: 0:12:45.730405
[batch 320] samples: 5120, Training Loss: 0.0029
   Time since start: 0:12:47.040135
[batch 340] samples: 5440, Training Loss: 0.0027
   Time since start: 0:12:48.475130
[batch 360] samples: 5760, Training Loss: 0.0045
   Time since start: 0:12:49.788777
[batch 380] samples: 6080, Training Loss: 0.0030
   Time since start: 0:12:51.100708
[batch 400] samples: 6400, Training Loss: 0.0025
   Time since start: 0:12:52.412431
[batch 420] samples: 6720, Training Loss: 0.0068
   Time since start: 0:12:53.730609
[batch 440] samples: 7040, Training Loss: 0.0023
   Time since start: 0:12:55.044418
[batch 460] samples: 7360, Training Loss: 0.0025
   Time since start: 0:12:56.357664
[batch 480] samples: 7680, Training Loss: 0.0023
   Time since start: 0:12:57.664755
[batch 500] samples: 8000, Training Loss: 0.0019
   Time since start: 0:12:58.978687
[batch 520] samples: 8320, Training Loss: 0.0017
   Time since start: 0:13:00.292544
[batch 540] samples: 8640, Training Loss: 0.0029
   Time since start: 0:13:01.606509
[batch 560] samples: 8960, Training Loss: 0.0021
   Time since start: 0:13:02.934134
[batch 580] samples: 9280, Training Loss: 0.0016
   Time since start: 0:13:04.287259
[batch 600] samples: 9600, Training Loss: 0.0058
   Time since start: 0:13:05.639922
[batch 620] samples: 9920, Training Loss: 0.0029
   Time since start: 0:13:06.995459
[batch 640] samples: 10240, Training Loss: 0.0019
   Time since start: 0:13:08.353655
[batch 660] samples: 10560, Training Loss: 0.0056
   Time since start: 0:13:09.711128
[batch 680] samples: 10880, Training Loss: 0.0018
   Time since start: 0:13:11.066993
[batch 700] samples: 11200, Training Loss: 0.0024
   Time since start: 0:13:12.419800
[batch 720] samples: 11520, Training Loss: 0.0021
   Time since start: 0:13:14.305599
[batch 740] samples: 11840, Training Loss: 0.0038
   Time since start: 0:13:16.210969
[batch 760] samples: 12160, Training Loss: 0.0032
   Time since start: 0:13:17.981883
[batch 780] samples: 12480, Training Loss: 0.0031
   Time since start: 0:13:19.614576
[batch 800] samples: 12800, Training Loss: 0.0023
   Time since start: 0:13:21.513845
[batch 820] samples: 13120, Training Loss: 0.0065
   Time since start: 0:13:23.414089
[batch 840] samples: 13440, Training Loss: 0.0016
   Time since start: 0:13:25.307027
[batch 860] samples: 13760, Training Loss: 0.0046
   Time since start: 0:13:26.870195
[batch 880] samples: 14080, Training Loss: 0.0036
   Time since start: 0:13:28.221657
[batch 900] samples: 14400, Training Loss: 0.0025
   Time since start: 0:13:29.573098
[batch 920] samples: 14720, Training Loss: 0.0023
   Time since start: 0:13:30.922528
[batch 940] samples: 15040, Training Loss: 0.0032
   Time since start: 0:13:32.272711
[batch 960] samples: 15360, Training Loss: 0.0012
   Time since start: 0:13:33.622691
[batch 980] samples: 15680, Training Loss: 0.0022
   Time since start: 0:13:35.199488
[batch 1000] samples: 16000, Training Loss: 0.0025
   Time since start: 0:13:37.094515
[batch 1020] samples: 16320, Training Loss: 0.0015
   Time since start: 0:13:39.005189
[batch 1040] samples: 16640, Training Loss: 0.0021
   Time since start: 0:13:40.897623
[batch 1060] samples: 16960, Training Loss: 0.0035
   Time since start: 0:13:42.390660
[batch 1080] samples: 17280, Training Loss: 0.0020
   Time since start: 0:13:43.741462
[batch 1100] samples: 17600, Training Loss: 0.0017
   Time since start: 0:13:45.424928
[batch 1120] samples: 17920, Training Loss: 0.0023
   Time since start: 0:13:47.242983
[batch 1140] samples: 18240, Training Loss: 0.0022
   Time since start: 0:13:49.062985
[batch 1160] samples: 18560, Training Loss: 0.0046
   Time since start: 0:13:50.881791
[batch 1180] samples: 18880, Training Loss: 0.0016
   Time since start: 0:13:52.713562
[batch 1200] samples: 19200, Training Loss: 0.0028
   Time since start: 0:13:54.526576
[batch 1220] samples: 19520, Training Loss: 0.0013
   Time since start: 0:13:55.853115
[batch 1240] samples: 19840, Training Loss: 0.0016
   Time since start: 0:13:57.171901
[batch 1260] samples: 20160, Training Loss: 0.0015
   Time since start: 0:13:58.490988
[batch 1280] samples: 20480, Training Loss: 0.0018
   Time since start: 0:13:59.811556
[batch 1300] samples: 20800, Training Loss: 0.0016
   Time since start: 0:14:01.131632
[batch 1320] samples: 21120, Training Loss: 0.0013
   Time since start: 0:14:02.450684
[batch 1340] samples: 21440, Training Loss: 0.0015
   Time since start: 0:14:03.773864
[batch 1360] samples: 21760, Training Loss: 0.0011
   Time since start: 0:14:05.099234
[batch 1380] samples: 22080, Training Loss: 0.0020
   Time since start: 0:14:06.422047
[batch 1400] samples: 22400, Training Loss: 0.0023
   Time since start: 0:14:07.739703
[batch 1420] samples: 22720, Training Loss: 0.0017
   Time since start: 0:14:09.060150
[batch 1440] samples: 23040, Training Loss: 0.0013
   Time since start: 0:14:10.379405
[batch 1460] samples: 23360, Training Loss: 0.0011
   Time since start: 0:14:11.699668
[batch 1480] samples: 23680, Training Loss: 0.0029
   Time since start: 0:14:13.020120
[batch 1500] samples: 24000, Training Loss: 0.0024
   Time since start: 0:14:14.366096
[batch 1520] samples: 24320, Training Loss: 0.0028
   Time since start: 0:14:15.728926
[batch 1540] samples: 24640, Training Loss: 0.0012
   Time since start: 0:14:17.123544
[batch 1560] samples: 24960, Training Loss: 0.0017
   Time since start: 0:14:18.958386
[batch 1580] samples: 25280, Training Loss: 0.0012
   Time since start: 0:14:20.906173
[batch 1600] samples: 25600, Training Loss: 0.0039
   Time since start: 0:14:22.733945
[batch 1620] samples: 25920, Training Loss: 0.0016
   Time since start: 0:14:24.137300
[batch 1640] samples: 26240, Training Loss: 0.0040
   Time since start: 0:14:25.454971
[batch 1660] samples: 26560, Training Loss: 0.0096
   Time since start: 0:14:26.931040
[batch 1680] samples: 26880, Training Loss: 0.0021
   Time since start: 0:14:28.249757
[batch 1700] samples: 27200, Training Loss: 0.0012
   Time since start: 0:14:29.836580
[batch 1720] samples: 27520, Training Loss: 0.0013
   Time since start: 0:14:31.713506
[batch 1740] samples: 27840, Training Loss: 0.0022
   Time since start: 0:14:33.593654
[batch 1760] samples: 28160, Training Loss: 0.0010
   Time since start: 0:14:35.475046
[batch 1780] samples: 28480, Training Loss: 0.0030
   Time since start: 0:14:37.080686
[batch 1800] samples: 28800, Training Loss: 0.0019
   Time since start: 0:14:38.428579
[batch 1820] samples: 29120, Training Loss: 0.0021
   Time since start: 0:14:39.778248
[batch 1840] samples: 29440, Training Loss: 0.0018
   Time since start: 0:14:41.153546
[batch 1860] samples: 29760, Training Loss: 0.0017
   Time since start: 0:14:42.570140
[batch 1880] samples: 30080, Training Loss: 0.0021
   Time since start: 0:14:43.989064
[batch 1900] samples: 30400, Training Loss: 0.0009
   Time since start: 0:14:45.582519
[batch 1920] samples: 30720, Training Loss: 0.0013
   Time since start: 0:14:47.289783
[batch 1940] samples: 31040, Training Loss: 0.0011
   Time since start: 0:14:49.179704
[batch 1960] samples: 31360, Training Loss: 0.0014
   Time since start: 0:14:51.054405
--m-Epoch 5 done.
   Training Loss: 0.0025
   Validation Loss: 0.0008
Epoch: 6 of 30
[batch 20] samples: 320, Training Loss: 0.0016
   Time since start: 0:15:04.463370
[batch 40] samples: 640, Training Loss: 0.0037
   Time since start: 0:15:06.190740
[batch 60] samples: 960, Training Loss: 0.0012
   Time since start: 0:15:07.920288
[batch 80] samples: 1280, Training Loss: 0.0009
   Time since start: 0:15:09.621570
[batch 100] samples: 1600, Training Loss: 0.0015
   Time since start: 0:15:11.328784
[batch 120] samples: 1920, Training Loss: 0.0012
   Time since start: 0:15:13.048450
[batch 140] samples: 2240, Training Loss: 0.0011
   Time since start: 0:15:14.790011
[batch 160] samples: 2560, Training Loss: 0.0019
   Time since start: 0:15:16.532073
[batch 180] samples: 2880, Training Loss: 0.0015
   Time since start: 0:15:18.256625
[batch 200] samples: 3200, Training Loss: 0.0016
   Time since start: 0:15:19.976553
[batch 220] samples: 3520, Training Loss: 0.0005
   Time since start: 0:15:21.684543
[batch 240] samples: 3840, Training Loss: 0.0008
   Time since start: 0:15:23.423159
[batch 260] samples: 4160, Training Loss: 0.0008
   Time since start: 0:15:25.173978
[batch 280] samples: 4480, Training Loss: 0.0012
   Time since start: 0:15:26.971920
[batch 300] samples: 4800, Training Loss: 0.0018
   Time since start: 0:15:28.763526
[batch 320] samples: 5120, Training Loss: 0.0013
   Time since start: 0:15:30.573409
[batch 340] samples: 5440, Training Loss: 0.0010
   Time since start: 0:15:32.384108
[batch 360] samples: 5760, Training Loss: 0.0007
   Time since start: 0:15:34.201159
[batch 380] samples: 6080, Training Loss: 0.0032
   Time since start: 0:15:35.992453
[batch 400] samples: 6400, Training Loss: 0.0012
   Time since start: 0:15:37.789562
[batch 420] samples: 6720, Training Loss: 0.0009
   Time since start: 0:15:39.488076
[batch 440] samples: 7040, Training Loss: 0.0009
   Time since start: 0:15:40.806529
[batch 460] samples: 7360, Training Loss: 0.0009
   Time since start: 0:15:42.124777
[batch 480] samples: 7680, Training Loss: 0.0024
   Time since start: 0:15:43.445004
[batch 500] samples: 8000, Training Loss: 0.0012
   Time since start: 0:15:44.770711
[batch 520] samples: 8320, Training Loss: 0.0011
   Time since start: 0:15:46.096373
[batch 540] samples: 8640, Training Loss: 0.0014
   Time since start: 0:15:47.435374
[batch 560] samples: 8960, Training Loss: 0.0009
   Time since start: 0:15:48.760184
[batch 580] samples: 9280, Training Loss: 0.0009
   Time since start: 0:15:50.410803
[batch 600] samples: 9600, Training Loss: 0.0011
   Time since start: 0:15:52.138370
[batch 620] samples: 9920, Training Loss: 0.0009
   Time since start: 0:15:53.856801
[batch 640] samples: 10240, Training Loss: 0.0014
   Time since start: 0:15:55.585505
[batch 660] samples: 10560, Training Loss: 0.0011
   Time since start: 0:15:57.320545
[batch 680] samples: 10880, Training Loss: 0.0009
   Time since start: 0:15:59.062869
[batch 700] samples: 11200, Training Loss: 0.0021
   Time since start: 0:16:00.784475
[batch 720] samples: 11520, Training Loss: 0.0006
   Time since start: 0:16:02.524583
[batch 740] samples: 11840, Training Loss: 0.0013
   Time since start: 0:16:03.902863
[batch 760] samples: 12160, Training Loss: 0.0021
   Time since start: 0:16:05.180020
[batch 780] samples: 12480, Training Loss: 0.0025
   Time since start: 0:16:06.458138
[batch 800] samples: 12800, Training Loss: 0.0010
   Time since start: 0:16:07.736434
[batch 820] samples: 13120, Training Loss: 0.0011
   Time since start: 0:16:09.014655
[batch 840] samples: 13440, Training Loss: 0.0009
   Time since start: 0:16:10.416039
[batch 860] samples: 13760, Training Loss: 0.0011
   Time since start: 0:16:11.931755
[batch 880] samples: 14080, Training Loss: 0.0009
   Time since start: 0:16:13.683823
[batch 900] samples: 14400, Training Loss: 0.0011
   Time since start: 0:16:15.420575
[batch 920] samples: 14720, Training Loss: 0.0008
   Time since start: 0:16:17.164937
[batch 940] samples: 15040, Training Loss: 0.0009
   Time since start: 0:16:18.894004
[batch 960] samples: 15360, Training Loss: 0.0007
   Time since start: 0:16:20.648893
[batch 980] samples: 15680, Training Loss: 0.0036
   Time since start: 0:16:22.395293
[batch 1000] samples: 16000, Training Loss: 0.0009
   Time since start: 0:16:24.152535
[batch 1020] samples: 16320, Training Loss: 0.0008
   Time since start: 0:16:25.579165
[batch 1040] samples: 16640, Training Loss: 0.0014
   Time since start: 0:16:26.863419
[batch 1060] samples: 16960, Training Loss: 0.0009
   Time since start: 0:16:28.765675
[batch 1080] samples: 17280, Training Loss: 0.0012
   Time since start: 0:16:30.727380
[batch 1100] samples: 17600, Training Loss: 0.0007
   Time since start: 0:16:32.687709
[batch 1120] samples: 17920, Training Loss: 0.0009
   Time since start: 0:16:34.659137
[batch 1140] samples: 18240, Training Loss: 0.0010
   Time since start: 0:16:36.621117
[batch 1160] samples: 18560, Training Loss: 0.0016
   Time since start: 0:16:38.572272
[batch 1180] samples: 18880, Training Loss: 0.0008
   Time since start: 0:16:40.543864
[batch 1200] samples: 19200, Training Loss: 0.0012
   Time since start: 0:16:42.505850
[batch 1220] samples: 19520, Training Loss: 0.0009
   Time since start: 0:16:44.477098
[batch 1240] samples: 19840, Training Loss: 0.0005
   Time since start: 0:16:46.448315
[batch 1260] samples: 20160, Training Loss: 0.0012
   Time since start: 0:16:48.419260
[batch 1280] samples: 20480, Training Loss: 0.0008
   Time since start: 0:16:50.388859
[batch 1300] samples: 20800, Training Loss: 0.0010
   Time since start: 0:16:52.350844
[batch 1320] samples: 21120, Training Loss: 0.0008
   Time since start: 0:16:54.321242
[batch 1340] samples: 21440, Training Loss: 0.0020
   Time since start: 0:16:56.292813
[batch 1360] samples: 21760, Training Loss: 0.0006
   Time since start: 0:16:58.152920
[batch 1380] samples: 22080, Training Loss: 0.0008
   Time since start: 0:16:59.541620
[batch 1400] samples: 22400, Training Loss: 0.0012
   Time since start: 0:17:00.934731
[batch 1420] samples: 22720, Training Loss: 0.0008
   Time since start: 0:17:02.325608
[batch 1440] samples: 23040, Training Loss: 0.0009
   Time since start: 0:17:03.724884
[batch 1460] samples: 23360, Training Loss: 0.0008
   Time since start: 0:17:05.111222
[batch 1480] samples: 23680, Training Loss: 0.0007
   Time since start: 0:17:07.017394
[batch 1500] samples: 24000, Training Loss: 0.0009
   Time since start: 0:17:08.988043
[batch 1520] samples: 24320, Training Loss: 0.0005
   Time since start: 0:17:10.949310
[batch 1540] samples: 24640, Training Loss: 0.0007
   Time since start: 0:17:12.799070
[batch 1560] samples: 24960, Training Loss: 0.0009
   Time since start: 0:17:14.628519
[batch 1580] samples: 25280, Training Loss: 0.0007
   Time since start: 0:17:16.373552
[batch 1600] samples: 25600, Training Loss: 0.0005
   Time since start: 0:17:18.101447
[batch 1620] samples: 25920, Training Loss: 0.0030
   Time since start: 0:17:19.843687
[batch 1640] samples: 26240, Training Loss: 0.0009
   Time since start: 0:17:21.590952
[batch 1660] samples: 26560, Training Loss: 0.0006
   Time since start: 0:17:23.342994
[batch 1680] samples: 26880, Training Loss: 0.0006
   Time since start: 0:17:25.138134
[batch 1700] samples: 27200, Training Loss: 0.0007
   Time since start: 0:17:26.895026
[batch 1720] samples: 27520, Training Loss: 0.0008
   Time since start: 0:17:28.663697
[batch 1740] samples: 27840, Training Loss: 0.0006
   Time since start: 0:17:30.431628
[batch 1760] samples: 28160, Training Loss: 0.0007
   Time since start: 0:17:32.213434
[batch 1780] samples: 28480, Training Loss: 0.0005
   Time since start: 0:17:33.988590
[batch 1800] samples: 28800, Training Loss: 0.0008
   Time since start: 0:17:35.754848
[batch 1820] samples: 29120, Training Loss: 0.0006
   Time since start: 0:17:37.494018
[batch 1840] samples: 29440, Training Loss: 0.0010
   Time since start: 0:17:39.244305
[batch 1860] samples: 29760, Training Loss: 0.0009
   Time since start: 0:17:41.001033
[batch 1880] samples: 30080, Training Loss: 0.0007
   Time since start: 0:17:42.755605
[batch 1900] samples: 30400, Training Loss: 0.0016
   Time since start: 0:17:44.526912
[batch 1920] samples: 30720, Training Loss: 0.0007
   Time since start: 0:17:46.308547
[batch 1940] samples: 31040, Training Loss: 0.0006
   Time since start: 0:17:48.099451
[batch 1960] samples: 31360, Training Loss: 0.0005
   Time since start: 0:17:49.400997
--m-Epoch 6 done.
   Training Loss: 0.0012
   Validation Loss: 0.0004
Epoch: 7 of 30
[batch 20] samples: 320, Training Loss: 0.0058
   Time since start: 0:18:02.299948
[batch 40] samples: 640, Training Loss: 0.0005
   Time since start: 0:18:04.141172
[batch 60] samples: 960, Training Loss: 0.0005
   Time since start: 0:18:05.996563
[batch 80] samples: 1280, Training Loss: 0.0006
   Time since start: 0:18:07.825898
[batch 100] samples: 1600, Training Loss: 0.0014
   Time since start: 0:18:09.663378
[batch 120] samples: 1920, Training Loss: 0.0005
   Time since start: 0:18:11.618196
[batch 140] samples: 2240, Training Loss: 0.0006
   Time since start: 0:18:13.517788
[batch 160] samples: 2560, Training Loss: 0.0014
   Time since start: 0:18:15.420688
[batch 180] samples: 2880, Training Loss: 0.0005
   Time since start: 0:18:17.300525
[batch 200] samples: 3200, Training Loss: 0.0007
   Time since start: 0:18:19.199324
[batch 220] samples: 3520, Training Loss: 0.0011
   Time since start: 0:18:21.100166
[batch 240] samples: 3840, Training Loss: 0.0008
   Time since start: 0:18:22.928806
[batch 260] samples: 4160, Training Loss: 0.0007
   Time since start: 0:18:24.246824
[batch 280] samples: 4480, Training Loss: 0.0005
   Time since start: 0:18:25.566365
[batch 300] samples: 4800, Training Loss: 0.0021
   Time since start: 0:18:26.886510
[batch 320] samples: 5120, Training Loss: 0.0005
   Time since start: 0:18:28.196372
[batch 340] samples: 5440, Training Loss: 0.0006
   Time since start: 0:18:29.507788
[batch 360] samples: 5760, Training Loss: 0.0007
   Time since start: 0:18:30.817809
[batch 380] samples: 6080, Training Loss: 0.0006
   Time since start: 0:18:32.129005
[batch 400] samples: 6400, Training Loss: 0.0006
   Time since start: 0:18:33.439791
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:18:34.750807
[batch 440] samples: 7040, Training Loss: 0.0005
   Time since start: 0:18:36.061809
[batch 460] samples: 7360, Training Loss: 0.0011
   Time since start: 0:18:37.373943
[batch 480] samples: 7680, Training Loss: 0.0013
   Time since start: 0:18:38.684439
[batch 500] samples: 8000, Training Loss: 0.0005
   Time since start: 0:18:39.995809
[batch 520] samples: 8320, Training Loss: 0.0005
   Time since start: 0:18:41.306929
[batch 540] samples: 8640, Training Loss: 0.0006
   Time since start: 0:18:42.619014
[batch 560] samples: 8960, Training Loss: 0.0007
   Time since start: 0:18:43.929297
[batch 580] samples: 9280, Training Loss: 0.0004
   Time since start: 0:18:45.241635
[batch 600] samples: 9600, Training Loss: 0.0004
   Time since start: 0:18:46.561729
[batch 620] samples: 9920, Training Loss: 0.0064
   Time since start: 0:18:47.871840
[batch 640] samples: 10240, Training Loss: 0.0006
   Time since start: 0:18:49.182981
[batch 660] samples: 10560, Training Loss: 0.0005
   Time since start: 0:18:50.492409
[batch 680] samples: 10880, Training Loss: 0.0006
   Time since start: 0:18:51.804703
[batch 700] samples: 11200, Training Loss: 0.0003
   Time since start: 0:18:53.121121
[batch 720] samples: 11520, Training Loss: 0.0008
   Time since start: 0:18:54.439766
[batch 740] samples: 11840, Training Loss: 0.0005
   Time since start: 0:18:55.756449
[batch 760] samples: 12160, Training Loss: 0.0004
   Time since start: 0:18:57.067204
[batch 780] samples: 12480, Training Loss: 0.0008
   Time since start: 0:18:58.376502
[batch 800] samples: 12800, Training Loss: 0.0021
   Time since start: 0:18:59.684987
[batch 820] samples: 13120, Training Loss: 0.0005
   Time since start: 0:19:00.992041
[batch 840] samples: 13440, Training Loss: 0.0004
   Time since start: 0:19:02.301386
[batch 860] samples: 13760, Training Loss: 0.0004
   Time since start: 0:19:03.612211
[batch 880] samples: 14080, Training Loss: 0.0008
   Time since start: 0:19:04.921217
[batch 900] samples: 14400, Training Loss: 0.0008
   Time since start: 0:19:06.233505
[batch 920] samples: 14720, Training Loss: 0.0004
   Time since start: 0:19:07.543150
[batch 940] samples: 15040, Training Loss: 0.0004
   Time since start: 0:19:08.850997
[batch 960] samples: 15360, Training Loss: 0.0014
   Time since start: 0:19:10.360317
[batch 980] samples: 15680, Training Loss: 0.0003
   Time since start: 0:19:12.332847
[batch 1000] samples: 16000, Training Loss: 0.0028
   Time since start: 0:19:14.306607
[batch 1020] samples: 16320, Training Loss: 0.0006
   Time since start: 0:19:16.206450
[batch 1040] samples: 16640, Training Loss: 0.0005
   Time since start: 0:19:17.607255
[batch 1060] samples: 16960, Training Loss: 0.0013
   Time since start: 0:19:19.008853
[batch 1080] samples: 17280, Training Loss: 0.0003
   Time since start: 0:19:20.411212
[batch 1100] samples: 17600, Training Loss: 0.0004
   Time since start: 0:19:22.226070
[batch 1120] samples: 17920, Training Loss: 0.0006
   Time since start: 0:19:24.063119
[batch 1140] samples: 18240, Training Loss: 0.0004
   Time since start: 0:19:25.546137
[batch 1160] samples: 18560, Training Loss: 0.0006
   Time since start: 0:19:26.950559
[batch 1180] samples: 18880, Training Loss: 0.0004
   Time since start: 0:19:28.447662
[batch 1200] samples: 19200, Training Loss: 0.0003
   Time since start: 0:19:30.169256
[batch 1220] samples: 19520, Training Loss: 0.0005
   Time since start: 0:19:31.930500
[batch 1240] samples: 19840, Training Loss: 0.0003
   Time since start: 0:19:33.410300
[batch 1260] samples: 20160, Training Loss: 0.0007
   Time since start: 0:19:34.681468
[batch 1280] samples: 20480, Training Loss: 0.0014
   Time since start: 0:19:35.950292
[batch 1300] samples: 20800, Training Loss: 0.0003
   Time since start: 0:19:37.219046
[batch 1320] samples: 21120, Training Loss: 0.0003
   Time since start: 0:19:38.487283
[batch 1340] samples: 21440, Training Loss: 0.0004
   Time since start: 0:19:39.758093
[batch 1360] samples: 21760, Training Loss: 0.0004
   Time since start: 0:19:41.158270
[batch 1380] samples: 22080, Training Loss: 0.0009
   Time since start: 0:19:42.443846
[batch 1400] samples: 22400, Training Loss: 0.0007
   Time since start: 0:19:43.753531
[batch 1420] samples: 22720, Training Loss: 0.0004
   Time since start: 0:19:45.622099
[batch 1440] samples: 23040, Training Loss: 0.0003
   Time since start: 0:19:47.643832
[batch 1460] samples: 23360, Training Loss: 0.0003
   Time since start: 0:19:49.666846
[batch 1480] samples: 23680, Training Loss: 0.0003
   Time since start: 0:19:51.678358
[batch 1500] samples: 24000, Training Loss: 0.0011
   Time since start: 0:19:53.502117
[batch 1520] samples: 24320, Training Loss: 0.0007
   Time since start: 0:19:54.771566
[batch 1540] samples: 24640, Training Loss: 0.0004
   Time since start: 0:19:56.040463
[batch 1560] samples: 24960, Training Loss: 0.0008
   Time since start: 0:19:57.312879
[batch 1580] samples: 25280, Training Loss: 0.0004
   Time since start: 0:19:58.581447
[batch 1600] samples: 25600, Training Loss: 0.0003
   Time since start: 0:19:59.851871
[batch 1620] samples: 25920, Training Loss: 0.0004
   Time since start: 0:20:01.121381
[batch 1640] samples: 26240, Training Loss: 0.0003
   Time since start: 0:20:02.389969
[batch 1660] samples: 26560, Training Loss: 0.0004
   Time since start: 0:20:03.658895
[batch 1680] samples: 26880, Training Loss: 0.0005
   Time since start: 0:20:04.928657
[batch 1700] samples: 27200, Training Loss: 0.0030
   Time since start: 0:20:06.198682
[batch 1720] samples: 27520, Training Loss: 0.0004
   Time since start: 0:20:07.471047
[batch 1740] samples: 27840, Training Loss: 0.0005
   Time since start: 0:20:08.741813
[batch 1760] samples: 28160, Training Loss: 0.0003
   Time since start: 0:20:10.014045
[batch 1780] samples: 28480, Training Loss: 0.0003
   Time since start: 0:20:11.287999
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:20:12.557547
[batch 1820] samples: 29120, Training Loss: 0.0003
   Time since start: 0:20:13.832184
[batch 1840] samples: 29440, Training Loss: 0.0003
   Time since start: 0:20:15.111281
[batch 1860] samples: 29760, Training Loss: 0.0002
   Time since start: 0:20:16.387368
[batch 1880] samples: 30080, Training Loss: 0.0008
   Time since start: 0:20:17.658577
[batch 1900] samples: 30400, Training Loss: 0.0004
   Time since start: 0:20:18.931565
[batch 1920] samples: 30720, Training Loss: 0.0005
   Time since start: 0:20:20.204379
[batch 1940] samples: 31040, Training Loss: 0.0002
   Time since start: 0:20:21.475008
[batch 1960] samples: 31360, Training Loss: 0.0003
   Time since start: 0:20:22.735288
--m-Epoch 7 done.
   Training Loss: 0.0007
   Validation Loss: 0.0003
Epoch: 8 of 30
[batch 20] samples: 320, Training Loss: 0.0004
   Time since start: 0:20:35.795788
[batch 40] samples: 640, Training Loss: 0.0003
   Time since start: 0:20:37.290854
[batch 60] samples: 960, Training Loss: 0.0004
   Time since start: 0:20:39.262939
[batch 80] samples: 1280, Training Loss: 0.0006
   Time since start: 0:20:41.218121
[batch 100] samples: 1600, Training Loss: 0.0004
   Time since start: 0:20:43.197151
[batch 120] samples: 1920, Training Loss: 0.0006
   Time since start: 0:20:45.164230
[batch 140] samples: 2240, Training Loss: 0.0004
   Time since start: 0:20:47.119163
[batch 160] samples: 2560, Training Loss: 0.0003
   Time since start: 0:20:49.090275
[batch 180] samples: 2880, Training Loss: 0.0005
   Time since start: 0:20:50.926569
[batch 200] samples: 3200, Training Loss: 0.0003
   Time since start: 0:20:52.321395
[batch 220] samples: 3520, Training Loss: 0.0002
   Time since start: 0:20:53.712229
[batch 240] samples: 3840, Training Loss: 0.0004
   Time since start: 0:20:55.107202
[batch 260] samples: 4160, Training Loss: 0.0006
   Time since start: 0:20:56.499474
[batch 280] samples: 4480, Training Loss: 0.0010
   Time since start: 0:20:57.898439
[batch 300] samples: 4800, Training Loss: 0.0002
   Time since start: 0:20:59.460154
[batch 320] samples: 5120, Training Loss: 0.0003
   Time since start: 0:21:01.418853
[batch 340] samples: 5440, Training Loss: 0.0003
   Time since start: 0:21:03.387001
[batch 360] samples: 5760, Training Loss: 0.0003
   Time since start: 0:21:05.354044
[batch 380] samples: 6080, Training Loss: 0.0003
   Time since start: 0:21:07.285388
[batch 400] samples: 6400, Training Loss: 0.0003
   Time since start: 0:21:08.643445
[batch 420] samples: 6720, Training Loss: 0.0004
   Time since start: 0:21:10.000448
[batch 440] samples: 7040, Training Loss: 0.0002
   Time since start: 0:21:11.771659
[batch 460] samples: 7360, Training Loss: 0.0003
   Time since start: 0:21:13.326501
[batch 480] samples: 7680, Training Loss: 0.0005
   Time since start: 0:21:14.692182
[batch 500] samples: 8000, Training Loss: 0.0003
   Time since start: 0:21:16.075074
[batch 520] samples: 8320, Training Loss: 0.0004
   Time since start: 0:21:17.470797
[batch 540] samples: 8640, Training Loss: 0.0005
   Time since start: 0:21:18.930208
[batch 560] samples: 8960, Training Loss: 0.0002
   Time since start: 0:21:20.877906
[batch 580] samples: 9280, Training Loss: 0.0003
   Time since start: 0:21:22.846330
[batch 600] samples: 9600, Training Loss: 0.0005
   Time since start: 0:21:24.817453
[batch 620] samples: 9920, Training Loss: 0.0003
   Time since start: 0:21:26.787060
[batch 640] samples: 10240, Training Loss: 0.0005
   Time since start: 0:21:28.891662
[batch 660] samples: 10560, Training Loss: 0.0005
   Time since start: 0:21:30.862271
[batch 680] samples: 10880, Training Loss: 0.0002
   Time since start: 0:21:32.824898
[batch 700] samples: 11200, Training Loss: 0.0003
   Time since start: 0:21:34.797258
[batch 720] samples: 11520, Training Loss: 0.0005
   Time since start: 0:21:36.357835
[batch 740] samples: 11840, Training Loss: 0.0002
   Time since start: 0:21:37.715077
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:21:39.217978
[batch 780] samples: 12480, Training Loss: 0.0004
   Time since start: 0:21:40.987853
[batch 800] samples: 12800, Training Loss: 0.0005
   Time since start: 0:21:42.754589
[batch 820] samples: 13120, Training Loss: 0.0002
   Time since start: 0:21:44.512051
[batch 840] samples: 13440, Training Loss: 0.0003
   Time since start: 0:21:46.277899
[batch 860] samples: 13760, Training Loss: 0.0003
   Time since start: 0:21:48.027988
[batch 880] samples: 14080, Training Loss: 0.0002
   Time since start: 0:21:49.794207
[batch 900] samples: 14400, Training Loss: 0.0002
   Time since start: 0:21:51.161094
[batch 920] samples: 14720, Training Loss: 0.0002
   Time since start: 0:21:52.591358
[batch 940] samples: 15040, Training Loss: 0.0006
   Time since start: 0:21:54.363918
[batch 960] samples: 15360, Training Loss: 0.0009
   Time since start: 0:21:56.173934
[batch 980] samples: 15680, Training Loss: 0.0002
   Time since start: 0:21:58.001536
[batch 1000] samples: 16000, Training Loss: 0.0011
   Time since start: 0:21:59.831025
[batch 1020] samples: 16320, Training Loss: 0.0003
   Time since start: 0:22:01.652100
[batch 1040] samples: 16640, Training Loss: 0.0002
   Time since start: 0:22:03.164543
[batch 1060] samples: 16960, Training Loss: 0.0002
   Time since start: 0:22:04.498912
[batch 1080] samples: 17280, Training Loss: 0.0002
   Time since start: 0:22:05.824017
[batch 1100] samples: 17600, Training Loss: 0.0002
   Time since start: 0:22:07.151590
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:22:08.477999
[batch 1140] samples: 18240, Training Loss: 0.0002
   Time since start: 0:22:09.805319
[batch 1160] samples: 18560, Training Loss: 0.0002
   Time since start: 0:22:11.133963
[batch 1180] samples: 18880, Training Loss: 0.0002
   Time since start: 0:22:12.461879
[batch 1200] samples: 19200, Training Loss: 0.0003
   Time since start: 0:22:13.791044
[batch 1220] samples: 19520, Training Loss: 0.0003
   Time since start: 0:22:15.192970
[batch 1240] samples: 19840, Training Loss: 0.0003
   Time since start: 0:22:16.710162
[batch 1260] samples: 20160, Training Loss: 0.0003
   Time since start: 0:22:18.493081
[batch 1280] samples: 20480, Training Loss: 0.0002
   Time since start: 0:22:20.272237
[batch 1300] samples: 20800, Training Loss: 0.0004
   Time since start: 0:22:22.064884
[batch 1320] samples: 21120, Training Loss: 0.0004
   Time since start: 0:22:23.886414
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:22:25.706948
[batch 1360] samples: 21760, Training Loss: 0.0002
   Time since start: 0:22:27.509279
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:22:28.903518
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:22:30.211565
[batch 1420] samples: 22720, Training Loss: 0.0005
   Time since start: 0:22:31.517334
[batch 1440] samples: 23040, Training Loss: 0.0002
   Time since start: 0:22:32.821765
[batch 1460] samples: 23360, Training Loss: 0.0002
   Time since start: 0:22:34.130151
[batch 1480] samples: 23680, Training Loss: 0.0003
   Time since start: 0:22:35.462662
[batch 1500] samples: 24000, Training Loss: 0.0003
   Time since start: 0:22:36.815938
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:22:38.161265
[batch 1540] samples: 24640, Training Loss: 0.0007
   Time since start: 0:22:39.529154
[batch 1560] samples: 24960, Training Loss: 0.0005
   Time since start: 0:22:40.906412
[batch 1580] samples: 25280, Training Loss: 0.0002
   Time since start: 0:22:42.219759
[batch 1600] samples: 25600, Training Loss: 0.0003
   Time since start: 0:22:43.532225
[batch 1620] samples: 25920, Training Loss: 0.0007
   Time since start: 0:22:44.851545
[batch 1640] samples: 26240, Training Loss: 0.0015
   Time since start: 0:22:46.172673
[batch 1660] samples: 26560, Training Loss: 0.0002
   Time since start: 0:22:47.489160
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:22:48.804553
[batch 1700] samples: 27200, Training Loss: 0.0005
   Time since start: 0:22:50.120937
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:22:51.436306
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:22:52.755577
[batch 1760] samples: 28160, Training Loss: 0.0002
   Time since start: 0:22:54.072132
[batch 1780] samples: 28480, Training Loss: 0.0002
   Time since start: 0:22:55.381094
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:22:56.678197
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:22:57.948608
[batch 1840] samples: 29440, Training Loss: 0.0002
   Time since start: 0:22:59.242231
[batch 1860] samples: 29760, Training Loss: 0.0002
   Time since start: 0:23:00.707276
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:23:02.062680
[batch 1900] samples: 30400, Training Loss: 0.0002
   Time since start: 0:23:03.423050
[batch 1920] samples: 30720, Training Loss: 0.0003
   Time since start: 0:23:04.774638
[batch 1940] samples: 31040, Training Loss: 0.0003
   Time since start: 0:23:06.129589
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:23:07.440543
--m-Epoch 8 done.
   Training Loss: 0.0004
   Validation Loss: 0.0002
Epoch: 9 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:23:19.785641
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:23:21.140944
[batch 60] samples: 960, Training Loss: 0.0002
   Time since start: 0:23:23.007996
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:23:24.422599
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:23:25.779850
[batch 120] samples: 1920, Training Loss: 0.0011
   Time since start: 0:23:27.138712
[batch 140] samples: 2240, Training Loss: 0.0002
   Time since start: 0:23:28.494465
[batch 160] samples: 2560, Training Loss: 0.0003
   Time since start: 0:23:29.850581
[batch 180] samples: 2880, Training Loss: 0.0004
   Time since start: 0:23:31.207697
[batch 200] samples: 3200, Training Loss: 0.0002
   Time since start: 0:23:32.562368
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:23:33.878742
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:23:35.194376
[batch 260] samples: 4160, Training Loss: 0.0003
   Time since start: 0:23:36.513423
[batch 280] samples: 4480, Training Loss: 0.0002
   Time since start: 0:23:37.831143
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:23:39.147280
[batch 320] samples: 5120, Training Loss: 0.0003
   Time since start: 0:23:40.462611
[batch 340] samples: 5440, Training Loss: 0.0003
   Time since start: 0:23:41.781463
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:23:43.097464
[batch 380] samples: 6080, Training Loss: 0.0004
   Time since start: 0:23:44.414531
[batch 400] samples: 6400, Training Loss: 0.0005
   Time since start: 0:23:45.730499
[batch 420] samples: 6720, Training Loss: 0.0002
   Time since start: 0:23:47.047786
[batch 440] samples: 7040, Training Loss: 0.0006
   Time since start: 0:23:48.367462
[batch 460] samples: 7360, Training Loss: 0.0002
   Time since start: 0:23:49.689898
[batch 480] samples: 7680, Training Loss: 0.0003
   Time since start: 0:23:51.009634
[batch 500] samples: 8000, Training Loss: 0.0008
   Time since start: 0:23:52.329532
[batch 520] samples: 8320, Training Loss: 0.0026
   Time since start: 0:23:53.656642
[batch 540] samples: 8640, Training Loss: 0.0002
   Time since start: 0:23:55.017495
[batch 560] samples: 8960, Training Loss: 0.0002
   Time since start: 0:23:56.380201
[batch 580] samples: 9280, Training Loss: 0.0003
   Time since start: 0:23:57.737901
[batch 600] samples: 9600, Training Loss: 0.0002
   Time since start: 0:23:59.095170
[batch 620] samples: 9920, Training Loss: 0.0003
   Time since start: 0:24:00.451425
[batch 640] samples: 10240, Training Loss: 0.0005
   Time since start: 0:24:01.809949
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:24:03.166370
[batch 680] samples: 10880, Training Loss: 0.0002
   Time since start: 0:24:04.525123
[batch 700] samples: 11200, Training Loss: 0.0002
   Time since start: 0:24:05.882661
[batch 720] samples: 11520, Training Loss: 0.0002
   Time since start: 0:24:07.592375
[batch 740] samples: 11840, Training Loss: 0.0002
   Time since start: 0:24:09.505608
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:24:10.859586
[batch 780] samples: 12480, Training Loss: 0.0003
   Time since start: 0:24:12.213485
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:24:13.565617
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:24:15.012573
[batch 840] samples: 13440, Training Loss: 0.0002
   Time since start: 0:24:16.406561
[batch 860] samples: 13760, Training Loss: 0.0002
   Time since start: 0:24:17.769400
[batch 880] samples: 14080, Training Loss: 0.0003
   Time since start: 0:24:19.141220
[batch 900] samples: 14400, Training Loss: 0.0003
   Time since start: 0:24:20.505075
[batch 920] samples: 14720, Training Loss: 0.0003
   Time since start: 0:24:21.869518
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:24:23.232241
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:24:24.895712
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:24:26.812987
[batch 1000] samples: 16000, Training Loss: 0.0002
   Time since start: 0:24:28.716936
[batch 1020] samples: 16320, Training Loss: 0.0002
   Time since start: 0:24:30.620285
[batch 1040] samples: 16640, Training Loss: 0.0003
   Time since start: 0:24:32.535515
[batch 1060] samples: 16960, Training Loss: 0.0002
   Time since start: 0:24:34.439030
[batch 1080] samples: 17280, Training Loss: 0.0006
   Time since start: 0:24:36.341015
[batch 1100] samples: 17600, Training Loss: 0.0002
   Time since start: 0:24:38.174652
[batch 1120] samples: 17920, Training Loss: 0.0002
   Time since start: 0:24:40.071578
[batch 1140] samples: 18240, Training Loss: 0.0002
   Time since start: 0:24:42.123178
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:24:43.609348
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:24:45.016616
[batch 1200] samples: 19200, Training Loss: 0.0002
   Time since start: 0:24:46.390418
[batch 1220] samples: 19520, Training Loss: 0.0003
   Time since start: 0:24:48.036733
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:24:49.864837
[batch 1260] samples: 20160, Training Loss: 0.0003
   Time since start: 0:24:51.693520
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:24:53.503654
[batch 1300] samples: 20800, Training Loss: 0.0002
   Time since start: 0:24:55.200672
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:24:56.523579
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:24:57.845353
[batch 1360] samples: 21760, Training Loss: 0.0002
   Time since start: 0:24:59.173971
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:25:00.538112
[batch 1400] samples: 22400, Training Loss: 0.0002
   Time since start: 0:25:01.903666
[batch 1420] samples: 22720, Training Loss: 0.0002
   Time since start: 0:25:03.264929
[batch 1440] samples: 23040, Training Loss: 0.0002
   Time since start: 0:25:04.625717
[batch 1460] samples: 23360, Training Loss: 0.0003
   Time since start: 0:25:05.990225
[batch 1480] samples: 23680, Training Loss: 0.0002
   Time since start: 0:25:07.878156
[batch 1500] samples: 24000, Training Loss: 0.0002
   Time since start: 0:25:09.613504
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:25:11.480008
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:25:13.360350
[batch 1560] samples: 24960, Training Loss: 0.0001
   Time since start: 0:25:14.990748
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:25:16.318365
[batch 1600] samples: 25600, Training Loss: 0.0002
   Time since start: 0:25:17.639944
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:25:18.964640
[batch 1640] samples: 26240, Training Loss: 0.0003
   Time since start: 0:25:20.286902
[batch 1660] samples: 26560, Training Loss: 0.0002
   Time since start: 0:25:21.616378
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:25:22.938814
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:25:24.261308
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:25:25.581721
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:25:26.906488
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:25:28.230886
[batch 1780] samples: 28480, Training Loss: 0.0002
   Time since start: 0:25:29.552890
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:25:30.873654
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:25:32.194974
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:25:33.516096
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:25:34.842444
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:25:36.162025
[batch 1900] samples: 30400, Training Loss: 0.0004
   Time since start: 0:25:37.480146
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:25:38.799149
[batch 1940] samples: 31040, Training Loss: 0.0002
   Time since start: 0:25:40.117498
[batch 1960] samples: 31360, Training Loss: 0.0004
   Time since start: 0:25:41.407256
--m-Epoch 9 done.
   Training Loss: 0.0003
   Validation Loss: 0.0002
Patience decreased: Patience is now  4
Epoch: 10 of 30
[batch 20] samples: 320, Training Loss: 0.0002
   Time since start: 0:25:53.597185
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:25:55.311351
[batch 60] samples: 960, Training Loss: 0.0002
   Time since start: 0:25:57.043881
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:25:58.784383
[batch 100] samples: 1600, Training Loss: 0.0003
   Time since start: 0:26:00.504902
[batch 120] samples: 1920, Training Loss: 0.0002
   Time since start: 0:26:02.249664
[batch 140] samples: 2240, Training Loss: 0.0002
   Time since start: 0:26:04.002986
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:26:05.719165
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:26:07.450963
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:26:09.203359
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:26:10.915177
[batch 240] samples: 3840, Training Loss: 0.0001
   Time since start: 0:26:12.652799
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:26:14.376187
[batch 280] samples: 4480, Training Loss: 0.0004
   Time since start: 0:26:16.114145
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:26:17.833575
[batch 320] samples: 5120, Training Loss: 0.0002
   Time since start: 0:26:19.568140
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:26:21.310863
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:26:23.051131
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:26:24.805177
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:26:26.671700
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:26:28.397989
[batch 440] samples: 7040, Training Loss: 0.0002
   Time since start: 0:26:30.107242
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:26:31.850463
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:26:33.597543
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:26:35.356976
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:26:37.107114
[batch 540] samples: 8640, Training Loss: 0.0002
   Time since start: 0:26:38.849526
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:26:40.592546
[batch 580] samples: 9280, Training Loss: 0.0001
   Time since start: 0:26:42.345183
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:26:44.083300
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:26:45.875971
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:26:47.692672
[batch 660] samples: 10560, Training Loss: 0.0003
   Time since start: 0:26:49.498250
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:26:51.276804
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:26:52.598315
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:26:53.919632
[batch 740] samples: 11840, Training Loss: 0.0003
   Time since start: 0:26:55.241261
[batch 760] samples: 12160, Training Loss: 0.0001
   Time since start: 0:26:56.561067
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:26:57.882864
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:26:59.207624
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:27:00.530867
[batch 840] samples: 13440, Training Loss: 0.0004
   Time since start: 0:27:01.858426
[batch 860] samples: 13760, Training Loss: 0.0002
   Time since start: 0:27:03.401420
[batch 880] samples: 14080, Training Loss: 0.0002
   Time since start: 0:27:05.215746
[batch 900] samples: 14400, Training Loss: 0.0002
   Time since start: 0:27:07.021620
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:27:08.824243
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:27:10.346838
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:27:11.666734
[batch 980] samples: 15680, Training Loss: 0.0002
   Time since start: 0:27:12.988339
[batch 1000] samples: 16000, Training Loss: 0.0003
   Time since start: 0:27:14.444951
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:27:16.275456
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:27:18.093863
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:27:19.919869
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:27:21.748237
[batch 1100] samples: 17600, Training Loss: 0.0003
   Time since start: 0:27:23.595038
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:27:25.416370
[batch 1140] samples: 18240, Training Loss: 0.0001
   Time since start: 0:27:27.285783
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:27:29.182519
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:27:31.066658
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:27:32.963514
[batch 1220] samples: 19520, Training Loss: 0.0005
   Time since start: 0:27:34.831762
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:27:36.722254
[batch 1260] samples: 20160, Training Loss: 0.0003
   Time since start: 0:27:38.554081
[batch 1280] samples: 20480, Training Loss: 0.0015
   Time since start: 0:27:40.379431
[batch 1300] samples: 20800, Training Loss: 0.0003
   Time since start: 0:27:42.198025
[batch 1320] samples: 21120, Training Loss: 0.0002
   Time since start: 0:27:44.008575
[batch 1340] samples: 21440, Training Loss: 0.0001
   Time since start: 0:27:45.836169
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:27:47.656858
[batch 1380] samples: 22080, Training Loss: 0.0002
   Time since start: 0:27:49.485454
[batch 1400] samples: 22400, Training Loss: 0.0003
   Time since start: 0:27:51.310908
[batch 1420] samples: 22720, Training Loss: 0.0003
   Time since start: 0:27:53.131598
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:27:54.928278
[batch 1460] samples: 23360, Training Loss: 0.0001
   Time since start: 0:27:56.719328
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:27:58.498171
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:28:00.315876
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:28:02.213262
[batch 1540] samples: 24640, Training Loss: 0.0002
   Time since start: 0:28:04.113901
[batch 1560] samples: 24960, Training Loss: 0.0001
   Time since start: 0:28:06.012057
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:28:07.915773
[batch 1600] samples: 25600, Training Loss: 0.0002
   Time since start: 0:28:09.786047
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:28:11.587095
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:28:13.528134
[batch 1660] samples: 26560, Training Loss: 0.0001
   Time since start: 0:28:15.387859
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:28:17.239101
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:28:19.079381
[batch 1720] samples: 27520, Training Loss: 0.0002
   Time since start: 0:28:20.917276
[batch 1740] samples: 27840, Training Loss: 0.0012
   Time since start: 0:28:22.744737
[batch 1760] samples: 28160, Training Loss: 0.0003
   Time since start: 0:28:24.236874
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:28:25.510960
[batch 1800] samples: 28800, Training Loss: 0.0003
   Time since start: 0:28:26.793159
[batch 1820] samples: 29120, Training Loss: 0.0002
   Time since start: 0:28:28.074501
[batch 1840] samples: 29440, Training Loss: 0.0003
   Time since start: 0:28:29.347835
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:28:30.622408
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:28:31.897612
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:28:33.172844
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:28:34.829761
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:28:36.803484
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:28:38.723870
--m-Epoch 10 done.
   Training Loss: 0.0002
   Validation Loss: 0.0003
Patience decreased: Patience is now  3
Epoch: 11 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:28:50.658142
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:28:51.936578
[batch 60] samples: 960, Training Loss: 0.0002
   Time since start: 0:28:53.676037
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:28:55.446604
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:28:57.277843
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:28:59.109035
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:29:00.970877
[batch 160] samples: 2560, Training Loss: 0.0001
   Time since start: 0:29:02.993773
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:29:05.017548
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:29:07.041146
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:29:09.061870
[batch 240] samples: 3840, Training Loss: 0.0001
   Time since start: 0:29:11.086013
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:29:13.107724
[batch 280] samples: 4480, Training Loss: 0.0001
   Time since start: 0:29:14.830143
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:29:16.120016
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:29:17.393885
[batch 340] samples: 5440, Training Loss: 0.0002
   Time since start: 0:29:18.663257
[batch 360] samples: 5760, Training Loss: 0.0002
   Time since start: 0:29:19.936207
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:29:21.210295
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:29:22.483734
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:29:23.757436
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:29:25.030996
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:29:26.313885
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:29:27.629661
[batch 500] samples: 8000, Training Loss: 0.0003
   Time since start: 0:29:28.941785
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:29:30.259509
[batch 540] samples: 8640, Training Loss: 0.0003
   Time since start: 0:29:31.574419
[batch 560] samples: 8960, Training Loss: 0.0005
   Time since start: 0:29:32.888460
[batch 580] samples: 9280, Training Loss: 0.0001
   Time since start: 0:29:34.362842
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:29:35.676554
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:29:36.990444
[batch 640] samples: 10240, Training Loss: 0.0002
   Time since start: 0:29:38.304070
[batch 660] samples: 10560, Training Loss: 0.0001
   Time since start: 0:29:39.619284
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:29:40.933929
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:29:42.311911
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:29:43.653645
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:29:45.005452
[batch 760] samples: 12160, Training Loss: 0.0001
   Time since start: 0:29:46.320334
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:29:47.632962
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:29:48.944062
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:29:50.263963
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:29:51.573193
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:29:52.881092
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:29:54.188447
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:29:55.495419
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:29:56.927759
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:29:58.236172
[batch 960] samples: 15360, Training Loss: 0.0003
   Time since start: 0:29:59.547065
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:30:00.856429
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:30:02.165997
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:30:03.474350
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:30:05.401540
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:30:07.360790
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:30:09.319759
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:30:11.274132
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:30:13.199258
[batch 1140] samples: 18240, Training Loss: 0.0001
   Time since start: 0:30:14.769162
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:30:16.186698
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:30:17.617247
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:30:19.047598
[batch 1220] samples: 19520, Training Loss: 0.0001
   Time since start: 0:30:20.472569
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:30:21.902874
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:30:23.515162
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:30:25.238079
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:30:26.962898
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:30:28.682915
[batch 1340] samples: 21440, Training Loss: 0.0004
   Time since start: 0:30:30.392580
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:30:32.136403
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:30:33.857394
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:30:35.593188
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:30:37.330390
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:30:39.087892
[batch 1460] samples: 23360, Training Loss: 0.0004
   Time since start: 0:30:40.815614
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:30:42.564224
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:30:44.291676
[batch 1520] samples: 24320, Training Loss: 0.0001
   Time since start: 0:30:45.680162
[batch 1540] samples: 24640, Training Loss: 0.0001
   Time since start: 0:30:47.189248
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:30:49.144035
[batch 1580] samples: 25280, Training Loss: 0.0001
   Time since start: 0:30:51.113673
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:30:53.079376
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:30:55.050533
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:30:57.046122
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:30:58.423074
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:30:59.730992
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:31:01.037324
[batch 1720] samples: 27520, Training Loss: 0.0002
   Time since start: 0:31:02.346344
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:31:03.652086
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:31:04.961159
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:31:06.267838
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:31:07.574932
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:31:08.886876
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:31:10.194516
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:31:11.501943
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:31:12.808587
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:31:14.120697
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:31:15.436610
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:31:16.748919
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:31:18.026431
--m-Epoch 11 done.
   Training Loss: 0.0002
   Validation Loss: 0.0002
Patience decreased: Patience is now  2
Epoch: 12 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:31:29.931038
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:31:31.314615
[batch 60] samples: 960, Training Loss: 0.0001
   Time since start: 0:31:32.700275
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:31:34.085536
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:31:35.468796
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:31:37.125897
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:31:39.087515
[batch 160] samples: 2560, Training Loss: 0.0013
   Time since start: 0:31:41.038361
[batch 180] samples: 2880, Training Loss: 0.0002
   Time since start: 0:31:42.991420
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:31:45.063589
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:31:46.572167
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:31:47.957206
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:31:49.657479
[batch 280] samples: 4480, Training Loss: 0.0001
   Time since start: 0:31:51.620227
[batch 300] samples: 4800, Training Loss: 0.0003
   Time since start: 0:31:53.008448
[batch 320] samples: 5120, Training Loss: 0.0002
   Time since start: 0:31:54.394820
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:31:55.779433
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:31:57.165603
[batch 380] samples: 6080, Training Loss: 0.0002
   Time since start: 0:31:58.547312
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:31:59.933431
[batch 420] samples: 6720, Training Loss: 0.0001
   Time since start: 0:32:01.318835
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:32:02.712764
[batch 460] samples: 7360, Training Loss: 0.0002
   Time since start: 0:32:04.108372
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:32:05.495611
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:32:06.881026
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:32:08.266471
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:32:09.653193
[batch 560] samples: 8960, Training Loss: 0.0001
   Time since start: 0:32:11.038471
[batch 580] samples: 9280, Training Loss: 0.0001
   Time since start: 0:32:12.425892
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:32:13.813661
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:32:15.211720
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:32:17.079873
[batch 660] samples: 10560, Training Loss: 0.0002
   Time since start: 0:32:19.051682
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:32:20.993337
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:32:22.710459
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:32:24.572472
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:32:26.512916
[batch 760] samples: 12160, Training Loss: 0.0002
   Time since start: 0:32:27.970409
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:32:29.285085
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:32:30.628576
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:32:32.308337
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:32:33.902561
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:32:35.250447
[batch 880] samples: 14080, Training Loss: 0.0002
   Time since start: 0:32:36.596111
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:32:37.940198
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:32:39.255296
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:32:40.560714
[batch 960] samples: 15360, Training Loss: 0.0001
   Time since start: 0:32:41.870770
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:32:43.182196
[batch 1000] samples: 16000, Training Loss: 0.0003
   Time since start: 0:32:44.976797
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:32:46.893778
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:32:48.780438
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:32:50.681477
[batch 1080] samples: 17280, Training Loss: 0.0001
   Time since start: 0:32:52.572018
[batch 1100] samples: 17600, Training Loss: 0.0001
   Time since start: 0:32:54.471393
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:32:56.369531
[batch 1140] samples: 18240, Training Loss: 0.0001
   Time since start: 0:32:58.235789
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:33:00.114819
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:33:01.982249
[batch 1200] samples: 19200, Training Loss: 0.0002
   Time since start: 0:33:03.854881
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:33:05.725266
[batch 1240] samples: 19840, Training Loss: 0.0001
   Time since start: 0:33:07.590717
[batch 1260] samples: 20160, Training Loss: 0.0011
   Time since start: 0:33:09.461190
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:33:11.391928
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:33:13.352112
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:33:15.237849
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:33:16.578099
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:33:17.895223
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:33:19.213682
[batch 1400] samples: 22400, Training Loss: 0.0001
   Time since start: 0:33:20.781185
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:33:22.378664
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:33:24.123880
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:33:25.930062
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:33:27.729150
[batch 1500] samples: 24000, Training Loss: 0.0001
   Time since start: 0:33:29.517278
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:33:31.332006
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:33:33.137378
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:33:34.712348
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:33:36.032430
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:33:37.352953
[batch 1620] samples: 25920, Training Loss: 0.0001
   Time since start: 0:33:38.671916
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:33:39.992425
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:33:41.476389
[batch 1680] samples: 26880, Training Loss: 0.0002
   Time since start: 0:33:43.397822
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:33:44.994267
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:33:46.343430
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:33:47.691704
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:33:49.416182
[batch 1780] samples: 28480, Training Loss: 0.0023
   Time since start: 0:33:51.235526
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:33:52.880842
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:33:54.200985
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:33:55.518696
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:33:57.231846
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:33:59.111400
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:34:00.980441
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:34:02.849441
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:34:04.709327
[batch 1960] samples: 31360, Training Loss: 0.0003
   Time since start: 0:34:06.569000
--m-Epoch 12 done.
   Training Loss: 0.0001
   Validation Loss: 0.0003
Patience decreased: Patience is now  1
Epoch: 13 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:34:20.031020
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:34:21.785516
[batch 60] samples: 960, Training Loss: 0.0001
   Time since start: 0:34:23.568146
[batch 80] samples: 1280, Training Loss: 0.0001
   Time since start: 0:34:25.318364
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:34:27.115207
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:34:28.884875
[batch 140] samples: 2240, Training Loss: 0.0001
   Time since start: 0:34:30.652880
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:34:32.421923
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:34:34.164319
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:34:35.903006
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:34:37.628679
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:34:39.349978
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:34:41.011375
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:34:42.319637
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:34:43.640373
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:34:44.975332
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:34:46.408111
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:34:48.370034
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:34:49.955725
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:34:51.376132
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:34:52.794883
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:34:54.215400
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:34:55.634953
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:34:57.260781
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:34:58.990679
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:35:00.709481
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:35:02.440140
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:35:04.181455
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:35:05.878118
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:35:07.609767
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:35:09.331958
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:35:11.050779
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:35:12.758651
[batch 680] samples: 10880, Training Loss: 0.0003
   Time since start: 0:35:14.517973
[batch 700] samples: 11200, Training Loss: 0.0001
   Time since start: 0:35:16.357816
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:35:18.086838
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:35:19.802927
[batch 760] samples: 12160, Training Loss: 0.0001
   Time since start: 0:35:21.129802
[batch 780] samples: 12480, Training Loss: 0.0001
   Time since start: 0:35:22.641876
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:35:24.662869
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:35:26.684986
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:35:28.537403
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:35:30.340473
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:35:32.142546
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:35:33.941189
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:35:35.738727
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:35:37.538816
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:35:39.322023
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:35:41.091742
[batch 1000] samples: 16000, Training Loss: 0.0001
   Time since start: 0:35:42.898110
[batch 1020] samples: 16320, Training Loss: 0.0001
   Time since start: 0:35:44.624632
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:35:46.343127
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:35:48.074962
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:35:49.762912
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:35:51.489291
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:35:53.190054
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:35:54.929403
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:35:56.649035
[batch 1180] samples: 18880, Training Loss: 0.0001
   Time since start: 0:35:58.363635
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:36:00.060520
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:36:01.760480
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:36:03.559725
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:36:05.570560
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:36:07.593125
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:36:09.615946
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:36:11.637091
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:36:13.659677
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:36:15.684154
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:36:17.705407
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:36:19.726868
[batch 1420] samples: 22720, Training Loss: 0.0001
   Time since start: 0:36:21.748693
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:36:23.772670
[batch 1460] samples: 23360, Training Loss: 0.0001
   Time since start: 0:36:25.796313
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:36:27.828260
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:36:29.851125
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:36:31.873891
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:36:33.899110
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:36:35.920794
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:36:37.944897
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:36:39.968396
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:36:41.989539
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:36:44.015029
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:36:46.038214
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:36:48.061982
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:36:50.085471
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:36:52.108951
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:36:54.131033
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:36:56.153912
[batch 1780] samples: 28480, Training Loss: 0.0001
   Time since start: 0:36:58.173701
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:37:00.196547
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:37:02.220077
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:37:04.233069
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:37:06.257194
[batch 1880] samples: 30080, Training Loss: 0.0001
   Time since start: 0:37:08.279232
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:37:10.300463
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:37:12.324977
[batch 1940] samples: 31040, Training Loss: 0.0001
   Time since start: 0:37:14.459572
[batch 1960] samples: 31360, Training Loss: 0.0001
   Time since start: 0:37:16.410040
--m-Epoch 13 done.
   Training Loss: 0.0001
   Validation Loss: 0.0001
Epoch: 14 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:37:29.972413
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:37:31.832989
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:37:33.722258
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:37:35.623956
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:37:37.538034
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:37:39.464409
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:37:41.372534
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:37:43.249642
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:37:45.158465
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:37:47.062281
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:37:48.942102
[batch 240] samples: 3840, Training Loss: 0.0001
   Time since start: 0:37:50.836339
[batch 260] samples: 4160, Training Loss: 0.0001
   Time since start: 0:37:52.732977
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:37:54.627254
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:37:56.531636
[batch 320] samples: 5120, Training Loss: 0.0003
   Time since start: 0:37:58.443647
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:38:00.355134
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:38:02.269590
[batch 380] samples: 6080, Training Loss: 0.0001
   Time since start: 0:38:04.189822
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:38:06.085700
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:38:07.458120
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:38:08.824918
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:38:10.189317
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:38:11.555259
[batch 500] samples: 8000, Training Loss: 0.0001
   Time since start: 0:38:13.250889
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:38:15.208509
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:38:17.175117
[batch 560] samples: 8960, Training Loss: 0.0002
   Time since start: 0:38:19.148757
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:38:21.117657
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:38:23.070865
[batch 620] samples: 9920, Training Loss: 0.0001
   Time since start: 0:38:25.039726
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:38:26.987165
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:38:28.928077
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:38:30.871616
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:38:32.824776
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:38:34.797090
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:38:36.766255
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:38:38.731996
[batch 780] samples: 12480, Training Loss: 0.0002
   Time since start: 0:38:40.700645
[batch 800] samples: 12800, Training Loss: 0.0018
   Time since start: 0:38:42.669074
[batch 820] samples: 13120, Training Loss: 0.0001
   Time since start: 0:38:44.638261
[batch 840] samples: 13440, Training Loss: 0.0001
   Time since start: 0:38:45.959924
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:38:47.598348
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:38:49.465399
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:38:51.098854
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:38:52.909057
[batch 940] samples: 15040, Training Loss: 0.0001
   Time since start: 0:38:54.720929
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:38:56.569335
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:38:58.209405
[batch 1000] samples: 16000, Training Loss: 0.0004
   Time since start: 0:38:59.531652
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:39:01.330622
[batch 1040] samples: 16640, Training Loss: 0.0001
   Time since start: 0:39:03.181654
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:39:05.031042
[batch 1080] samples: 17280, Training Loss: 0.0002
   Time since start: 0:39:06.899978
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:39:08.751969
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:39:10.603925
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:39:12.452555
[batch 1160] samples: 18560, Training Loss: 0.0001
   Time since start: 0:39:14.315086
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:39:16.179735
[batch 1200] samples: 19200, Training Loss: 0.0001
   Time since start: 0:39:18.054929
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:39:20.065793
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:39:21.931965
[batch 1260] samples: 20160, Training Loss: 0.0001
   Time since start: 0:39:23.795743
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:39:25.241326
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:39:26.581424
[batch 1320] samples: 21120, Training Loss: 0.0001
   Time since start: 0:39:27.967965
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:39:29.388191
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:39:30.714887
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:39:32.049613
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:39:33.535387
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:39:35.357610
[batch 1440] samples: 23040, Training Loss: 0.0001
   Time since start: 0:39:37.174525
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:39:38.993702
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:39:40.721925
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:39:42.082464
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:39:43.465267
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:39:44.819030
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:39:46.385967
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:39:48.196531
[batch 1600] samples: 25600, Training Loss: 0.0001
   Time since start: 0:39:50.035138
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:39:51.874344
[batch 1640] samples: 26240, Training Loss: 0.0001
   Time since start: 0:39:53.693351
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:39:55.534431
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:39:57.364614
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:39:59.193556
[batch 1720] samples: 27520, Training Loss: 0.0001
   Time since start: 0:40:01.022232
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:40:02.872518
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:40:04.683194
[batch 1780] samples: 28480, Training Loss: 0.0003
   Time since start: 0:40:06.521639
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:40:08.357838
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:40:10.191213
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:40:12.031480
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:40:13.571902
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:40:15.422391
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:40:16.796500
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:40:18.261492
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:40:20.158408
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:40:22.040808
--m-Epoch 14 done.
   Training Loss: 0.0001
   Validation Loss: 0.0001
Epoch: 15 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:40:34.799854
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:40:36.690358
[batch 60] samples: 960, Training Loss: 0.0003
   Time since start: 0:40:38.589580
[batch 80] samples: 1280, Training Loss: 0.0007
   Time since start: 0:40:40.489569
[batch 100] samples: 1600, Training Loss: 0.0002
   Time since start: 0:40:42.378660
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:40:44.260062
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:40:46.148673
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:40:48.069698
[batch 180] samples: 2880, Training Loss: 0.0001
   Time since start: 0:40:49.950216
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:40:51.869327
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:40:53.749794
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:40:55.640216
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:40:57.538911
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:40:59.438467
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:41:01.337705
[batch 320] samples: 5120, Training Loss: 0.0001
   Time since start: 0:41:03.237077
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:41:05.107998
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:41:06.938686
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:41:08.779679
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:41:10.607562
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:41:12.438338
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:41:14.277781
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:41:16.107480
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:41:18.003191
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:41:20.027922
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:41:21.916964
[batch 540] samples: 8640, Training Loss: 0.0001
   Time since start: 0:41:23.890695
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:41:25.862169
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:41:27.832928
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:41:29.806715
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:41:31.779246
[batch 640] samples: 10240, Training Loss: 0.0001
   Time since start: 0:41:33.226434
[batch 660] samples: 10560, Training Loss: 0.0002
   Time since start: 0:41:34.507299
[batch 680] samples: 10880, Training Loss: 0.0001
   Time since start: 0:41:35.787146
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:41:37.068011
[batch 720] samples: 11520, Training Loss: 0.0001
   Time since start: 0:41:38.355280
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:41:39.986360
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:41:41.477943
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:41:42.762068
[batch 800] samples: 12800, Training Loss: 0.0001
   Time since start: 0:41:44.044899
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:41:45.329154
[batch 840] samples: 13440, Training Loss: 0.0004
   Time since start: 0:41:46.611521
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:41:47.898268
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:41:49.183874
[batch 900] samples: 14400, Training Loss: 0.0001
   Time since start: 0:41:50.466635
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:41:51.750392
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:41:53.033577
[batch 960] samples: 15360, Training Loss: 0.0002
   Time since start: 0:41:54.318362
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:41:55.601642
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:41:56.888487
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:41:58.206501
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:41:59.528830
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:42:00.851039
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:42:02.175248
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:42:03.509319
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:42:04.839929
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:42:06.162084
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:42:07.485004
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:42:08.806247
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:42:10.131798
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:42:11.455490
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:42:12.778510
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:42:14.106256
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:42:15.453372
[batch 1300] samples: 20800, Training Loss: 0.0052
   Time since start: 0:42:16.815473
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:42:18.172704
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:42:19.531789
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:42:20.895418
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:42:22.260844
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:42:24.024929
[batch 1420] samples: 22720, Training Loss: 0.0016
   Time since start: 0:42:25.860945
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:42:27.628290
[batch 1460] samples: 23360, Training Loss: 0.0001
   Time since start: 0:42:28.962521
[batch 1480] samples: 23680, Training Loss: 0.0001
   Time since start: 0:42:30.296463
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:42:31.630434
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:42:32.969746
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:42:34.304429
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:42:35.638664
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:42:36.976503
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:42:38.313812
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:42:39.657851
[batch 1640] samples: 26240, Training Loss: 0.0002
   Time since start: 0:42:41.001844
[batch 1660] samples: 26560, Training Loss: 0.0001
   Time since start: 0:42:42.894593
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:42:44.787108
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:42:46.666177
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:42:48.471002
[batch 1740] samples: 27840, Training Loss: 0.0002
   Time since start: 0:42:50.526066
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:42:52.449415
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:42:54.357637
[batch 1800] samples: 28800, Training Loss: 0.0001
   Time since start: 0:42:56.238780
[batch 1820] samples: 29120, Training Loss: 0.0001
   Time since start: 0:42:58.118660
[batch 1840] samples: 29440, Training Loss: 0.0001
   Time since start: 0:42:59.611758
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:43:01.474005
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:43:03.332464
[batch 1900] samples: 30400, Training Loss: 0.0001
   Time since start: 0:43:05.201892
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:43:07.062761
[batch 1940] samples: 31040, Training Loss: 0.0020
   Time since start: 0:43:08.944057
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:43:10.800506
--m-Epoch 15 done.
   Training Loss: 0.0001
   Validation Loss: 0.0001
Patience decreased: Patience is now  2
Epoch: 16 of 30
[batch 20] samples: 320, Training Loss: 0.0001
   Time since start: 0:43:22.818583
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:43:24.547701
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:43:26.246812
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:43:27.996654
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:43:29.609382
[batch 120] samples: 1920, Training Loss: 0.0001
   Time since start: 0:43:30.886861
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:43:32.164880
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:43:33.445745
[batch 180] samples: 2880, Training Loss: 0.0015
   Time since start: 0:43:34.728807
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:43:36.027873
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:43:37.344119
[batch 240] samples: 3840, Training Loss: 0.0002
   Time since start: 0:43:38.658168
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:43:39.972178
[batch 280] samples: 4480, Training Loss: 0.0001
   Time since start: 0:43:41.286748
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:43:42.605405
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:43:43.925251
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:43:45.247621
[batch 360] samples: 5760, Training Loss: 0.0001
   Time since start: 0:43:46.569334
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:43:47.894284
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:43:49.221998
[batch 420] samples: 6720, Training Loss: 0.0003
   Time since start: 0:43:50.544893
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:43:51.868375
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:43:53.190715
[batch 480] samples: 7680, Training Loss: 0.0001
   Time since start: 0:43:54.510636
[batch 500] samples: 8000, Training Loss: 0.0002
   Time since start: 0:43:55.830010
[batch 520] samples: 8320, Training Loss: 0.0001
   Time since start: 0:43:57.149004
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:43:58.472227
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:44:00.104650
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:44:01.424672
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:44:02.742979
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:44:04.060122
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:44:05.378040
[batch 660] samples: 10560, Training Loss: 0.0003
   Time since start: 0:44:06.692335
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:44:08.009407
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:44:09.816816
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:44:11.645441
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:44:13.489054
[batch 760] samples: 12160, Training Loss: 0.0020
   Time since start: 0:44:15.345486
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:44:16.989461
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:44:18.309663
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:44:19.646450
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:44:21.041851
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:44:22.385687
[batch 880] samples: 14080, Training Loss: 0.0001
   Time since start: 0:44:23.704463
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:44:25.023419
[batch 920] samples: 14720, Training Loss: 0.0002
   Time since start: 0:44:26.347507
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:44:27.666525
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:44:28.984217
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:44:30.305044
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:44:31.752939
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:44:33.076660
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:44:34.398668
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:44:35.720289
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:44:37.043556
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:44:38.369082
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:44:39.697261
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:44:41.047645
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:44:42.453522
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:44:43.839328
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:44:45.640149
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:44:47.135876
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:44:49.016873
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:44:50.979046
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:44:52.951276
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:44:54.841409
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:44:56.647821
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:44:57.965727
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:44:59.284206
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:45:00.604893
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:45:01.917665
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:45:03.235997
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:45:04.555413
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:45:05.871370
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:45:07.190423
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:45:08.510498
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:45:09.830318
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:45:11.148498
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:45:12.467212
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:45:13.790577
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:45:15.126922
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:45:16.485374
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:45:17.842082
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:45:19.202227
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:45:20.558898
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:45:21.914756
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:45:23.430034
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:45:25.264875
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:45:27.083958
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:45:28.926041
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:45:30.677126
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:45:32.293089
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:45:34.185663
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:45:36.086846
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:45:37.976815
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:45:39.807004
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:45:41.648210
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:45:43.503209
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:45:45.354472
--m-Epoch 16 done.
   Training Loss: 0.0001
   Validation Loss: 0.0002
Patience decreased: Patience is now  1
Epoch: 17 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:45:58.851010
[batch 40] samples: 640, Training Loss: 0.0001
   Time since start: 0:46:00.718124
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:46:02.592526
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:46:04.467474
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:46:06.349164
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:46:08.230434
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:46:10.140505
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:46:12.065980
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:46:13.975348
[batch 200] samples: 3200, Training Loss: 0.0001
   Time since start: 0:46:15.892027
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:46:17.777792
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:46:19.659293
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:46:21.525069
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:46:23.541153
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:46:25.438100
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:46:27.327397
[batch 340] samples: 5440, Training Loss: 0.0001
   Time since start: 0:46:28.975446
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:46:30.295895
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:46:31.620133
[batch 400] samples: 6400, Training Loss: 0.0001
   Time since start: 0:46:32.940102
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:46:34.448148
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:46:36.313797
[batch 460] samples: 7360, Training Loss: 0.0001
   Time since start: 0:46:38.213554
[batch 480] samples: 7680, Training Loss: 0.0017
   Time since start: 0:46:40.113526
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:46:42.005722
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:46:43.889961
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:46:45.638099
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:46:46.991916
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:46:48.342131
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:46:49.777793
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:46:51.546455
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:46:53.316177
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:46:55.053149
[batch 680] samples: 10880, Training Loss: 0.0007
   Time since start: 0:46:56.847858
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:46:58.358306
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:46:59.647497
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:47:00.933541
[batch 760] samples: 12160, Training Loss: 0.0001
   Time since start: 0:47:02.312920
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:47:04.265313
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:47:06.233695
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:47:08.204466
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:47:09.954635
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:47:11.717623
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:47:13.491019
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:47:15.016968
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:47:16.970544
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:47:18.919095
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:47:20.809248
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:47:22.197430
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:47:23.588056
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:47:24.977275
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:47:26.369837
[batch 1060] samples: 16960, Training Loss: 0.0001
   Time since start: 0:47:28.280914
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:47:30.252480
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:47:32.213020
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:47:34.183516
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:47:36.143308
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:47:38.090110
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:47:39.978055
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:47:41.885591
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:47:43.781885
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:47:45.692196
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:47:47.578648
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:47:49.482196
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:47:51.381823
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:47:52.886955
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:47:54.242163
[batch 1360] samples: 21760, Training Loss: 0.0001
   Time since start: 0:47:55.596502
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:47:56.952305
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:47:58.312884
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:47:59.672503
[batch 1440] samples: 23040, Training Loss: 0.0002
   Time since start: 0:48:01.027894
[batch 1460] samples: 23360, Training Loss: 0.0001
   Time since start: 0:48:02.383464
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:48:03.742412
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:48:05.098217
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:48:06.572191
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:48:07.933510
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:48:09.291459
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:48:10.648347
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:48:12.009414
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:48:13.370011
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:48:14.753833
[batch 1660] samples: 26560, Training Loss: 0.0001
   Time since start: 0:48:16.131021
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:48:17.492333
[batch 1700] samples: 27200, Training Loss: 0.0001
   Time since start: 0:48:18.849432
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:48:20.209019
[batch 1740] samples: 27840, Training Loss: 0.0000
   Time since start: 0:48:21.568022
[batch 1760] samples: 28160, Training Loss: 0.0001
   Time since start: 0:48:23.304925
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:48:25.056465
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:48:26.419865
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:48:27.831924
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:48:29.723163
[batch 1860] samples: 29760, Training Loss: 0.0001
   Time since start: 0:48:31.608649
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:48:33.493394
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:48:35.437219
[batch 1920] samples: 30720, Training Loss: 0.0001
   Time since start: 0:48:37.409253
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:48:39.378378
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:48:41.290040
--m-Epoch 17 done.
   Training Loss: 0.0001
   Validation Loss: 0.0001
Epoch: 18 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:48:53.383210
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:48:55.013676
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:48:56.287078
[batch 80] samples: 1280, Training Loss: 0.0005
   Time since start: 0:48:57.561643
[batch 100] samples: 1600, Training Loss: 0.0000
   Time since start: 0:48:58.835712
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:49:00.122102
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:49:01.400853
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:49:02.678179
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:49:03.954687
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:49:05.228645
[batch 220] samples: 3520, Training Loss: 0.0000
   Time since start: 0:49:06.502921
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:49:07.777645
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:49:09.051357
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:49:10.326380
[batch 300] samples: 4800, Training Loss: 0.0001
   Time since start: 0:49:11.601259
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:49:12.879182
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:49:14.160417
[batch 360] samples: 5760, Training Loss: 0.0000
   Time since start: 0:49:15.467040
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:49:16.766633
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:49:18.042260
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:49:19.321915
[batch 440] samples: 7040, Training Loss: 0.0000
   Time since start: 0:49:20.597197
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:49:21.874395
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:49:23.151764
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:49:24.428874
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:49:25.703622
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:49:26.987953
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:49:28.266153
[batch 580] samples: 9280, Training Loss: 0.0000
   Time since start: 0:49:29.545770
[batch 600] samples: 9600, Training Loss: 0.0000
   Time since start: 0:49:30.824670
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:49:32.103257
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:49:33.386111
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:49:34.664408
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:49:35.943966
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:49:37.222312
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:49:38.501823
[batch 740] samples: 11840, Training Loss: 0.0000
   Time since start: 0:49:39.781516
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:49:41.062027
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:49:42.366536
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:49:43.793707
[batch 820] samples: 13120, Training Loss: 0.0004
   Time since start: 0:49:45.092233
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:49:46.372891
[batch 860] samples: 13760, Training Loss: 0.0000
   Time since start: 0:49:47.652360
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:49:48.934606
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:49:50.221218
[batch 920] samples: 14720, Training Loss: 0.0001
   Time since start: 0:49:51.507567
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:49:52.785137
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:49:54.064415
[batch 980] samples: 15680, Training Loss: 0.0000
   Time since start: 0:49:55.346002
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:49:56.624549
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:49:57.903939
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:49:59.182998
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:50:00.463202
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:50:01.741885
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:50:03.027035
[batch 1120] samples: 17920, Training Loss: 0.0001
   Time since start: 0:50:04.308176
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:50:05.976309
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:50:07.731416
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:50:09.502838
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:50:11.272142
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:50:13.045926
[batch 1240] samples: 19840, Training Loss: 0.0004
   Time since start: 0:50:14.489756
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:50:15.778161
[batch 1280] samples: 20480, Training Loss: 0.0001
   Time since start: 0:50:17.057410
[batch 1300] samples: 20800, Training Loss: 0.0000
   Time since start: 0:50:18.335703
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:50:19.614191
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:50:20.891485
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:50:22.169183
[batch 1380] samples: 22080, Training Loss: 0.0001
   Time since start: 0:50:23.448392
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:50:24.729025
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:50:26.006438
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:50:27.629221
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:50:29.471030
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:50:31.301871
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:50:33.121008
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:50:34.947783
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:50:36.772831
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:50:38.570143
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:50:40.379845
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:50:42.228978
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:50:43.573089
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:50:45.035250
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:50:46.352279
[batch 1680] samples: 26880, Training Loss: 0.0001
   Time since start: 0:50:47.686057
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:50:49.010924
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:50:50.329393
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:50:51.646858
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:50:52.966986
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:50:54.286384
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:50:55.605108
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:50:56.930396
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:50:58.694257
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:51:00.193069
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:51:02.094222
[batch 1900] samples: 30400, Training Loss: 0.0003
   Time since start: 0:51:03.987590
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:51:05.867983
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:51:07.760127
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:51:09.622957
--m-Epoch 18 done.
   Training Loss: 0.0001
   Validation Loss: 0.0002
Patience decreased: Patience is now  1
Epoch: 19 of 30
[batch 20] samples: 320, Training Loss: 0.0000
   Time since start: 0:51:21.858438
[batch 40] samples: 640, Training Loss: 0.0000
   Time since start: 0:51:23.757154
[batch 60] samples: 960, Training Loss: 0.0000
   Time since start: 0:51:25.766965
[batch 80] samples: 1280, Training Loss: 0.0000
   Time since start: 0:51:27.638640
[batch 100] samples: 1600, Training Loss: 0.0001
   Time since start: 0:51:29.440498
[batch 120] samples: 1920, Training Loss: 0.0000
   Time since start: 0:51:30.793683
[batch 140] samples: 2240, Training Loss: 0.0000
   Time since start: 0:51:32.148574
[batch 160] samples: 2560, Training Loss: 0.0000
   Time since start: 0:51:33.501631
[batch 180] samples: 2880, Training Loss: 0.0000
   Time since start: 0:51:34.858340
[batch 200] samples: 3200, Training Loss: 0.0000
   Time since start: 0:51:36.213821
[batch 220] samples: 3520, Training Loss: 0.0001
   Time since start: 0:51:37.568336
[batch 240] samples: 3840, Training Loss: 0.0000
   Time since start: 0:51:38.923151
[batch 260] samples: 4160, Training Loss: 0.0000
   Time since start: 0:51:40.276343
[batch 280] samples: 4480, Training Loss: 0.0000
   Time since start: 0:51:41.631535
[batch 300] samples: 4800, Training Loss: 0.0000
   Time since start: 0:51:42.988031
[batch 320] samples: 5120, Training Loss: 0.0000
   Time since start: 0:51:44.342822
[batch 340] samples: 5440, Training Loss: 0.0000
   Time since start: 0:51:45.696386
[batch 360] samples: 5760, Training Loss: 0.0003
   Time since start: 0:51:47.053464
[batch 380] samples: 6080, Training Loss: 0.0000
   Time since start: 0:51:48.408517
[batch 400] samples: 6400, Training Loss: 0.0000
   Time since start: 0:51:49.762331
[batch 420] samples: 6720, Training Loss: 0.0000
   Time since start: 0:51:51.114317
[batch 440] samples: 7040, Training Loss: 0.0001
   Time since start: 0:51:52.471519
[batch 460] samples: 7360, Training Loss: 0.0000
   Time since start: 0:51:53.826625
[batch 480] samples: 7680, Training Loss: 0.0000
   Time since start: 0:51:55.182040
[batch 500] samples: 8000, Training Loss: 0.0000
   Time since start: 0:51:56.543575
[batch 520] samples: 8320, Training Loss: 0.0000
   Time since start: 0:51:57.898102
[batch 540] samples: 8640, Training Loss: 0.0000
   Time since start: 0:51:59.252855
[batch 560] samples: 8960, Training Loss: 0.0000
   Time since start: 0:52:00.603708
[batch 580] samples: 9280, Training Loss: 0.0001
   Time since start: 0:52:01.983506
[batch 600] samples: 9600, Training Loss: 0.0001
   Time since start: 0:52:03.383162
[batch 620] samples: 9920, Training Loss: 0.0000
   Time since start: 0:52:04.779458
[batch 640] samples: 10240, Training Loss: 0.0000
   Time since start: 0:52:06.171359
[batch 660] samples: 10560, Training Loss: 0.0000
   Time since start: 0:52:07.564906
[batch 680] samples: 10880, Training Loss: 0.0000
   Time since start: 0:52:08.956967
[batch 700] samples: 11200, Training Loss: 0.0000
   Time since start: 0:52:10.349927
[batch 720] samples: 11520, Training Loss: 0.0000
   Time since start: 0:52:11.743125
[batch 740] samples: 11840, Training Loss: 0.0001
   Time since start: 0:52:13.136324
[batch 760] samples: 12160, Training Loss: 0.0000
   Time since start: 0:52:14.540992
[batch 780] samples: 12480, Training Loss: 0.0000
   Time since start: 0:52:16.423452
[batch 800] samples: 12800, Training Loss: 0.0000
   Time since start: 0:52:18.050519
[batch 820] samples: 13120, Training Loss: 0.0000
   Time since start: 0:52:19.452617
[batch 840] samples: 13440, Training Loss: 0.0000
   Time since start: 0:52:20.847492
[batch 860] samples: 13760, Training Loss: 0.0001
   Time since start: 0:52:22.558029
[batch 880] samples: 14080, Training Loss: 0.0000
   Time since start: 0:52:24.528936
[batch 900] samples: 14400, Training Loss: 0.0000
   Time since start: 0:52:26.499778
[batch 920] samples: 14720, Training Loss: 0.0000
   Time since start: 0:52:28.461769
[batch 940] samples: 15040, Training Loss: 0.0000
   Time since start: 0:52:30.432930
[batch 960] samples: 15360, Training Loss: 0.0000
   Time since start: 0:52:32.352983
[batch 980] samples: 15680, Training Loss: 0.0001
   Time since start: 0:52:34.262998
[batch 1000] samples: 16000, Training Loss: 0.0000
   Time since start: 0:52:36.175152
[batch 1020] samples: 16320, Training Loss: 0.0000
   Time since start: 0:52:38.094348
[batch 1040] samples: 16640, Training Loss: 0.0000
   Time since start: 0:52:40.004502
[batch 1060] samples: 16960, Training Loss: 0.0000
   Time since start: 0:52:41.894937
[batch 1080] samples: 17280, Training Loss: 0.0000
   Time since start: 0:52:43.817354
[batch 1100] samples: 17600, Training Loss: 0.0000
   Time since start: 0:52:45.737153
[batch 1120] samples: 17920, Training Loss: 0.0000
   Time since start: 0:52:47.647719
[batch 1140] samples: 18240, Training Loss: 0.0000
   Time since start: 0:52:49.233370
[batch 1160] samples: 18560, Training Loss: 0.0000
   Time since start: 0:52:50.594651
[batch 1180] samples: 18880, Training Loss: 0.0000
   Time since start: 0:52:51.953041
[batch 1200] samples: 19200, Training Loss: 0.0000
   Time since start: 0:52:53.310071
[batch 1220] samples: 19520, Training Loss: 0.0000
   Time since start: 0:52:54.665145
[batch 1240] samples: 19840, Training Loss: 0.0000
   Time since start: 0:52:56.026280
[batch 1260] samples: 20160, Training Loss: 0.0000
   Time since start: 0:52:57.383883
[batch 1280] samples: 20480, Training Loss: 0.0000
   Time since start: 0:52:58.743071
[batch 1300] samples: 20800, Training Loss: 0.0001
   Time since start: 0:53:00.215405
[batch 1320] samples: 21120, Training Loss: 0.0000
   Time since start: 0:53:01.574569
[batch 1340] samples: 21440, Training Loss: 0.0000
   Time since start: 0:53:02.932367
[batch 1360] samples: 21760, Training Loss: 0.0000
   Time since start: 0:53:04.290557
[batch 1380] samples: 22080, Training Loss: 0.0000
   Time since start: 0:53:05.921373
[batch 1400] samples: 22400, Training Loss: 0.0000
   Time since start: 0:53:07.885016
[batch 1420] samples: 22720, Training Loss: 0.0000
   Time since start: 0:53:09.572994
[batch 1440] samples: 23040, Training Loss: 0.0000
   Time since start: 0:53:10.982341
[batch 1460] samples: 23360, Training Loss: 0.0000
   Time since start: 0:53:12.692051
[batch 1480] samples: 23680, Training Loss: 0.0000
   Time since start: 0:53:14.095528
[batch 1500] samples: 24000, Training Loss: 0.0000
   Time since start: 0:53:15.493164
[batch 1520] samples: 24320, Training Loss: 0.0000
   Time since start: 0:53:16.889567
[batch 1540] samples: 24640, Training Loss: 0.0000
   Time since start: 0:53:18.288434
[batch 1560] samples: 24960, Training Loss: 0.0000
   Time since start: 0:53:19.848346
[batch 1580] samples: 25280, Training Loss: 0.0000
   Time since start: 0:53:21.820644
[batch 1600] samples: 25600, Training Loss: 0.0000
   Time since start: 0:53:23.790555
[batch 1620] samples: 25920, Training Loss: 0.0000
   Time since start: 0:53:25.659853
[batch 1640] samples: 26240, Training Loss: 0.0000
   Time since start: 0:53:27.562597
[batch 1660] samples: 26560, Training Loss: 0.0000
   Time since start: 0:53:29.463343
[batch 1680] samples: 26880, Training Loss: 0.0000
   Time since start: 0:53:30.887751
[batch 1700] samples: 27200, Training Loss: 0.0000
   Time since start: 0:53:32.257411
[batch 1720] samples: 27520, Training Loss: 0.0000
   Time since start: 0:53:33.619001
[batch 1740] samples: 27840, Training Loss: 0.0001
   Time since start: 0:53:34.980594
[batch 1760] samples: 28160, Training Loss: 0.0000
   Time since start: 0:53:36.343529
[batch 1780] samples: 28480, Training Loss: 0.0000
   Time since start: 0:53:37.709362
[batch 1800] samples: 28800, Training Loss: 0.0000
   Time since start: 0:53:39.067602
[batch 1820] samples: 29120, Training Loss: 0.0000
   Time since start: 0:53:40.432386
[batch 1840] samples: 29440, Training Loss: 0.0000
   Time since start: 0:53:41.793205
[batch 1860] samples: 29760, Training Loss: 0.0000
   Time since start: 0:53:43.153322
[batch 1880] samples: 30080, Training Loss: 0.0000
   Time since start: 0:53:44.513285
[batch 1900] samples: 30400, Training Loss: 0.0000
   Time since start: 0:53:45.870165
[batch 1920] samples: 30720, Training Loss: 0.0000
   Time since start: 0:53:47.230244
[batch 1940] samples: 31040, Training Loss: 0.0000
   Time since start: 0:53:48.588221
[batch 1960] samples: 31360, Training Loss: 0.0000
   Time since start: 0:53:49.901140
--m-Epoch 19 done.
   Training Loss: 0.0000
   Validation Loss: 0.0002
Patience decreased: Patience is now  0
Stopping early
     precision    recall  f1-score  support  epoch  class
0     0.999831  0.998986  0.999408   5916.0      1      0
1     1.000000  1.000000  1.000000    378.0      1      1
2     1.000000  0.999113  0.999557   1128.0      1      2
3     1.000000  0.997619  0.998808    420.0      1      3
4     1.000000  0.993056  0.996516    576.0      1      4
..         ...       ...       ...      ...    ...    ...
888   1.000000  1.000000  1.000000     72.0     19     42
889   0.999847  0.999908  0.999877  32592.0     19      0
890   0.999721  0.999836  0.999778  32592.0     19      1
891   0.999848  0.999908  0.999878  32592.0     19      2
892   0.999881  0.999923  0.999900  32592.0     19      3

[893 rows x 6 columns]
